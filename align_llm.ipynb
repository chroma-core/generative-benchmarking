{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align LLM Judge\n",
    "\n",
    "This notebook walks through how to align your LLM for document quality filtering. \n",
    "\n",
    "We use our adaptation of the [EvalGen](https://arxiv.org/pdf/2404.12272) framework, a systematic approach to aligning your LLM judge with human preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install & Import\n",
    "\n",
    "Install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI as OpenAIClient\n",
    "from anthropic import Anthropic as AnthropicClient\n",
    "from functions.llm import *\n",
    "from functions.evaluate import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTHROPIC_API_KEY = \"YOUR ANTHROPIC API KEY\"\n",
    "\n",
    "anthropic_client = AnthropicClient(api_key=ANTHROPIC_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load in Labeled Data\n",
    "\n",
    "Load in your manually labeled data and your entire corpus of documents.\n",
    "- Reference data schema in `data/human_labeled_data.json` and `data/chroma_docs.json`.\n",
    "\n",
    "We recommend ~200 labeled entries to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/human_labeled_data.json', 'r') as f:\n",
    "    human_labeled_documents = json.load(f)\n",
    "\n",
    "with open('data/chroma_docs.json', 'r') as f:\n",
    "    all_documents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ids = list(human_labeled_documents.keys())\n",
    "labeled_documents = [all_documents[id] for id in labeled_ids]\n",
    "\n",
    "unlabeled_ids = [key for key in all_documents if key not in labeled_ids]\n",
    "unlabeled_documents = [all_documents[id] for id in unlabeled_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Criteria\n",
    "\n",
    "We define our baseline criteria that we can iterate on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in `context` and `user_intent` according to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"FILL IN WITH YOUR CONTEXT\"\n",
    "user_intent = \"FILL IN WITH YOUR USER'S INTENT (e.g. seeking help with technical issues)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to modify/add criteria as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = f\"The document is relevant and something that users would search for considering the following context: {context}\"\n",
    "\n",
    "completeness = \"The document is complete, meaning that it contains useful information to answer queries and does not only serve as an introduction or summary for the main content that users may be looking for.\"\n",
    "\n",
    "intent = f\"The document would be relevant in the case of a user {user_intent}\"\n",
    "\n",
    "criteria = [relevance, completeness, intent]\n",
    "criteria_labels = [\"relevant\", \"complete\", \"intent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get LLM Labels\n",
    "\n",
    "We create a batch request for our LLM calls (this is cheaper and typically faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_documents_v1_id = create_document_filter_batch(\n",
    "    client=anthropic_client,\n",
    "    documents=labeled_documents,\n",
    "    ids=labeled_ids,\n",
    "    criteria=criteria,\n",
    "    criteria_labels=criteria_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the status of your batch through the [Anthropic Console](https://console.anthropic.com/workspaces/default/batches).\n",
    "\n",
    "Retrieve the batch once it is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_documents_v1 = retrieve_document_filter_batch(\n",
    "    client=anthropic_client,\n",
    "    batch_id=filtered_documents_v1_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Compare LLM vs Human Labels\n",
    "\n",
    "We take our LLM-labeled data and compare with our manual labling.\n",
    "\n",
    "`criteria_threshold` indicates the number of criterion that must be met in order for a document to be considered \"good quality\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_vs_human(\n",
    "    llm_judgements=filtered_documents_v1,\n",
    "    human_judgements=human_labeled_documents,\n",
    "    documents_mapping=all_documents,\n",
    "    criteria_labels=criteria_labels,\n",
    "    criteria_threshold=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Iterate\n",
    "\n",
    "Based on the results above, improve your LLM vs Human alignment score by iterating on your criteria:\n",
    "- Modify prompts\n",
    "- Add/remove criteria\n",
    "- Notice how the overall alignment score and criterion-specific scores change"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
