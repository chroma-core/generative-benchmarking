{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate Results\n",
    "\n",
    "This notebook demonstrates how to replicate our results for:\n",
    "- **Generating representative benchmarks** with public datasets using the English subset of the [multilingual Wikipedia dataset](https://huggingface.co/datasets/ellamind/wikipedia-2023-11-retrieval-multilingual-queries)\n",
    "\n",
    "- **Aligning our LLM Judge** for document quality filtering with [Weights and Biases](https://wandb.ai/site/) data\n",
    "\n",
    "The rest of our results can be replicated with through replacing the dataset and embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install & Import\n",
    "\n",
    "Install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from openai import OpenAI as OpenAIClient\n",
    "from anthropic import Anthropic as AnthropicClient\n",
    "from functions.llm import *\n",
    "from functions.embed import *\n",
    "from functions.chroma import *\n",
    "from functions.evaluate import *\n",
    "from functions.utils import *\n",
    "from functions.visualize import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load API Keys\n",
    "\n",
    "To use Chroma Cloud, you can sign up for a Chroma Cloud account [here](https://www.trychroma.com/) and create a new database. If you want to use local Chroma, skip this step and simply input `OPENAI_API_KEY` and `CLAUDE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma Cloud\n",
    "CHROMA_TENANT = \"YOUR CHROMA TENANT ID\"\n",
    "X_CHROMA_TOKEN = \"YOUR CHROMA API KEY\"\n",
    "DATABASE_NAME = \"YOUR CHROMA DATABASE NAME\"\n",
    "\n",
    "# Embedding Model\n",
    "OPENAI_API_KEY = \"YOUR OPENAI API KEY\"\n",
    "\n",
    "# LLM\n",
    "CLAUDE_API_KEY = \"YOUR CLAUDE API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Set Clients\n",
    "\n",
    "Initialize the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.HttpClient(\n",
    "  ssl=True,\n",
    "  host='api.trychroma.com',\n",
    "  tenant=CHROMA_TENANT,\n",
    "  database=DATABASE_NAME,\n",
    "  headers={\n",
    "    'x-chroma-token': X_CHROMA_TOKEN\n",
    "  }\n",
    ")\n",
    "\n",
    "# If you want to use the local Chroma instead, uncomment the following line:\n",
    "# chroma_client = chromadb.Client()\n",
    "\n",
    "openai_client = OpenAIClient(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = AnthropicClient(api_key=CLAUDE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Load Data\n",
    "\n",
    "We'll use the English subset of the [multilingual Wikipedia dataset](https://huggingface.co/datasets/ellamind/wikipedia-2023-11-retrieval-multilingual-queries).\n",
    "\n",
    "We use the `test` split for this demonstration, which contains:\n",
    "- 1500 queries\n",
    "- 1500 query-document relevance judgments (qrels)\n",
    "- 13500 corpus documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the queries, documents, and query-document relevance judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_queries = datasets.load_dataset(\"ellamind/wikipedia-2023-11-retrieval-multilingual-queries\", \"en\")[\"test\"].to_pandas()\n",
    "wiki_corpus = datasets.load_dataset(\"ellamind/wikipedia-2023-11-retrieval-multilingual-corpus\", \"en\")[\"test\"].to_pandas()\n",
    "wiki_qrels = datasets.load_dataset(\"ellamind/wikipedia-2023-11-retrieval-multilingual-qrels\", \"en\")[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this specific dataset, the query-documnet relevance judgements include distractors as indicated by scores of 0.5 and target matches as indicated by scores of 1.0.\n",
    "\n",
    "We'll filter the query-document relevance judgments to only include target matches. Then, we'll combine the queries, documents, and query-document relevance judgments into a single dataframe for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qrels = wiki_qrels[wiki_qrels[\"score\"] == 1.0]\n",
    "\n",
    "wiki_qrels = combined_datasets_dataframes(wiki_queries, wiki_corpus, wiki_qrels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embed Corpus & Store in Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_corpus_ids = wiki_corpus[\"_id\"].tolist()\n",
    "wiki_corpus_texts = wiki_corpus[\"text\"].tolist()\n",
    "\n",
    "wiki_corpus_embeddings = openai_embed_in_batches(\n",
    "    openai_client=openai_client, \n",
    "    texts=wiki_corpus_texts, \n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "wiki_corpus_lookup = {\n",
    "    id: {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    } for id, text, embedding in zip(wiki_corpus_ids, wiki_corpus_texts, wiki_corpus_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create & Add to Chroma Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"wiki-text-embedding-3-small\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "collection_add_in_batches(\n",
    "    collection=wiki_collection, \n",
    "    ids=wiki_corpus_ids, \n",
    "    texts=wiki_corpus_texts, \n",
    "    embeddings=wiki_corpus_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Query Generation\n",
    "\n",
    "We will demonstrate that LLMs have memorized a substantial portion of public benchmarks, which limits their ability to reliably generate unseen queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Generate Queries\n",
    "\n",
    "Generate 1500 queries, only including the document as context.\n",
    "\n",
    "We batch the LLM calls for efficiency\n",
    "- ids are converted to align with Anthropic's batch id formatting\n",
    "- batch processing status can be viewed through [Anthropic's Console](https://console.anthropic.com/workspaces/default/batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ids_qrels = wiki_qrels[\"corpus-id\"].tolist()\n",
    "corpus_texts_qrels = wiki_qrels[\"corpus-text\"].tolist()\n",
    "\n",
    "ids_for_batching = [clean_id_for_batching(id) for id in corpus_ids_qrels]\n",
    "\n",
    "naive_queries_batch_id = create_naive_query_batch(\n",
    "    client=anthropic_client,\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    documents=corpus_texts_qrels,\n",
    "    ids=ids_for_batching\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the generated queries once the batch is complete and merge with `wiki_qrels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_queries_df = retrieve_batch(\n",
    "    client=anthropic_client,\n",
    "    batch_id=naive_queries_batch_id\n",
    ")\n",
    "\n",
    "naive_queries_df[\"id\"] = naive_queries_df[\"id\"].apply(revert_id_from_batching)\n",
    "\n",
    "wiki_qrels = wiki_qrels.merge(naive_queries_df, left_on=\"corpus-id\", right_on=\"id\", how=\"left\")\n",
    "wiki_qrels.rename(columns={\"query\": \"naively-generated-query\"}, inplace=True)\n",
    "wiki_qrels.drop(columns=[\"id\"], inplace=True)\n",
    "wiki_qrels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compare with Ground Truth Queries\n",
    "\n",
    "Embed the ground truth queries and generated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_queries = wiki_qrels[\"query-text\"].tolist()\n",
    "query_ids = wiki_qrels[\"query-id\"].tolist()\n",
    "naive_queries = wiki_qrels[\"naively-generated-query\"].tolist()\n",
    "\n",
    "ground_truth_query_embeddings = openai_embed_in_batches(\n",
    "    openai_client=openai_client, \n",
    "    texts=ground_truth_queries, \n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "naive_query_embeddings = openai_embed_in_batches(\n",
    "    openai_client=openai_client, \n",
    "    texts=naive_queries, \n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "ground_truth_query_lookup = {\n",
    "    id: {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    } for id, text, embedding in zip(query_ids, ground_truth_queries, ground_truth_query_embeddings)\n",
    "}\n",
    "\n",
    "naive_query_lookup = {\n",
    "    id: {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    } for id, text, embedding in zip(query_ids, naive_queries, naive_query_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot the cosine similarity scores between each ground truth query and its corresponding generated query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_query_comparison = score_query_query(\n",
    "    qrels=wiki_qrels, \n",
    "    query_embeddings_1=ground_truth_query_lookup, \n",
    "    query_embeddings_2=naive_query_lookup,\n",
    "    column_name=\"naive-query-score\"\n",
    ")\n",
    "\n",
    "plot_single_distribution(\n",
    "    df=naive_query_comparison, \n",
    "    column=\"naive-query-score\", \n",
    "    title=\"Ground Truth vs Naively Generated Queries\", \n",
    "    xlabel=\"Cosine Similarity\", \n",
    "    ylabel=\"Normalized Frequency\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With further investigation, we can see that identical queries have been generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_query_comparison.sort_values(by=\"naive-query-score\", ascending=False, inplace=True)\n",
    "\n",
    "for i, row in naive_query_comparison.head(10).iterrows():\n",
    "    print(f\"Score: {row['naive-query-score']:.4f}\")\n",
    "    print(f\"Original Query: {row['query-text']}\")\n",
    "    print(f\"Generated Query: {row['naively-generated-query']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distinct Query Generation\n",
    "\n",
    "Since models have memorized these public benchmarks, we will generate unseen queries by explicitely prompting the model to generate a distinct query. \n",
    "\n",
    "Then, we will demonstrate that these newly generated distinct queries are also representative of the ground truth dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Generate Queries\n",
    "\n",
    "We generate 1500 queries, now including both the ground truth query and the corpus as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_batch_id = create_distinct_query_batch(\n",
    "    client=anthropic_client,\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    documents=corpus_texts_qrels,\n",
    "    ids=ids_for_batching,\n",
    "    queries=ground_truth_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_queries_df = retrieve_batch(\n",
    "    client=anthropic_client,\n",
    "    batch_id=distinct_batch_id\n",
    ")\n",
    "distinct_queries_df[\"id\"] = distinct_queries_df[\"id\"].apply(revert_id_from_batching)\n",
    "\n",
    "wiki_qrels = wiki_qrels.merge(distinct_queries_df, left_on=\"corpus-id\", right_on=\"id\", how=\"left\")\n",
    "wiki_qrels.rename(columns={\"query\": \"distinct-generated-query\"}, inplace=True)\n",
    "wiki_qrels.drop(columns=[\"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Compare with Ground Truth Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_queries = wiki_qrels[\"distinct-generated-query\"].tolist()\n",
    "\n",
    "distinct_query_embeddings = openai_embed_in_batches(\n",
    "    openai_client=openai_client, \n",
    "    texts=distinct_queries, \n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "distinct_query_lookup = {\n",
    "    id: {\n",
    "        \"text\": text,\n",
    "        \"embedding\": embedding\n",
    "    } for id, text, embedding in zip(query_ids, distinct_queries, distinct_query_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the cosine similarity scores between each ground truth query and its corresponding generated (distinct) query, and compare with our previous plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_query_comparison = score_query_query(\n",
    "    qrels=wiki_qrels, \n",
    "    query_embeddings_1=ground_truth_query_lookup, \n",
    "    query_embeddings_2=distinct_query_lookup,\n",
    "    column_name=\"distinct-query-score\"\n",
    ")\n",
    "\n",
    "plot_overlaid_distribution(\n",
    "    df_1=naive_query_comparison, \n",
    "    df_2=distinct_query_comparison, \n",
    "    column_1=\"naive-query-score\", \n",
    "    column_2=\"distinct-query-score\", \n",
    "    title=\"Distinct vs Ground Truth Queries\", \n",
    "    xlabel=\"Cosine Similarity\", \n",
    "    ylabel=\"Normalized Frequency\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the most similar query-query scores and see that no identical queries have been generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_query_comparison.sort_values(by=\"distinct-query-score\", ascending=False, inplace=True)\n",
    "\n",
    "for i, row in distinct_query_comparison.head(10).iterrows():\n",
    "    print(f\"Score: {row['distinct-query-score']:.4f}\")\n",
    "    print(f\"Original Query: {row['query-text']}\")\n",
    "    print(f\"Generated Query: {row['distinct-generated-query']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_metrics = evaluate_and_visualize(\n",
    "    ground_truth_query_dict=ground_truth_query_lookup,\n",
    "    generated_query_dict=distinct_query_lookup,\n",
    "    corpus_embeddings_dict=wiki_corpus_lookup,\n",
    "    qrels=wiki_qrels,\n",
    "    collection=wiki_collection,\n",
    "    dataset_name=\"Wikipedia (English)\",\n",
    "    model_name=\"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Align LLM Judge\n",
    "\n",
    "We used labeled documents from [wandbot](https://github.com/wandb/wandbot), a technical support bot for Weights & Biases' AI developer tools.\n",
    "\n",
    "These documents were manually labeled `true` or `false` based on whether they are good for generating relevant queires from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/wandb_human_labels.json\", \"r\") as f:\n",
    "    human_labeled_documents = json.load(f)\n",
    "\n",
    "with open(\"data/wandb_docs.json\", \"r\") as f:\n",
    "    all_documents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ids = list(human_labeled_documents.keys())\n",
    "labeled_documents = [all_documents[id] for id in labeled_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Set Baseline Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_v1 = \"The document is relevant and contains information that users would search for in the context of a question-answering bot for Weights & Biases. It should address topics that are useful to machine learning practitioners.\"\n",
    "\n",
    "completeness_v1 = \"The document is complete, meaning it provides comprehensive information to answer queries rather than merely serving as an introduction.\"\n",
    "\n",
    "clarity_v1 = \"The document contains clear ideas and is comprehensible.\"\n",
    "\n",
    "criteria_v1 = [relevance_v1, completeness_v1, clarity_v1]\n",
    "criteria_labels_v1 = [\"relevant\", \"complete\", \"clear\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Get LLM Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_v1_id = create_document_filter_batch(\n",
    "    client=anthropic_client,\n",
    "    documents=labeled_documents,\n",
    "    ids=labeled_ids,\n",
    "    criteria=criteria_v1,\n",
    "    criteria_labels=criteria_labels_v1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_v1 = retrieve_document_filter_batch(\n",
    "    client=anthropic_client,\n",
    "    batch_id=batch_v1_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take our LLM-labeled data and compare with our manual labling.\n",
    "\n",
    "`criteria_threshold` indicates the number of criterion that must be met in order for a document to be considered \"good quality\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 0.652, 'complete': 0.528, 'clear': 0.508}\n",
      "Documents aligned with Human Judgement: 114, 45.6%\n",
      "\n",
      "\n",
      "Number of documents meeting threshold: 163\n",
      "Number of documents 100% aligned: 23\n",
      "Number of documents 0% aligned: 14\n",
      "Aligned: ## Challenges and Limitations of Q-learning\n",
      "* Slow Convergence and High Computational Requirements - Q-learning can take significant time to converge, especially in complex environments. It may require substantial computational resources, making it less feasible for real-time applications.\n",
      "* Curse of Dimensionality - The performance of Q-learning can deteriorate in high-dimensional state and action spaces, leading to increased computational complexity and reduced efficiency.\n",
      "* Lack of Generalisation - Q-learning tends to focus on specific states and actions, potentially leading to difficulties in generalizing learned policies to new, unseen environments and increased susceptibility to overfitting.\n",
      "* Exploration vs. Exploitation Trade-off - Striking the right balance between exploration (trying new actions) and exploitation (choosing the best-known actions) can be tricky. Over-exploration can lead to inefficiency, while under-exploitation can prevent discovering better strategies.\n",
      "* Handling Continuous State and Action Spaces - Q-learning is primarily designed for discrete state and action spaces. Adapting it for continuous spaces involves complex discretization techniques and can lead to suboptimal results.\n",
      "* Sensitivity to Hyperparameters - Q-learning's performance can depend highly on the choice of hyperparameters, such as the learning rate and discount factor. Finding the right values can be challenging.\n",
      "* Lack of Prior Knowledge - Q-learning doesn't incorporate prior knowledge about the environment, making it less efficient when some level of pre-existing understanding is available.\n",
      "* Non-Markovian Environments - Q-learning assumes that the environment follows the Markov property, meaning the future depends only on the current state and action. In non-Markovian environments, it may not perform optimally.\n",
      "\n",
      "\n",
      "Aligned: \"  \n",
      "\n",
      "# The Woven Planet (Lyft) Level 5 Dataset  \n",
      "\n",
      "Description: In this article, we'll be exploring the Woven Planet (Lyft) Level 5 dataset. We'll look at what it is as well as the autonomous vehicle tasks and techniques it supports  \n",
      "\n",
      "Body:  \n",
      "\n",
      "# What Is The Woven Planet Level 5 Dataset?  \n",
      "\n",
      "[The ](https://level-5.global/) is the largest [autonomous-driving dataset](https://wandb.ai/av-datasets/av-dataset/reports/The-Many-Datasets-of-Autonomous-Driving--VmlldzoyNjU1OTg0) for motion planning and prediction tasks. It contains over 1,000 hours of data collected by 20 self-driving cars and is annotated with semantic maps and high-definition aerial views.  \n",
      "\n",
      "There are 15,242 labeled elements in the dataset for [autonomous driving-related machine learning tasks](https://wandb.ai/av-team/av-tasks/reports/The-ML-Tasks-Of-Autonomous-Vehicle-Development--VmlldzoyNTc2Nzkx), such as motion forecasting, motion planning, and simulation.  \n",
      "\n",
      "## What We're Covering About The Level 5 Dataset  \n",
      "\n",
      "# Recommended Reading  \n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "Aligned: ## Dedicated Weights & Biases Hook\n",
      "### Checkpointing\n",
      "MMDetection uses MMCV's CheckpointHook to periodically save model checkpoints. The period is determined by checkpoint_config.interval. However, these checkpoints are saved locally and might get overwritten by a new experiment.  \n",
      "\n",
      "You can reliably store these checkpoints as W&B Artifacts by using the log_checkpoint=True argument. By saving them as W&B Artifacts, you can easily transfer the checkpoints across machines, keep different model ideas separately, and compare them across variants.  \n",
      "\n",
      "Here are a few things worth noting:  \n",
      "\n",
      "* There are 3 versions of checkpoints in the UI as shown above. That's because the model was trained for 12 epochs with `checkpoint_config.interval=4`.\n",
      "* Each version has an alias `epoch_x` where `x` is the current epoch.\n",
      "* The last checkpoint is marked with the alias `latest`.  \n",
      "\n",
      "We recommend you set the checkpoint interval with caution to save both local and W&B storage space.\n",
      "\n",
      "\n",
      "Aligned: ## Transcript\n",
      "### Leadership lessons that Jensen has learned\n",
      "\n",
      "Lukas:  \n",
      "\n",
      "> You've been running NVIDIA for quite a long time. I was curious how you feel you've changed as a leader over the decades of running the company.  \n",
      "\n",
      "Jensen:  \n",
      "\n",
      "> You know, you're almost asking the wrong person. You could ask almost anybody else around me.  \n",
      "\n",
      "Lukas:  \n",
      "\n",
      "> Fair enough. How has your experience changed?  \n",
      "\n",
      "Jensen:  \n",
      "\n",
      "> That's an easier question for me.  \n",
      "\n",
      "When I was 30 years old, I didn't know anything about being CEO. I did a lot of learning on the job. There were many management techniques that were just really dumb, and I don't use them anymore.  \n",
      "\n",
      "Lukas:  \n",
      "\n",
      "> Like what?  \n",
      "\n",
      "Jensen:  \n",
      "\n",
      "> Well, alright. I'll give you a couple.  \n",
      "\n",
      "Lukas:  \n",
      "\n",
      "> Awesome. Thank you.  \n",
      "\n",
      "Jensen:\n",
      "\n",
      "\n",
      "Aligned: ## About SBX Robotics\n",
      "\n",
      "Working on a computer vision problem? We can help.  \n",
      "\n",
      "At [SBX Robotics](http://sbxrobotics.com/) we are experts in using synthetic data to bootstrap and improve computer vision systems.  \n",
      "\n",
      "Our clients send us ~25 images from their production setting, and we generate 25,000 synthetic training samples proven to work on the original validation data. All of our datasets ship with:  \n",
      "\n",
      "* Our best benchmark model trained on the synthetic data, tested on your validation data, and loaded into a Google Colab.\n",
      "* Code to evaluate the benchmark model, , and our  with code snippets  \n",
      "\n",
      "### Ready to try synthetic data for your project?  \n",
      "\n",
      "[Use this link](https://www.sbxrobotics.com/?ref=wandb1#get-started) to submit 25-50 images from your production setting, or contact us at info@sbxrobotics.com.  \n",
      "\n",
      ">\n",
      "Mention  \"Weights & Biases Tables Tutorial\" for 20% off your first synthetic dataset.\n",
      "\n",
      "\n",
      "Not aligned: ## 🕶 What is HellaSwag?\n",
      "### 😎 Swag\n",
      "#### Adversarial Filtering\n",
      "The point of constructing this formula is to show, if we want an adversarial dataset, then we expect high empirical error on . In other words, in an ideal case, none of the examples generalize to another example within the dataset.  \n",
      "\n",
      "Definitions for adversarial-filtered dataset:  \n",
      "\n",
      "* for each instance , there is one positive instance  and many negative instances  where  so\n",
      "* we filter these negative instances for each instance  to  such that\n",
      "* thus, we construct a set of assignments (list of indices)\n",
      "* our adversarial-filtered dataset:  \n",
      "\n",
      "Great! We have a formal understanding of how this should be framed. How does it look algorithmically?  \n",
      "\n",
      "We initialize and maintain set of assignments  where .  This is iteratively updated via Algorithm 1 (Adversarial Filtering).\n",
      "\n",
      "\n",
      "Not aligned: ## Performing In-Context Training\n",
      "### Demonstration Design\n",
      "#### Scoring Function\n",
      "The takeaway from these different methods is that there is still a lot of work to be done in creating a scoring function that mitigates sensitivity and reduces bias. In a nutshell, it isn't easy to calibrate how well in-context learning performs. This field is still very new, and standard metrics have not been established.  \n",
      "\n",
      "Above is a table of factors that play into in-context learning.  \n",
      "\n",
      "They've concluded the following findings in the pretraining stage:  \n",
      "\n",
      "* domain source is more important than corpus size\n",
      "* corpora related to downstream tasks don't necessarily improve ICL ability\n",
      "* lower perplexity != better ICL\n",
      "* ICL emergent ability comes after some number of pretraining steps and model size  \n",
      "\n",
      "During the inference stage:  \n",
      "\n",
      "* input-label formatting matters\n",
      "* exposure of label space (what labels you use as examples)\n",
      "* input distribution\n",
      "* order of examples\n",
      "* examples with embeddings close to query embedding  \n",
      "\n",
      "The takeaway from numerous works attempting to understand why in-context learning works is:\n",
      "\n",
      "\n",
      "Not aligned: ## 🦄 A Brief Overview of Stable Diffusion XL\n",
      "* SDXL leverages a larger UNet backbone. Three times the size, in fact. The increase in parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder.\n",
      "* SDXL leverages multiple novel conditioning schemes and is trained on multiple aspect ratios.\n",
      "* The image generation pipeline also leverages a specialized high-resolution refiner model which is used to improve the visual fidelity of samples generated by SDXL using an image-to-image diffusion technique.\n",
      "* The base diffusion model generates initial latent tensors of size 128x128, which can be passed through a  loss to generate the high-resolution image.\n",
      "* The latent tensors could also be passed on to the refiner model that applies , using the same prompt. Although the base SDXL model is capable of generating stunning images with high fidelity, using the refiner model useful in many cases, especially to refine samples of low local quality such as deformed faces, eyes, lips, etc.\n",
      "* SDXL and the refinement model use the same autoencoder.\n",
      "\n",
      "\n",
      "Not aligned: ## How to Train Your Dragons Models\n",
      "### Zero-DCE: Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement\n",
      "#### Non-Reference Loss Functions\n",
      "where  and  represent the horizontal and vertical gradient operations, respectively.  \n",
      "\n",
      "Spatial Constancy Loss: The spatial consistency loss encourages spatial coherence of the enhanced image by preserving the difference of neighboring regions between the input image and its enhanced version. The Spatial Constancy Loss is given by:  \n",
      "\n",
      "$$\n",
      "\\Large{L_{s p a}=\\frac{1}{K} \\sum_{i=1}^K \\sum_{j \\in \\Omega(i)}\\left(\\left|\\left(Y_i-Y_j\\right)\\right|-\\left|\\left(I_i-I_j\\right)\\right|\\right)^2}\n",
      "$$  \n",
      "\n",
      "where...  \n",
      "\n",
      "*  is the four neighboring regions (top, down, left, right) centered at the region\n",
      "*  and  denote the average intensity value of the local region in the enhanced version and input image, respectively.  \n",
      "\n",
      "All these loss functions have been implemented as part of the [restorers.losses](https://github.com/soumik12345/restorers/tree/main/restorers/losses) API and are automatically initialized when calling [model.compile](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile).\n",
      "\n",
      "\n",
      "Not aligned: ## Types of Language Models\n",
      "### Neural Language Models\n",
      "#### LSTM Language Model\n",
      "* The input gate controls the new information to add to the memory cell. It uses a  to determine which values from the current input should be added to the memory cell.\n",
      "* The forget gate controls the amount of old information to forget. It also uses a sigmoid function to determine this.\n",
      "* The memory cell stores the information from the current and previous inputs, processed by the input and forgets gates.\n",
      "* The output gate controls the amount of information to output as the prediction. It uses a sigmoid function to determine this, followed by a tanh activation function to squish the values between -1 and 1.  \n",
      "\n",
      "This allows LSTM to effectively handle long sequences of data and make predictions based on the context of the entire sequence.  \n",
      "\n",
      "The cell state here is a  memory cell that maintains information across time steps in a sequence, and the hidden state is the output of the LSTM unit at each time step that is passed to the next time step in a sequence.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_vs_human(\n",
    "    llm_judgements=batch_v1,\n",
    "    human_judgements=human_labeled_documents,\n",
    "    documents_mapping=all_documents,\n",
    "    criteria_labels=criteria_labels_v1,\n",
    "    criteria_threshold=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the baseline results, we iterate on our criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_v2 = \"\"\"\n",
    "    The document is relevant and something that users would search for considering the following context: \n",
    "    We are building a question-answering bot designed specifically for Weights & Biases, an AI developer for training, fine-tuning, and managing models.\n",
    "    Any information that would be useful to a user working in machine learning is considered as relevant.\n",
    "    \"\"\"\n",
    "\n",
    "completeness_v2 = \"The document is complete, meaning that it contains useful information to answer queries and does not only serve as an introduction to the main content that users may be looking for.\"\n",
    "\n",
    "intent_v2 = \"The document would be relevant in the use case of a user working in machine learning, who may be seeking help or learn more about Weights & Biases or machine learning in general.\"\n",
    "\n",
    "criteria_v2 = [relevance_v2, completeness_v2, intent_v2]\n",
    "criteria_labels_v2 = [\"relevant\", \"complete\", \"intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_v2_id = create_document_filter_batch(\n",
    "    client=anthropic_client,\n",
    "    documents=labeled_documents,\n",
    "    ids=labeled_ids,\n",
    "    criteria=criteria_v2,\n",
    "    criteria_labels=criteria_labels_v2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_v2 = retrieve_document_filter_batch(\n",
    "    client=anthropic_client,\n",
    "    batch_id=batch_v2_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 0.64, 'complete': 0.656, 'intent': 0.568}\n",
      "Documents aligned with Human Judgement: 188, 75.2%\n",
      "\n",
      "\n",
      "Number of documents meeting threshold: 159\n",
      "Number of documents 100% aligned: 65\n",
      "Number of documents 0% aligned: 8\n",
      "Aligned: ## Data Preparation and Annotation\n",
      "### Annotating a Summarization Dataset for Fine-Tuning\n",
      "\n",
      "There is a given format required by ChatGPT in order to fine-tune the model on. This format includes 3 sections:  \n",
      "\n",
      "* System: This is the prompt that you will pass to ChatGPT. In our case, the prompt would be “GPT is a great and to-the-point dialogue summarization tool.”\n",
      "* User: This is the question asked to the model. In our case, it would be the text that we are required to summarize.\n",
      "* Assistant: This is the answer that our model would return. In this case, it would be a brief summary of the text.\n",
      "\n",
      "\n",
      "Aligned: ## Applications of Q-learning\n",
      "* The agent can be the program controlling a robot. In this scenario, the agent observes the environment (the real world) through sensors like cameras and touch sensors and acts accordingly by sending signals to the motors. It gets positive rewards for efficient navigation towards the goal and negative rewards for detours or time wastage.\n",
      "* The agent can be the program playing a board game like Go or chess.\n",
      "* The agent can be the program controlling Ms. Pac-Man where the environment is a simulation of the Atari game, actions are the nine joystick positions, and the rewards are the game points.\n",
      "* The agent can observe stock market prices and take action on whether to buy or sell.\n",
      "* The agent can be a smart thermostat that learns to understand human needs, getting positive rewards whenever it is close to the target temperature and saves energy, and getting negative rewards when humans need to tweak the temperature.\n",
      "* The agent can be a program solving a maze where it gets negative rewards for every time step, so it has to reach the exit as soon as possible.\n",
      "* There are various other tasks where it is well suited, such as driving cars, recommender systems, or placing ads on a web page.\n",
      "\n",
      "\n",
      "Aligned: ## Challenges and Limitations of Q-learning\n",
      "* Slow Convergence and High Computational Requirements - Q-learning can take significant time to converge, especially in complex environments. It may require substantial computational resources, making it less feasible for real-time applications.\n",
      "* Curse of Dimensionality - The performance of Q-learning can deteriorate in high-dimensional state and action spaces, leading to increased computational complexity and reduced efficiency.\n",
      "* Lack of Generalisation - Q-learning tends to focus on specific states and actions, potentially leading to difficulties in generalizing learned policies to new, unseen environments and increased susceptibility to overfitting.\n",
      "* Exploration vs. Exploitation Trade-off - Striking the right balance between exploration (trying new actions) and exploitation (choosing the best-known actions) can be tricky. Over-exploration can lead to inefficiency, while under-exploitation can prevent discovering better strategies.\n",
      "* Handling Continuous State and Action Spaces - Q-learning is primarily designed for discrete state and action spaces. Adapting it for continuous spaces involves complex discretization techniques and can lead to suboptimal results.\n",
      "* Sensitivity to Hyperparameters - Q-learning's performance can depend highly on the choice of hyperparameters, such as the learning rate and discount factor. Finding the right values can be challenging.\n",
      "* Lack of Prior Knowledge - Q-learning doesn't incorporate prior knowledge about the environment, making it less efficient when some level of pre-existing understanding is available.\n",
      "* Non-Markovian Environments - Q-learning assumes that the environment follows the Markov property, meaning the future depends only on the current state and action. In non-Markovian environments, it may not perform optimally.\n",
      "\n",
      "\n",
      "Aligned: ## Fundamentals of Neural Networks\n",
      "### 1. Basic Neural Network Structure\n",
      "#### Hidden Layers and Neurons per Hidden Layers\n",
      "* The number of hidden layers is highly dependent on the problem and the architecture of your neural network. You’re essentially trying to Goldilocks your way into the perfect neural network architecture – not too big, not too small, just right.\n",
      "* Generally, 1-5 hidden layers will serve you well for most problems. When working with image or speech data, you’d want your network to have dozens-hundreds of layers, not all of which might be fully connected. For these use cases, there are pre-trained models (, , ) that allow you to use large parts of their networks, and train your model on top of these networks to learn only the higher order features. In this case, your model will still have only a few layers to train.\n",
      "* In general using the same number of neurons for all hidden layers will suffice. For some datasets, having a large first layer and following it up with smaller layers will lead to better performance as the first layer can learn a lot of lower-level features that can feed into a few higher order features in the subsequent layers.\n",
      "\n",
      "\n",
      "Aligned: ## Setting up Evaluation using LlamaIndex\n",
      "### Generating Questions using LlamaIndex\n",
      "* Evaluating response for hallucination: Is the generated response coming from the provided context, or is it making up things?\n",
      "* Relevance of the retrieved chunks: Evaluate each retrieved chunk (node) against the generated response to see if that node contains the answer to the query.\n",
      "* Evaluating the answer quality: Does the query + generated response come from the provided context?  \n",
      "\n",
      "Using DatasetGenerator is easy, and one can pass the loaded documents to the DatasetGenerator.from_documents method. Calling generate_questions_from_nodes() on the object's instance will generate N questions per chunk. The default chunk size is 512, and N is 10. You might quickly realize that it will take a long time and a lot of API calls to generate a lot of questions. Let's customize the data generation process.\n",
      "\n",
      "\n",
      "Not aligned: ## Example App: NewsTrackr\n",
      "\n",
      "In this section, we'll learn how to build a web app that can extract NEWS data on a given topic (stock/index in our case) using NEWS API, then perform Aspect Based Sentiment Analysis (ABSA) to determine the sentiment of different aspects related to a stock or an index. We'll use [AI21](https://studio.ai21.com/overview) to build our model, [Streamlit](https://streamlit.io/) as our web framework, and [NewsAPI](https://newsapi.org/) to collect news articles.  \n",
      "\n",
      "But wait, why do we need to determine the sentiment of different aspects related to a stock? More importantly, what's ABSA?  \n",
      "\n",
      "## Problem Statement and Workflow  \n",
      "\n",
      ">  \n",
      "\n",
      "Let's say that a friend of mine is new to trading and needs some guidance on how to make informed trading decisions.  \n",
      "\n",
      "The right way to start trading would involve understanding the basics of technical and fundamental analysis, which are two primary methods used to analyze the markets and make informed investment decisions.  \n",
      "\n",
      "## ABSA Model Fine-Tune Pipeline\n",
      "\n",
      "\n",
      "Not aligned: ## An Introduction To HuggingFace Transformers for NLP\n",
      "### What Are Transformers in Machine Learning?\n",
      "After converting our data into a more understandable format, the embedded data is passed into the next layer, known as the self-attention layer.  \n",
      "\n",
      "By utilizing self-attention, a transformer is capable of detecting distant data relations and resolving vanishing gradients. Meaning that a given transformer model will still be able to study a given relationship between two related words even if both these words are too far away from each other in a given context.  \n",
      "\n",
      "The self-attention process represents how relevant a specific word is in relation to its neighboring words in a given sentence. This relation is then represented as what we call an attention vector.  \n",
      "\n",
      "There are three additional types of vectors created in the self-attention layer which are key, query, and value vectors. Each vector is then multiplied by the input vector in order to return a weighted value.\n",
      "\n",
      "\n",
      "Not aligned: ## Human Dynamics\n",
      "### Reward Design\n",
      "#### Good Alignment with Scene Objects\n",
      "* Distance:  ensures that the simulated end-effector (hands and feet) are in contact with the desired object. If  is the position of the end-effector and  is the target zone of the target object (in our case, an area of a quarter on the center of the surface), this reward is calculated using:\n",
      "* Alignment favors the alignment of the character and the object when in contact. If  is a unit vector along the frontal axis of the pelvis, this reward is calculated using:\n",
      "* Center of Mass:  informs the suitability of the trajectory of the character. If  is the distance between the center of mass and the end-effector on landing time and  is the distance between the expected center of mass and the expected landing position, this reward is calculated using:  \n",
      "\n",
      ">  \n",
      "\n",
      "Thus, the total Scene Loss is given by:  \n",
      "\n",
      "$$\n",
      "\\huge r_{\\text{scene}} = w_{\\text{dist}}r_{\\text{dist}} + w_{\\text{align}}r_{\\text{align}} + w_{\\text{com}}r_{\\text{com}}\n",
      "$$\n",
      "\n",
      "\n",
      "Not aligned: ## This week in AI: Meta LLaMA 2, Meta-Transformer, StabilityAI FreeWilly\n",
      "### Meta-Transformer\n",
      "\n",
      "Meta-Transformer, not from Meta, is a unified, multi-modal Transformer architecture!  \n",
      "\n",
      "It's safe to say this transformer is really multi-modal, not just text and images. Their website has a great video, below, walking through their paper's method.  \n",
      "\n",
      "The overall architecture of their Meta-[Transformer](http://wandb.ai/fully-connected/blog/transformer) consists of a data-to-sequence tokenizer layer which, itself, consists of multiple modality-specific tokenizers. The tokenized input enters a shared token space which can all be fed into the unified model. The output of this unified model is fed into task-specific models.  \n",
      "\n",
      "They benchmarked their model across dozens of benchmarks and other models!\n",
      "\n",
      "\n",
      "Not aligned: ## Part 2: Applications\n",
      "### Converting Text into Dataframes\n",
      "#### Defining the Data Structures\n",
      "The RowData class represents a single row of data in the dataframe. It contains a row attribute for the values in each row and a citation attribute for the citation from the original source data.  \n",
      "\n",
      "The Dataframe class represents a dataframe and consists of a name attribute, a list of RowData objects in the data attribute, and a list of column names in the columns attribute. It also provides a to_pandas method to convert the dataframe into a Pandas DataFrame.  \n",
      "\n",
      "The Database class represents a set of tables in a database. It contains a list of Dataframe objects in the tables attribute.  \n",
      "\n",
      "Now we can define our own extraction function as usual and see what happens.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_vs_human(\n",
    "    llm_judgements=batch_v2,\n",
    "    human_judgements=human_labeled_documents,\n",
    "    documents_mapping=all_documents,\n",
    "    criteria_labels=criteria_labels_v2,\n",
    "    criteria_threshold=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice an improvement in our overall LLM vs Human alignment, as well as individual criterion categories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
