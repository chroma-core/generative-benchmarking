{
    "1d686cbf-5a84-4bc3-b19a-27a067d9f87f": "## Step 3: Create and customize a Trace Debugging Board\n\nClick on the \"View data at:\" link above to view your data stream in Weave.  \n\nYou can click on \"+ New board from template\" on the right to create a Trace Debug Board, which enables:\n\\* key LLM tuning metrics at a glance: latency and success vs failure, for each call and as a distribution\n\\* complete view of call details: inputs, outputs, status, timestamp, etc\u2014also available for downtream queries & analaysis\n\\* interactive W&B Trace view: a color-coded flow chart of every step of an LLM chain, with full metadata, model details, and similar span views updating in sync with every selected span\n\\* monitor & analyze from the automatic Board; customize & publish the Board for streamlined collaboration with your team",
    "4657b9fc-835a-4b9f-9038-2e82c17f0cc1": "## What Is ResNet50?\nThe groundbreaking contribution of ResNet is the introduction of the residual block. These residual blocks allow connecting the activations of previous layers with the next while \u2018skipping\u2019 some layers in between, allowing the gradient to flow without being altered by a large magnitude.  \n\nImagine you're trying to get from Point A to Point B, but there are some confusing twists and turns in the middle. Instead of going through every single twisty path, ResNet takes a clever shortcut, letting it skip over a few of the tricky parts. So, this magical shortcut helps ResNet learn faster and better. It's a little like learning to ride a bike: once you've mastered balancing, you don't need to relearn it every time you pedal.  \n\nThe complete architecture of the ResNet50 model with four of its major parts is shown below:  \n\n## Is ResNet50 a CNN?  \n\n## Is ResNet50 Supervised or Unsupervised?",
    "e1abac2a-3eef-4c41-83b2-1476c087c170": "  \n\n# Youtube Video Summarisation using Mistral 7B, LangChain, and Whisper  \n\nDescription: An interactive report showcasing how to create a video summarization pipeline with three common ML tools  \n\nBody:  \n\n## Introduction  \n\nYouTube is a treasure trove of knowledge and entertainment, but it can be challenging to sift through long videos to extract the key takeaways.  \n\nEnter Whisper, LangChain and Mistral, groundbreaking technologies reshaping the landscape of video summarization, enabling users to grasp the essence of lengthy videos swiftly and efficiently.  \n\nIn this report, we'll walk you through how to use the three tools together to quickly and easily summarize video content.  \n\n## Table of Contents  \n\n## The Process of Video Summarization using LLMs:  \n\n## Understanding Whisper:  \n\n## LangChain's Role in Enhancing Summarization:  \n\n## Understanding the Mistral Model: A Breakthrough in Natural Language Processing  \n\n## Using W&B  \n\nCreate a W&B account and install W&B using  \n\n## Dataset",
    "ca90baa9-3e8d-483f-b8fb-58d2313226a2": "## \ud83d\udd76 What is HellaSwag?\n### \ud83d\ude0e Swag\n#### Adversarial Filtering\nThe point of constructing this formula is to show, if we want an adversarial dataset, then we expect high empirical error on . In other words, in an ideal case, none of the examples generalize to another example within the dataset.  \n\nDefinitions for adversarial-filtered dataset:  \n\n* for each instance , there is one positive instance  and many negative instances  where  so\n* we filter these negative instances for each instance  to  such that\n* thus, we construct a set of assignments (list of indices)\n* our adversarial-filtered dataset:  \n\nGreat! We have a formal understanding of how this should be framed. How does it look algorithmically?  \n\nWe initialize and maintain set of assignments  where .  This is iteratively updated via Algorithm 1 (Adversarial Filtering).",
    "53649ab9-7c2f-44ad-8701-5f58c84a3648": "## \ud83c\udfed CI/CD with GitOps and W&B\n\nThis section is inspired by the [W&B CI/CD Course](https://www.wandb.courses/courses/ci-cd-for-machine-learning).  \n\nWe will set up a simple GitHub workflow that, upon making a comment /wandb <run_id> to a PR, a [W&B Report](https://wandb.ai/site/reports/) will be generated comparing the run specified by the run_id within this project with the run tagged with \"baseline\". This is done with a combination of GitOps (and [ghapi ](https://github.com/fastai/ghapi)by fastai) and W&B Reports API.  \n\nEverything can be found on this article's [repository ](https://github.com/alckasoc/wandb-cicd)and the W&B Project [reports page. ](https://wandb.ai/vincenttu/cicd_and_wandb/reportlist?workspace=user-vincenttu)Let's get started!  \n\nWe need 2 files: compare_runs.py and .github/workflows/ci.yaml. compare_runs.py will be responsible for comparing the run run_id with the baseline tagged run. Our ci.yaml is the workflow folder for parsing the PR comment and running compare_runs.py.",
    "64bbb931-62be-4e3e-9e93-1d31af6e2be2": "## What is the TimesNet Architecture?\nThe TimesNet architecture introduces a novel approach to time series analysis by transforming one-dimensional (1D) time series data into a two-dimensional (2D) tensor format. This transformation is designed to capture complex temporal variations more effectively.  \n\nFor each distinct period present, TimesNet uses a custom module called TimesBlock to transform the one-dimensional time series data into a two-dimensional representation. This 2D transformation allows TimesBlock to jointly capture the variations within a period and variations between periods efficiently using an inception-style block with shared parameters. TimesNet leverages its modular architecture with periodicity-specific TimesBlocks to model complex temporal patterns effectively by converting 1D time series to 2D and learning intraperiod and interperiod variations simultaneously.  \n\n## Transform 1D-Variations into 2D-Variations  \n\n## What Is TimesBlock?  \n\n## Adaptive Aggregation  \n\n## Generality in 2D vision backbone  \n\n## Code Example",
    "5a44a38d-b0a1-4745-b264-b65b860fabda": "## \ud83d\udd75 What is AgentInstruct?\n\nLet's move on a moment and talk about another paper, [AgentTuning: Enabling Generalized Agent Abilities for LLMs. ](https://arxiv.org/abs/2310.12823) We're going to use the data here on our Zephyr model. The AgentInstruct paper introduces a couple things.  \n\n* AgentTuning: A general method to enhance agent abilities in LLMs while maintaining generality\n* AgentInstruct: Instruction-tuning dataset with high-quality interactions\n* AgentLM-7B/13B/70B: agent-tuned LLMs  \n\n### \ud83d\udd27 Method  \n\nThe first step in the AgentTuning process is creating the dataset, AgentInstruct. The second step is their instruction-tuning method. This diagram will make sense in a couple minutes, I promise!  \n\n## Creating AgentInstruct  \n\n## AgentTuning",
    "38c6c781-e0f6-4aa8-b1c5-d16619bc9b01": "## Data Preparation and Annotation\n### Annotating a Summarization Dataset for Fine-Tuning\n\nThere is a given format required by ChatGPT in order to fine-tune the model on. This format includes 3 sections:  \n\n* System: This is the prompt that you will pass to ChatGPT. In our case, the prompt would be \u201cGPT is a great and to-the-point dialogue summarization tool.\u201d\n* User: This is the question asked to the model. In our case, it would be the text that we are required to summarize.\n* Assistant: This is the answer that our model would return. In this case, it would be a brief summary of the text.",
    "34035153-18a9-4b80-ac62-e77cb357bd1a": "## Model CI/CD for Enterprise-Grade Production ML: An LLM Example\n* The practitioners or prompt engineers build, train, and iterate on new models, running experiments, tracking metrics and visualizing results.\n* The MLOps engineer provisioning infrastructure and environments, and managing and deploying models\n* Team lead or business stakeholder approving models, ensuring alignment across the company, and tracking how end users interact with the model  \n\nThese various team members all work together to deploy models to production, get fresh data, see how users interact with the model, get more data, fine-tune, retrain the model, deploy again, and on and on. This ongoing Model CI/CD loop is a critical element in delivering enterprise-grade production ML at scale.  \n\nThese personas also need tools at each stage, and all the parts of the process and their supporting tools need to connect and work together seamlessly.",
    "7cf2411d-8f82-41f5-a961-c65034a3b2c8": "## Applications of Q-learning\n* The agent can be the program controlling a robot. In this scenario, the agent observes the environment (the real world) through sensors like cameras and touch sensors and acts accordingly by sending signals to the motors. It gets positive rewards for efficient navigation towards the goal and negative rewards for detours or time wastage.\n* The agent can be the program playing a board game like Go or chess.\n* The agent can be the program controlling Ms. Pac-Man where the environment is a simulation of the Atari game, actions are the nine joystick positions, and the rewards are the game points.\n* The agent can observe stock market prices and take action on whether to buy or sell.\n* The agent can be a smart thermostat that learns to understand human needs, getting positive rewards whenever it is close to the target temperature and saves energy, and getting negative rewards when humans need to tweak the temperature.\n* The agent can be a program solving a maze where it gets negative rewards for every time step, so it has to reach the exit as soon as possible.\n* There are various other tasks where it is well suited, such as driving cars, recommender systems, or placing ads on a web page.",
    "2620b795-69ee-4c1e-98ac-bda97a0ccd98": "## Challenges and Limitations of Q-learning\n* Slow Convergence and High Computational Requirements - Q-learning can take significant time to converge, especially in complex environments. It may require substantial computational resources, making it less feasible for real-time applications.\n* Curse of Dimensionality - The performance of Q-learning can deteriorate in high-dimensional state and action spaces, leading to increased computational complexity and reduced efficiency.\n* Lack of Generalisation - Q-learning tends to focus on specific states and actions, potentially leading to difficulties in generalizing learned policies to new, unseen environments and increased susceptibility to overfitting.\n* Exploration vs. Exploitation Trade-off - Striking the right balance between exploration (trying new actions) and exploitation (choosing the best-known actions) can be tricky. Over-exploration can lead to inefficiency, while under-exploitation can prevent discovering better strategies.\n* Handling Continuous State and Action Spaces - Q-learning is primarily designed for discrete state and action spaces. Adapting it for continuous spaces involves complex discretization techniques and can lead to suboptimal results.\n* Sensitivity to Hyperparameters - Q-learning's performance can depend highly on the choice of hyperparameters, such as the learning rate and discount factor. Finding the right values can be challenging.\n* Lack of Prior Knowledge - Q-learning doesn't incorporate prior knowledge about the environment, making it less efficient when some level of pre-existing understanding is available.\n* Non-Markovian Environments - Q-learning assumes that the environment follows the Markov property, meaning the future depends only on the current state and action. In non-Markovian environments, it may not perform optimally.",
    "ce964329-e1c3-4e74-b0e2-8d6cd0c63d0c": "## Understanding our distribution of user queries\nThe query is the questions asked by W&B's discord community. The response is generated by various beta versions of WandBot. The feedback is collected as emoji (\ud83d\udc4d\ud83c\udffb/\ud83d\udc4e\ud83c\udffb) reacts to the response mostly by the person who asked the question.  \n\n### How many users gave a feedback (\ud83d\udc4d/\ud83d\udc4e)?  \n\nOut of 872 questions, most did not receive any feedback. But with 187 thumbs ups and 74 thumbs downs, we've got 261 questions with feedback\u2014that's a solid number for RAG evaluation.  \n\n## Preprocessing the user queries  \n\nFirst, let's clean up our data here. For starters, most queries start with a substring \"@wandbot (beta)\" which is an unnecessary information. We also see a few queries like \"@wandbot (beta) are you there?\" which, again, is not a relevant question.  \n\nWe can also do simple deduplication with df.drop_duplicates. Note that this is a good first way of removing exact same strings from the dataframe.  \n\nThe preprocessed data is shown below:  \n\n## Can we count tokens to filter out bad samples?  \n\n## Final thoughts",
    "94948376-20c8-47ca-9c09-b07d6d8191ff": "## How to Teach Your Computer Japanese\n### Hyperparameters Galore\n#### Learning Rate\nNow, before you get all giddy and pick 1e-01 as the learning rate, I\u2019ll have you know that it\u2019s NOT the best choice.  \n\nThat\u2019s because fastai implements a smoothening technique called exponentially weighted averages, which is the deep learning researcher version of an Instagram filter. It prevents our plots from looking like the result of giving your neighbors\u2019 kid too much time with a blue crayon.  \n\nSince we\u2019re using a form of averaging to make the plot look smooth, the \u201cminimum\u201d point that you\u2019re looking at on the learning rate finder isn\u2019t actually a minimum. It\u2019s an average.  \n\nInstead, to actually find the learning rate, a good rule of thumb is to pick the learning rate that\u2019s an order of magnitude lower than the minimum point on the smoothened plot. That tends to work really well in practice.",
    "c4831068-196b-407f-a0cf-2bcf9532325e": "## Fundamentals of Neural Networks\n### 1. Basic Neural Network Structure\n#### Hidden Layers and Neurons per Hidden Layers\n* The number of hidden layers is highly dependent on the problem and the architecture of your neural network. You\u2019re essentially trying to Goldilocks your way into the perfect neural network architecture \u2013 not too big, not too small, just right.\n* Generally, 1-5 hidden layers will serve you well for most problems. When working with image or speech data, you\u2019d want your network to have dozens-hundreds of layers, not all of which might be fully connected. For these use cases, there are pre-trained models (, , ) that allow you to use large parts of their networks, and train your model on top of these networks to learn only the higher order features. In this case, your model will still have only a few layers to train.\n* In general using the same number of neurons for all hidden layers will suffice. For some datasets, having a large first layer and following it up with smaller layers will lead to better performance as the first layer can learn a lot of lower-level features that can feed into a few higher order features in the subsequent layers.",
    "b7126e3b-916d-42c5-b030-50e33ed3eee7": "## What Is Baysian Hyperparameter Optimization?\n### Expected Improvement\nExpected improvement works by introducing a threshold value for the objective function, and we are tasked to find a set of hyperparameters that beats that threshold. So, mathematically it would be -  \n\nwhere -  \n\n* y^* is a threshold value for the objective function\n* y is the actual value of the objective function\n* p(y|x) is the surrogate model\n* The above equation enforces the following:  \n\n> If for a certain x (a combination of hyperparameters) p(y|x) becomes such that y^* > y then it means that hyperparameter combination will not yield better score than the threshold but if it\u2019s the opposite then it\u2019s worth pursuing that combination of hyperparameters.  \n\nThe last piece of the puzzle remains to be added however - the surrogate model.",
    "f8d42d0a-9d2e-495b-9bd8-e420dac54256": "## Improving Deepfake Performance with Data\nTo illustrate the difficulty of the challenge, here is an example of what we would achieve with default settings (the red box shows which part of the image is generated).  \n\nWe can notice a few issues:  \n\n* skin tone is pretty well faded but zooming in, we can clearly see the differences\n* the hair and beard are extremely difficult to reconstitute, even more due to the section cropped of the face which may include only one side  \n\nIn addition, the long hair vs short hair actually causes issues to define well the contour of the face to reproduce.  \n\nThose are often solved (at least partly) by:  \n\n* using people with same skin tone\n* using people with similar facial hair\n* limiting the samples to front faces only (or only slight angles to the sides)\n* when used on people with long hair, they often have a clear face (no hair coming to the front of the face)  \n\nIn this case, we are going to keep trying to improve the results with the two people selected.",
    "3fc17a14-4b7b-42d1-9a48-3780e5e245ec": "  \n\n# Customize the scatter plot  \n\nDescription: Visualize many different experiments and spot trends  \n\nBody:  \n\nWe added some great new features to the scatter plot!  \n\n* Plot the min, max, and average\n* Custom metadata tooltips\n* Control point colors\n* Set axes ranges\n* Switch axes to log scale  \n\nHere\u2019s an example of validation accuracy of different models over a couple of weeks of experimentation. The tooltip is customized to include the batch size and dropout as well as the values on the axes. There\u2019s also a line plotting the running average of validation accuracy.  \n\n[See a live example \u2192](https://app.wandb.ai/l2k2/l2k/reports?view=carey%2FScatter%20Plot)  \n\n'",
    "623b3e8f-134e-4e22-b9bc-9dc396e98936": "## Performing In-Context Training\n### Demonstration Design\n#### Scoring Function\nThe takeaway from these different methods is that there is still a lot of work to be done in creating a scoring function that mitigates sensitivity and reduces bias. In a nutshell, it isn't easy to calibrate how well in-context learning performs. This field is still very new, and standard metrics have not been established.  \n\nAbove is a table of factors that play into in-context learning.  \n\nThey've concluded the following findings in the pretraining stage:  \n\n* domain source is more important than corpus size\n* corpora related to downstream tasks don't necessarily improve ICL ability\n* lower perplexity != better ICL\n* ICL emergent ability comes after some number of pretraining steps and model size  \n\nDuring the inference stage:  \n\n* input-label formatting matters\n* exposure of label space (what labels you use as examples)\n* input distribution\n* order of examples\n* examples with embeddings close to query embedding  \n\nThe takeaway from numerous works attempting to understand why in-context learning works is:",
    "0986f055-4abd-4f1c-901c-4a30d22fefeb": "## \ud83e\udd84 A Brief Overview of Stable Diffusion XL\n* SDXL leverages a larger UNet backbone. Three times the size, in fact. The increase in parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder.\n* SDXL leverages multiple novel conditioning schemes and is trained on multiple aspect ratios.\n* The image generation pipeline also leverages a specialized high-resolution refiner model which is used to improve the visual fidelity of samples generated by SDXL using an image-to-image diffusion technique.\n* The base diffusion model generates initial latent tensors of size 128x128, which can be passed through a  loss to generate the high-resolution image.\n* The latent tensors could also be passed on to the refiner model that applies , using the same prompt. Although the base SDXL model is capable of generating stunning images with high fidelity, using the refiner model useful in many cases, especially to refine samples of low local quality such as deformed faces, eyes, lips, etc.\n* SDXL and the refinement model use the same autoencoder.",
    "e766a48e-d572-450a-a73f-137bc18d563a": "## Setting up Evaluation using LlamaIndex\n### Generating Questions using LlamaIndex\n* Evaluating response for hallucination: Is the generated response coming from the provided context, or is it making up things?\n* Relevance of the retrieved chunks: Evaluate each retrieved chunk (node) against the generated response to see if that node contains the answer to the query.\n* Evaluating the answer quality: Does the query + generated response come from the provided context?  \n\nUsing DatasetGenerator is easy, and one can pass the loaded documents to the DatasetGenerator.from_documents method. Calling generate_questions_from_nodes() on the object's instance will generate N questions per chunk. The default chunk size is 512, and N is 10. You might quickly realize that it will take a long time and a lot of API calls to generate a lot of questions. Let's customize the data generation process.",
    "6fcb6a6e-9a67-4f12-bd44-0453b2f7e792": "## Tips for using clustering algorithms\n\n## Choosing the right number of clusters  \n\nChoosing the right number of clusters is a challenging task in clustering analysis. Relying solely on statistical techniques may not always yield accurate results.  \n\nTo address this, leveraging domain knowledge or prior understanding of the data is crucial in estimating the expected number of clusters. Contextual information guides the selection process for more meaningful results. Practices like visually inspecting clustering outcomes, utilizing algorithms with automated estimation, and employing ensemble methods for stability analysis can assist in making informed decisions. Balancing interpretability and complexity is important when selecting the number of clusters.  \n\nHaving said that, validating the chosen number through expert feedback or real-world application ensures its relevance and usefulness.  \n\n## Preprocessing the data correctly  \n\n## Interpreting the clustering results",
    "663edc1a-a910-471b-90f6-ed59edfcca2b": "## Introduction\n### What is a token?\n#### Python Implementation\nThe LogitBias class is a wrapper around the OpenAI API which can be used to explore token banning. This is a simple script which applies the same bias to a list of phrases.  \n\n_augment_phrases  \n\n* This method enhances the suppression list by countering the model's typical workarounds. When a token is barred, the language model frequently defaults to case variations of the token or versions with additional spaces  \n\n_create_logit_bias  \n\n* Tiktoken is a fast tokenizer from OpenAI which will convert a phrase into its token representation. We initialize this in the LogitBias constructor (`self.encoding = tiktoken.encoding_for_model(self.model)`), where the encoder is selected based off the specified model.\n* This function constructs the dictionary of token-bias pairs to use in the API call.  \n\ngenerate_response",
    "b32912e6-133a-4a2d-a0b1-48fdabe6b84e": "## LLMs Evaluating LLMs\n### 1. Generate Eval Dataset Using An LLM\n* It's scalable. We can generate a vast number of test cases on demand.\n* It's flexible. The test cases can be generated for special edge cases and adapted to multiple domains, ensuring relevance and applicability.\n* It's cheap and fast. LLMs can quickly collate information from multiple documents at a far lower price.  \n\nAs for limitations, we covered the biggest above: use cases where you need expert labelers are the sorts of use cases you most often want them (i.e., in the medical domain).  \n\n### Langchain's QAGenerationChain",
    "6ba47446-32db-4aae-9070-f9e3ec449e56": "## Solving Unconstrained Optimization Problems\n\n## Overview of Unconstrained Optimization  \n\nUnconstrained optimization problems are of the following general form:  \n\n$$\n\\large \\text{min} \\, f(x) \\hspace{2em} x \\in \\mathbb{R}^n\n$$  \n\nThe main question in the study of optimization is how to characterize a local minimum. This characterization allows us to develop computational techniques for solving these problems. There are two conditions that we need:  \n\n* Necessary Condition: conditions that are necessary but not sufficient for a point to be a maxima or a minima\n* Sufficient Condition: conditions that are sufficient for a point to be maxima or a minima  \n\n## Overview of algorithms for unconstrained optimization  \n\nThere are a number of algorithms available to us for solving unconstrained optimization problems :  \n\n* Nelder-Mead Simplex method: (`'nelder-mead'`)\n* Broyden-Fletcher-Goldfarb-Shanno method: (`'BFGS'`)\n* Newton-Conjugate Gradient method: (`'Newton-CG'`)  \n\n## Examples and use cases",
    "88d96360-99c7-4899-831b-18188a41276b": "## Practical Tips and Best Practices for Using SciPy Optimize\n\n## Preprocessing and scaling data  \n\nPreprocessing and scaling work great for optimization algorithms. For instance, in unconstrained optimization, we mostly don't know the value of the function at all points of the domain. Or even if we do, computing them would be expensive. Thus if we preprocess and scale the data within a certain range, it helps us optimize quicker.  \n\n## Handling noisy or uncertain objective functions  \n\nIf the phenomenon we're modeling is noisy either due to errors in measurement or the nature of the phenomenon itself, the most common approach used to optimize such objective functions is a two-fold approach.  \n\n* A shallower step aimed at identifying sub-regions where the minima could be.\n* A much deeper step aimed at finding the local minima of the identified subregions.  \n\n## Efficient usage of optimization algorithms and parameter tuning",
    "2a33a443-8489-486d-9e33-8883425f4a69": "## Tutorial: Sentiment Analysis\n### Choosing an algorithm\n\nOk let's build a classifier. But first - how do we choose an algorithm? There are a number of popular methods for different use cases.  \n\nThe choice of algorithm is tough for beginners and argued about by experts but I think a great rule of thumb is to use this excellent flowchart made by scikit.  \n\nIf you generally walk through this flow chart, you will get to a reasonable algorithm. Let's start at the top. We have greater than 50 samples of data, and are predicting a category. We have labelled data, and less than 100,000 samples (we have around 10,000). It then recommends a linear SVC. Linear SVC for this case doesn't actually work very well, so if we follow the 'not working arrow', we get to Naive Bayes.  \n\nNaive Bayes is a simple algorithm that generally does work really well and it works very fast. Let's try it out on our dataset.  \n\nOpen up classifier.py. From scikit learn we just import the multinomial naive bayes algorithm using the following code:",
    "d6f8fc41-f891-43d4-a45e-d827a10938f5": "## Applying Ethical Principles & Scoring to Langchain & Visualizing with W&B Prompts\n### \ud83c\udfad Creating the Custom BadActorChain Workflow with Langchain\n#### Using Guardrails to standardize our scoring\n2. Corrective Actions: When the output generated by the LLM does not meet the specified criteria, Guardrails can take corrective actions to guide the model towards producing a more suitable output. These actions include re-prompting the LLM, truncating the text, or even replacing certain elements within the output.  \n\n3. Enforcing Structure and Type Guarantees: Guardrails uses a specialized markup language called RAIL (Reliable AI markup Language) to define the structure and types of the desired output. By specifying a RAIL schema, developers can ensure that the generated output adheres to the structure and types defined, making it more organized and easier to process.",
    "85ae6d59-2b8a-408f-a299-02c853f72d70": "## Working with PyG\n### Learning Methods on Graphs\nPyTorch Geometric provides various graph neural network layers and architectures that can be easily integrated with the PyTorch framework. Some popular layers include:  \n\n* `torch_geometric.nn.GCNConv`: Graph Convolutional Network (GCN) layer.\n* `torch_geometric.nn.GATConv`: Graph Attention Network (GAT) layer.\n* `torch_geometric.nn.ChebConv`: ChebNet layer based on Chebyshev polynomials.\n* `torch_geometric.nn.SAGEConv`: GraphSAGE layer for inductive learning on large graphs.  \n\nThese layers can be used to build custom graph neural network architectures. In the forward pass of the network, the non-linearity is not integrated into the convolutional layers and needs to be applied afterward. This design choice allows for greater flexibility and is consistent across all operators in PyG.",
    "9f511ac9-13ed-4170-bf0b-0c806be5fe84": "\"  \n\n# Testing GTP3.5 vs. GPT4: Which Model Writes Better Code?  \n\nDescription: In this article, we compare outputs from GPT-3.5_turbo and GPT-4, and explore how to use GPT-4 as a code assistant, using a simple CLI termGPT to access the models.  \n\nBody:  \n\nThis article will compare model outputs from gpt3.5_turbo and gpt4 using the OpenAI API. We will refer to gpt3.5_turbo as [GPT-3 ](https://wandb.ai/fully-connected/blog/gpt3)from now on. I'm using my own simple [CLI termGPT](https://wandb.ai/capecape/termgpt/reports/termGPT-Interacting-with-openAI-s-chatGPT-on-your-terminal--VmlldzozNjk0ODgw) to access the models.  \n\nActually. That introduction could be better. Let's ask GPT3.5 and GPT4 to write it instead. Here's our prompt:  \n\n>  \n\n## Introduction by GPT-4  \n\n## Introduction by GPT-3.5  \n\n## A Day With an MLE Using ChatGPT for Code Completion  \n\n## GPT Knows Weights & Biases  \n\n## What Happens With Brand New Libraries Like PyTorch 2.0?  \n\n## Conclusions",
    "6893f1bf-360e-4c0d-a159-cbdfc6b79040": "## How to Train Your Dragons Models\n### Zero-DCE: Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement\n#### Non-Reference Loss Functions\nwhere  and  represent the horizontal and vertical gradient operations, respectively.  \n\nSpatial Constancy Loss: The spatial consistency loss encourages spatial coherence of the enhanced image by preserving the difference of neighboring regions between the input image and its enhanced version. The Spatial Constancy Loss is given by:  \n\n$$\n\\Large{L_{s p a}=\\frac{1}{K} \\sum_{i=1}^K \\sum_{j \\in \\Omega(i)}\\left(\\left|\\left(Y_i-Y_j\\right)\\right|-\\left|\\left(I_i-I_j\\right)\\right|\\right)^2}\n$$  \n\nwhere...  \n\n*  is the four neighboring regions (top, down, left, right) centered at the region\n*  and  denote the average intensity value of the local region in the enhanced version and input image, respectively.  \n\nAll these loss functions have been implemented as part of the [restorers.losses](https://github.com/soumik12345/restorers/tree/main/restorers/losses) API and are automatically initialized when calling [model.compile](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile).",
    "1fa8d381-69f6-430d-a57e-6bb8ad6817b4": "## Use a Commercial LLM's API\n* Commercial LLM services can get expensive with a high volume of fine-tuning or inference tasks. It comes down to LLM total-cost-of-ownership (TCO) amortized to each inference.\n* Many industries / use cases forbid the use of commercial LLM services as sensitive data / PII data cannot be seen by the service for compliance (healthcare use cases, for example).\n* If building external apps, you\u2019ll need to find other moats and de-risk your business if you\u2019re highly reliant on external LLM service technology.\n* Less flexible downstream: doesn\u2019t support edge inference, limited ability to customize the model (fine-tuning gets expensive), limited ability for",
    "ad3aaa7b-13b1-4e52-8e51-6e8c86ac5ffd": "## Named Entity Recognition Techniques\n### Statistical and Probabilistic Models\n#### Hidden Markov Models (HMM)\nWhile the independence and Markov assumptions simplify the modeling process and make it computationally efficient, they can also lead to limitations in the model's ability to capture complex dependencies in the data. For example, the independence assumption may not hold in cases where the observations are correlated, such as in language modeling where the context of a word can affect its meaning. Similarly, the Markov assumption may not hold in cases where the current hidden state depends on multiple previous hidden states or observations, such as in some types of sequential data.  \n\nHMMs are constrained to discrete states and rely solely on the previous state, making it challenging to create a state that functions based on multiple others. Additionally, HMMs have limited feature options, which restricts their ability to utilize the entire sequence's context. As a result, this restricts their effectiveness when the context of the entire sequence is crucial, as opposed to solely the previous state.",
    "6fb05009-32f2-41c9-94dc-fe6d3e7df621": "## Example App: NewsTrackr\n\nIn this section, we'll learn how to build a web app that can extract NEWS data on a given topic (stock/index in our case) using NEWS API, then perform Aspect Based Sentiment Analysis (ABSA) to determine the sentiment of different aspects related to a stock or an index. We'll use [AI21](https://studio.ai21.com/overview) to build our model, [Streamlit](https://streamlit.io/) as our web framework, and [NewsAPI](https://newsapi.org/) to collect news articles.  \n\nBut wait, why do we need to determine the sentiment of different aspects related to a stock? More importantly, what's ABSA?  \n\n## Problem Statement and Workflow  \n\n>  \n\nLet's say that a friend of mine is new to trading and needs some guidance on how to make informed trading decisions.  \n\nThe right way to start trading would involve understanding the basics of technical and fundamental analysis, which are two primary methods used to analyze the markets and make informed investment decisions.  \n\n## ABSA Model Fine-Tune Pipeline",
    "37b0719f-ad97-4d7c-9a5c-c3bbdb29b23c": "## Music Generation with Google's MusicLM\n#### How does it work?\nSoundStream is an end-to-end encoder-decoder framework with Residual Vector Quantization (RVQ) which you can think of as a method to improve bitrate and the quality of the actual audio representation. W2v-BERT combines an NLP training framework called Masked Language Modeling (MLM) with a contrastive loss to train an LM that can both learn good speech representations and how to contextualize this in speech.  \n\nWhile these 2 models focus mainly on audio, MuLan focuses on jointly training an LM contrastively to understand patterns between audio and text (i.e. is this piece of text associated with this audio?).  \n\nThese 3 models are organized into what they call a hierarchical sequence-to-sequence training task shown below.  \n\nThe diagram on the left details training and the right one details inference.",
    "48fced0a-a9e0-4da4-b1eb-2c7a4ba63aa5": "## How To Use LSTM for Sequence Classification\nThe LSTM layers process the input sequence and generate hidden states that capture the context of the input sequence. The output layer then takes the hidden states and generates a probability distribution over the possible class labels (step 7 in the example model).  \n\nThe model is then trained on a labeled dataset using a supervised learning algorithm. The model is presented with input sequences and their corresponding class labels, and the weights of the model are adjusted to minimize the classification error.  \n\nAfter training, the LSTM model can classify new, unlabeled sequences. This is done by passing them through the input layer and the LSTM layers and then using the output layer to generate class labels.  \n\nIt's worth noting that LSTMs can be combined with other architectures, such as CNNs, and transformer-based models, such as BERT, to improve their performance.",
    "00fe7a20-91ab-426a-9c6e-13db04fb351e": "## What Is Language Modeling?\nBased on the training data we used, the model would assign high probabilities to certain words that usually follow this phrase, such as \"store\" (0.8), \"park\" (0.6), \"beach\" (0.4), and \"movies\" (0.2).  \n\nThe model would also assign probabilities to different verb forms that have a higher chance to come next, such as \"buy\" (0.7), \"visit\" (0.5), \"enjoy\" (0.3), and \"watch\" (0.1).  \n\nThe model then selects the word with the highest probability - \"store\", and assigns it to the next word in the sentence.  \n\nThe model will then repeat the process with the updated context \"I am going to the store\" and assigns probabilities to different words and verb forms that might come next, such as \"to buy\" (0.8), \"to look\" (0.6), \"to grab\" (0.4), and \"to eat\" (0.2).  \n\nThen the model selects the words with the highest probability, in our case, \"to buy,\" and assigns it to the next word in the sentence.  \n\nThe final sentence constructed by the model would then be, \"I am going to the store to buy.\"  \n\n>",
    "23f9d53e-4c29-4910-872c-505383867808": "## Types of Language Models\n### Neural Language Models\n#### LSTM Language Model\n* The input gate controls the new information to add to the memory cell. It uses a  to determine which values from the current input should be added to the memory cell.\n* The forget gate controls the amount of old information to forget. It also uses a sigmoid function to determine this.\n* The memory cell stores the information from the current and previous inputs, processed by the input and forgets gates.\n* The output gate controls the amount of information to output as the prediction. It uses a sigmoid function to determine this, followed by a tanh activation function to squish the values between -1 and 1.  \n\nThis allows LSTM to effectively handle long sequences of data and make predictions based on the context of the entire sequence.  \n\nThe cell state here is a  memory cell that maintains information across time steps in a sequence, and the hidden state is the output of the LSTM unit at each time step that is passed to the next time step in a sequence.",
    "f2dc7102-697f-44ca-87e3-96ff5348009b": "## Common Experiments in RL using OpenAI Gym\n### 3. BipedalWalker-v3\nBipedal Walker is a 4-Joint walker robot that has to cross through uneven terrain. The episode terminates if the robot\u2019s hull touches the ground.  \n\nAction Space: Box space with motor speed values ranging from -1 to +1 for each joint.  \n\nObservation Space: Box space with 17 positional values including hull angle speed and horizontal speed.  \n\nReward Function: The robot gets up to 300 points if it reached the farthest end, it receives -100 if it falls.  \n\nThe episode terminates if the robot gets 300 points in 1600 timesteps or when the hull touches the ground  \n\nWe will follow the same boilerplate code, only changing the env_name and total_timesteps",
    "fb74812b-3bc1-46cd-859e-dea2b79920c2": "## Point-E: OpenAI's Open-Source Text-To-3D & Image-To-3D Diffusion Model\n### How does Point-E work?\nWhere many text-to-3D models convert text prompts to 3D objects directly, Point-E opts to split the process into two steps for text-to-image and image-to-3D. This split architecture was chosen because it is more scalable, less reliant on hard-to-come-by 3D object datasets, and takes advantage of the already-established flexibility of cheap-but-powerful text-to-image generation models.  \n\n### Point-E's training dataset  \n\nTo make a model, you need a dataset. For Point-E, this means a dataset built from 3D models. Though this is a model for 3D object generation, the final dataset actually only contains 2D renders and 3D point cloud data.  \n\nFirst, they collected several million 3D models, which varied wildly in quality and format. These models were fed through an image rendering pipeline carefully constructed to make output images as consistent as possible (in size, lighting, etc), and in the end, each model had 20 renders produced of them from various angles.  \n\n### Point-E's text-to-image internal model",
    "b617a5f1-62d1-49f6-a39e-5d68dacc7661": "## Methods for Automated Hyperparameter Optimization\n### Grid Search\nGrid search is a hyperparameter tuning technique that evaluates all possible hyperparameter combinations in a specified grid (Cartesian product). It is a brute-force approach recommended only for ML models with few hyperparameters.  \n\n### Inputs  \n\n* A set of hyperparameters you want to optimize\n* A discretized search space for each hyperparameter either as specific values\n* A performance metric to optimize\n* (Implicit number of runs: Because the search space is a fixed set of values, you don't have to specify the number of experiments to run)  \n\n(The differences between random search and Bayesian optimization are highlighted in bold above.)  \n\nA popular way to implement grid search in Python is to use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) from the [scikit learn](https://scikit-learn.org) library. Alternatively, as shown below, you can set up a grid search for hyperparameter tuning with W&B:  \n\n### Steps  \n\n### Output  \n\n### Advantages",
    "33682393-fc61-49bd-9cd3-81474a0ce6d0": "\"  \n\n# Digging Into the ShapeNetCore Dataset  \n\nDescription: In this article, we dive into the ShapeNetCore Dataset for the classification and segmentation of point cloud data and explore how to use it using Weights & Biases.  \n\nBody:  \n\nIn this article, we cover the ShapeNetCore Dataset for the classification and segmentation of point cloud data. We also cover how to explore it using Weights & Biases.  \n\nHere's what we'll go over:  \n\n## Table of Contents  \n\nLet's get started!  \n\n# What Is ShapeNet?  \n\n[ShapeNet](https://shapenet.org/) is an ongoing effort to establish a richly-annotated, large-scale dataset of 3D shapes. ShapeNet provides global researchers with this data to enable research in computer graphics, [computer vision](http://wandb.ai/fully-connected/blog/computer-vision), robotics, and other related disciplines.  \n\nThe project is a collaborative effort between researchers at Princeton, Stanford, and TTIC.  \n\n# The ShapeNetCore Subset  \n\n# Dataset Usage  \n\n---  \n\n# Leaderboards  \n\n---",
    "b125afa7-8c49-4e05-bd09-1f05824767e7": "## AlphaCode: DeepMind's Code-Competition AI Solves Problems At Human-Level Competency\n### How does AlphaCode work?\n\nThe core of AlphaCode is a pretty typical encoder-decoder transformer model, starting with natural language input and ending with programming language output. This is a good choice because it follows the format for coding competitions: solving a complex word problem with code.  \n\n### Training AlphaCode  \n\nAlphaCode went through two training steps: Pre-training with general code samples collected from GitHub, then fine-tuned with a custom made dataset of problem-solution pairs mimicking a code contest format.  \n\n### How AlphaCode generates solutions  \n\nAlphaCode's base model by itself wouldn't be very reliable at creating good answers, so it employs a few extra steps to make sure its output is competent. It's a similar process to many other language generation models, in which the model generates many samples which are filtered down to the best output.",
    "30e541ce-1a3f-41fa-ba24-061bee96d711": "## Transformers to the rescue\n### Is there a cost to poor tokenization?\n#### When tokenization goes badly\nWords that have corresponding in-vocab subword parts  \n\nIn these cases, the authors find that the similarly of in-vocab words, e.g. \u201ctime\u201d or \u201cworld\u201d, is similar to their hashed counterparts. In other words, \u201ctime\u201d is similar to \u201c##time\u201d and \u201cworld\u201d is similar to \u201c##world\u201d.  \n\nTo be precise, the authors find that, on average, the cosine similarity for these words is 0.66.  \n\nWhat does this mean? Well, the authors claim that when you have a word that has an in-vocab subpart then the high cosine similarity should make it easier for your model to get the right meaning from the context. The thinking being that a lower score would make it harder for the model to \"compensate\" from the surrounding context.",
    "dd3a9f31-59c4-40ca-a4d1-fceff28260eb": "## A Deep Dive Into Paella\n### Disadvantages of Existing Models\n* Transformers usually employ a spatial compression to a low-dimensional space before learning, which is necessary due to the self-attention mechanism growing quadratically with latent space dimensions.\n* Transformers treat images as one-dimensional sequences by flattening the encoded image tokens, which is an unnatural projection of images and requires a significantly higher model complexity to learn the 2D structure of images.\n* The auto-regressive nature of transformers requires the sampling of one token at a time, resulting in long sampling times and high computational costs.\n* Diffusion models, unlike transformers, can effectively learn at the pixel level but by default require a tremendously high number of sampling steps as well.",
    "000c87b7-fb30-4dcd-8c73-deb4227f54b3": "## Downstream Applications of Paella\n### Inpainting and Outpainting\nIn addition to semantically extending an image, we can also perform inpainting, i.e, replace and fill in existing content based on a textual prompt. The overall procedure is as follows:  \n\n* A mask is defined in the latent space for tokens that should be resampled.\n* Then we start sampling and after each iteration only keep the tokens at the inpainted positions.  \n\n### Storing Inpainting and Outpainting Results as [Weights & Biases Tables](https://docs.wandb.ai/guides/data-vis/log-tables) (Click to Expand) \ud83d\udc49",
    "b309e553-6373-4473-8647-e3511d0ff10f": "## An Introduction To HuggingFace Transformers for NLP\n### What Are Transformers in Machine Learning?\nAfter converting our data into a more understandable format, the embedded data is passed into the next layer, known as the self-attention layer.  \n\nBy utilizing self-attention, a transformer is capable of detecting distant data relations and resolving vanishing gradients. Meaning that a given transformer model will still be able to study a given relationship between two related words even if both these words are too far away from each other in a given context.  \n\nThe self-attention process represents how relevant a specific word is in relation to its neighboring words in a given sentence. This relation is then represented as what we call an attention vector.  \n\nThere are three additional types of vectors created in the self-attention layer which are key, query, and value vectors. Each vector is then multiplied by the input vector in order to return a weighted value.",
    "b2f8e831-e27c-4bcf-96a5-9d8d9a670f6c": "## Challenges of Multimodal Generative Modeling\n### Heterogeneous Training Data\n\nThe challenges:  \n\n* Large models require huge datasets.\n* Paired image/caption datasets used in  and  may not be general enough to reach GPT-3 style few-shot learning.\n* Large internet-based text-only dataset exists, but not for multimodal data.\n* One approach is to scrape webpages with interleaved images and text. Despite the generality of the data, the images and text are often weakly related.  \n\nProposed approach: combine the interleaved dataset (which the authors created) with standard paired image/text and video/text datasets where the visual and language are typically more strongly related.  \n\nLet's run down the primary and innovative ideas of Flamingo as outlined by the authors in the paper before delving into each component in more detail.",
    "6e5e5fbd-3160-477c-b745-9d159bcd441f": "## MLPerf Results Published For November 9th Benchmarking Rounds\n### MLPerf November 9th rounds\n#### MLPerf Inference: Tiny v1.0\nUnlike the other two, this round tests inference on small-scale hardware scenarios, such as the processors onboard small internet-of-things devices. Here, the hardware is tested not just for speed, but also energy efficiency.  \n\nThe tasks for this round are also the same as they have been in previous versions:  \n\nThe results are available here: [https://mlcommons.org/en/inference-tiny-10/](https://mlcommons.org/en/inference-tiny-10/)",
    "9d57f347-d659-4cc9-a6d9-70f073d11cfb": "## Reinforcement Learning with Python\n\nNow that we have some background, let's try out hands at some reinforcement learning.  \n\n## The Environment  \n\nWe are going to test out the `CarRacing-v0` environment. We will be using [OpenAI Gym](https://docs.wandb.ai/guides/integrations/other/openai-gym) which offers a simple pythonic way of representing reinforcement learning problems. It also offers various environments out of the box, which reduces the time spent setting up the environment.  \n\nOur environment consists of a red race car (with powerful rear drive) and a track. The objective of the agent (car) is to maximize the cumulative rewards (points).  \n\nIt is easier to learn with continuous control, which is set as Default. There are three continuous actions: Steer, Gas, and Brake.  \n\n## PPO",
    "a51566f3-fc4f-4f3b-9120-b86c865ce7bb": "## What Is The T5 Transformer Model?\n### How Does The T5 Transformer Model Work?\n#### 2. Training dataset: C4 dataset\nColossal Clean Crawled Corpus, C4 is a 750 GB clean English text scraped from the Internet. This dataset was collected from Common Crawl, which is a publicly available web archive.  \n\nAfter extracting from Common Crawl, the authors then proceeded to remove offensive words, filler sentences(Loren Ipsum), Code Brackets \u201c{\u201d, Duplicates, and sentences that don\u2019t end with a punctuation mark.  \n\nIt is a clean and huge dataset, this meant the model can be trained on the dataset without ever repeating the same data.  \n\nThese were the most important features of T5, we will see how they perform in the next section.",
    "6655b201-03f1-43ca-b449-12a7b1f54f88": "\"  \n\n# The Woven Planet (Lyft) Level 5 Dataset  \n\nDescription: In this article, we'll be exploring the Woven Planet (Lyft) Level 5 dataset. We'll look at what it is as well as the autonomous vehicle tasks and techniques it supports  \n\nBody:  \n\n# What Is The Woven Planet Level 5 Dataset?  \n\n[The ](https://level-5.global/) is the largest [autonomous-driving dataset](https://wandb.ai/av-datasets/av-dataset/reports/The-Many-Datasets-of-Autonomous-Driving--VmlldzoyNjU1OTg0) for motion planning and prediction tasks. It contains over 1,000 hours of data collected by 20 self-driving cars and is annotated with semantic maps and high-definition aerial views.  \n\nThere are 15,242 labeled elements in the dataset for [autonomous driving-related machine learning tasks](https://wandb.ai/av-team/av-tasks/reports/The-ML-Tasks-Of-Autonomous-Vehicle-Development--VmlldzoyNTc2Nzkx), such as motion forecasting, motion planning, and simulation.  \n\n## What We're Covering About The Level 5 Dataset  \n\n# Recommended Reading  \n\n\"",
    "c7a49c45-dc75-4906-a88c-a4631fcbfbb2": "## The Levels Of Autonomous Driving\n### Autonomous Driving Level 4: High Driving Automation\n\nThis is the fifth level in the automation spectrum. At level 4 autonomy, there is no need for human intervention in driving. The system drives on its own, and we are tagging along for the ride, potentially eliminating the need for steering wheels and pedals.  \n\nOne major limiting factor to level 4 autonomous vehicles is the use of geofencing. These vehicles are geofenced to particular areas and won\u2019t be able to drive anywhere other than those places. Severe weather conditions would also affect these vehicles and would likely disturb their operation.  \n\n### Level 4 Summary  \n\n* Human requirements: Enter your destination and have some fun (maybe read a book, or put in some time on an installed rowing machine).\n* Vehicle features: Driverless Taxis and Transportation Systems.\n* Example: Waymo, Google\u2019s autonomous vehicles products are targeted to work at this level. As of writing, it is operating only in Phoenix.",
    "ffc92604-418f-48eb-b6ed-0417edeb7391": "## The Levels Of Autonomous Driving\n### Autonomous Driving Level 5: Full Driving Automation\n\nLevel 5 is the highest level in the autonomous driving spectrum. These vehicles can drive themselves in all conditions. They are not bounded by geofences and can travel anywhere. They can safely transport humans in severe weather and on damaged roads.  \n\nCurrently, there are no real examples of this level outside of science fiction (say, the iconic Batmobile). Using GPS, the Batmobile can track Batman and reach him when the need arises. It can also easily move through traffic, damaged roads, and severe weather.  \n\n### Level 5 Summary  \n\n* Human Work: Enter the destination and relax, read books or sleep.\n* Vehicle Features: Driverless transportation with the ability to go anywhere.\n* Example: Batmobile and Lexus 2054.",
    "90fb50fe-a736-461f-905e-5bbeecf37805": "## What's Hot In NLP: How To Participate\n### Why participate?\n* Shine a light on the best NLP techniques through experimentation.\n* Grow your skills. Writing things down helps you to organize what you\u2019ve learned, which often lets you uncover gaps in your knowledge.\n* Meet like-minded people and grow your network. Join a , helping one another learn about all things Machine Learning and MLOps.  \n\nWe\u2019ll be giving out prizes including GPU credits and W&B merch. We want to encourage high-quality work as much as we can so we\u2019ll reward submissions based on practicality, creativity, clarity, and usefulness for the W&B community.",
    "15f037a2-dc6d-456d-b42e-d9facc8641aa": "## Building Four ML-Powered Language Applications with Bloom LLM\n### What Is Bloom LLM?\n* It is trained on 46 natural world languages and 13 programming languages\n* This multi-language approach gives Bloom a more inclusive worldview than GPT-3, , or\n* The training corpus is of size 1.6 TB in pre-processed text, which was later converted into 350B unique tokens\n* It was trained continuously for 117 days (!)\n* The model is released with Big Science RAIL (or Responsible AI License), which means that if you fine-tune this model or otherwise use it, you will have to release your work under the same license. This license also prohibits using the Bloom model for certain things like generating text that could violate federal or state laws and publishing generated text without disclaiming that the text is machine-generated, using Bloom to harass or impersonate others, and so on.",
    "cd4454e7-1e03-4c91-a804-045e6aa71881": "## Building Four ML-Powered Language Applications with Bloom LLM\n### Bloom-Powered Apps\n#### App 1: Step By Step With Bloom\n* I am providing simple mathematical word problems as examples. These word problems will be displayed as different radio buttons.\n* Further, I am providing another set of prompts as radio buttons which can be suffixed to a word problem selected in the previous step.\n* I am also initiating a text box in which users of this app can write their own prompts following my given examples.\n* Lastly, I am creating a button with on-click event as a call to my inference function and receiving the outputs in a separate text box.\n* All this is done using Gradio's intuitive Blocks API, which allows me to decide on a very neat layout for all my components in the app.",
    "d8e9ed6c-d8a0-44f9-b35a-bed78885976c": "\"  \n\n# PyTorch 1.12 Released  \n\nDescription: PyTorch 1.12 has released, bring a large list of changes and improvements.  \n\nBody:  \n\nThe newest version of [PyTorch](https://wandb.ai/fully-connected/blog/pytorch) has finally rolled out. PyTorch 1.12 comes with a huge list of changes, including new features and bug fixes galore. The last big update came out in March, so there's a lot to catch up on.  \n\n## What's new in PyTorch 1.12?  \n\nThere's way too much changed in this version of PyTorch to list everything here, so I'll try to summarize and highlight the important stuff featured in the update [blog post](https://pytorch.org/blog/pytorch-1.12-released/). You can take a look at the full GitHub changelog [here](https://github.com/pytorch/pytorch/releases/tag/v1.12.0).  \n\n## Find out more  \n\n[Read the update blog post for more details by clicking here.](https://pytorch.org/blog/pytorch-1.12-released/)",
    "3d52f225-0f22-4ef8-9d85-8cf980f2a19f": "## Dedicated Weights & Biases Hook\n### Checkpointing\nMMDetection uses MMCV's CheckpointHook to periodically save model checkpoints. The period is determined by checkpoint_config.interval. However, these checkpoints are saved locally and might get overwritten by a new experiment.  \n\nYou can reliably store these checkpoints as W&B Artifacts by using the log_checkpoint=True argument. By saving them as W&B Artifacts, you can easily transfer the checkpoints across machines, keep different model ideas separately, and compare them across variants.  \n\nHere are a few things worth noting:  \n\n* There are 3 versions of checkpoints in the UI as shown above. That's because the model was trained for 12 epochs with `checkpoint_config.interval=4`.\n* Each version has an alias `epoch_x` where `x` is the current epoch.\n* The last checkpoint is marked with the alias `latest`.  \n\nWe recommend you set the checkpoint interval with caution to save both local and W&B storage space.",
    "cc68ac06-f27c-4bcc-b095-1b399ff11bb8": "## Human Dynamics\n### Reward Design\n#### Good Alignment with Scene Objects\n* Distance:  ensures that the simulated end-effector (hands and feet) are in contact with the desired object. If  is the position of the end-effector and  is the target zone of the target object (in our case, an area of a quarter on the center of the surface), this reward is calculated using:\n* Alignment favors the alignment of the character and the object when in contact. If  is a unit vector along the frontal axis of the pelvis, this reward is calculated using:\n* Center of Mass:  informs the suitability of the trajectory of the character. If  is the distance between the center of mass and the end-effector on landing time and  is the distance between the expected center of mass and the expected landing position, this reward is calculated using:  \n\n>  \n\nThus, the total Scene Loss is given by:  \n\n$$\n\\huge r_{\\text{scene}} = w_{\\text{dist}}r_{\\text{dist}} + w_{\\text{align}}r_{\\text{align}} + w_{\\text{com}}r_{\\text{com}}\n$$",
    "f28b8915-689d-48f4-ad40-5feaf9e73d64": "## Potential for Negative Impact\n* NeRF-like models have recently been incorporated into generative modeling approaches such as . Generative modeling techniques, in general, can be used to synthesize deep fakes that could be used to mislead people. Although Mip-NeRF 360 doesn't directly concern generative modeling and instead aims to reconstruct accurate physical models of a scene from which new views can be generated, it may be useful for generative approaches that build on NeRF.\n* The ability to reconstruct accurate models of a scene from photographs may have modest potential negative impacts. Mip-NeRF 360 could conceivably be used to construct a surveillance system, and such a system could have a negative impact if used negligently or maliciously.\n* Mip-NeRF 360 could be used to generate visual effects (a task that is currently labor-intensive), and as such, it may negatively affect job opportunities for artists.\n* Training a NeRF is computationally demanding and requires multiple hours of optimization on an accelerator. This expensive training requires energy, and this may be of concern if that energy was produced in a way that damages the climate.",
    "86a7f322-4968-4992-96b6-faeb67ed3783": "## New Techniques for Generating Images With Text\n* Conditioning these models on text (with a few extra tricks like classifier-free guidance) turns them into text-to-image systems. See , for example.\n* Advanced sampling methods like  can cut down the number of steps required to generate an image from thousands to as low as 25 or 50. This is important since, normally, it is very slow to generate images with these models.\n* Working in the latent space of an autoencoder (rather than directly operating on pixels) cuts down the compute required to train these models, with the tradeoff that autoencoders might introduce visible artifacts.  \n\nWith these ideas combined, we get fantastic models like [CompVis' latent diffusion](https://github.com/CompVis/latent-diffusion) project. But even with fancy sampling methods, diffusion models are still a little slow to sample. Enter [Denoising Diffusion GANs from NVLabs](https://github.com/NVlabs/denoising-diffusion-gan):",
    "6f3b30fa-a585-475d-b3f9-8081c33ca1f9": "\"  \n\n# Finbots.AI Raises $3 Million To Bring AI Into Banking  \n\nDescription: In a round of Series A funding, Finbots.AI has raised $3 Million with plans to expand business and it's ZScore service for enhanding credit score calculation with AI.  \n\nBody:  \n\n[Finbots.AI](https://finbots.ai/) sees a spot in the market for the use of AI in banking, particularily in the fair calculation of credit scores. Many prospective borrowers might face difficulty with legacy platforms and practices which determine they're too high risk, when they're actually perfectly suitable to borrow  \n\nFinbots.AI is offering a solution, bringing what they claim is a fair and balanced method into the credit scoring business with ZScore, their advanced AI-powered solution for credit score calculation. ZScore works across the whole credit cycle and offers a number of easily accessible capabilities to help assign credit scores and manage borrowers.  \n\n## Who's funding Finbots.AI and where is the money going?  \n\n## Find out more",
    "7ef702f2-3333-43e0-ac33-7e3ee6d1d309": "## Transcript\n### ML teams vs engineering teams\nWe're also more flexible, easier to move people from one project to another. For us now, our CV model, our DU model, our task mining model, they have a lot of things in common.  \n\nLukas:  \n\n> It's funny. We were talking to Jeremy Howard, the fastai founder, and he was saying that he thinks that engineering software is kind of more fun because you make incremental progress that you can really see. And I was kind of reflecting on that.  \n\nI think my background is more in ML, but actually adding features to the Weights & Biases product is definitely more satisfying for me than training ML models. I feel like ML models, mostly they don't work and the debugging cycles are way longer and harder.  \n\nIs that consistent with your experience? Or there must be something about ML that you love.  \n\nMircea:  \n\n> Yes, but I mean, we do new features, although we don't...I mean, it depends how we define engineering.",
    "f4374002-b43b-4abd-af08-1c49ded9e085": "## Transcript\n### ML teams vs engineering teams\nWe, in this team, don't just train models and then tell others, \"Okay, take the models.\" We do the post-processing. In most cases, there is more post-processing than the model itself. We do pre-processing, we do data manipulation.  \n\nSo we build some feature, not just a model that doesn't do anything, it's just nice and shiny. But I know what you're saying. We also like to build features.  \n\nLukas:  \n\n> Are there different ways that your team collaborates together? Is it a different kind of collaboration than an engineering team?  \n\nEven though I know you're applied science, it's still kind of a different thing than software engineering, I think. So are there kind of different ways to do code reviews and things like that on your team?  \n\nMircea:  \n\n> We have less process than engineering teams I'm aware of. I don't know in detail how an engineering team functions now, but I think many things are in common.",
    "76b7a4c7-58ea-4d55-80c5-8fa4e43aa7b5": "## How the TorchData API Works: a Tutorial with Code\n### What Is TorchData? \ud83e\uddd0\n> List all files in a directory  \u27a1 Filter the image files \u27a1 Find the labels for each image  \n\nThere are a few built-in DataPipes that can help us do exactly that:  \n\n* FileLister - lists out files in a directory\n* Filter - filters the elements in DataPipe based on a given function\n* FileOpener - consumes file paths and returns opened file streams\n* Mapper - Applies a function over each item from the source DataPipe  \n\n> \u2705  The code for this report can be found here:  \n\nLet's see how this looks in action:",
    "6b7ce968-c5c4-4a1b-83ec-44ed0ca02837": "## Limitations and Future Work\n\n## Limitations  \n\n* The proposed method handles transient objects by filtering them out during training via masking using a segmentation algorithm. If objects are not properly masked, they can cause artifacts in the resulting renderings. For example,\n* Temporal inconsistencies in the training data, such as construction work, are not automatically handled and require the manual retraining of the affected blocks.\n* The inability to render scenes containing dynamic objects currently limits the applicability of Block-NeRF towards closed-loop simulation tasks in robotics.  \n\n## Future Works to Overcome the Limitations",
    "c37253bd-9553-48f6-81ea-1313d6d77086": "## Transcript\n### Leadership lessons that Jensen has learned\n\nLukas:  \n\n> You've been running NVIDIA for quite a long time. I was curious how you feel you've changed as a leader over the decades of running the company.  \n\nJensen:  \n\n> You know, you're almost asking the wrong person. You could ask almost anybody else around me.  \n\nLukas:  \n\n> Fair enough. How has your experience changed?  \n\nJensen:  \n\n> That's an easier question for me.  \n\nWhen I was 30 years old, I didn't know anything about being CEO. I did a lot of learning on the job. There were many management techniques that were just really dumb, and I don't use them anymore.  \n\nLukas:  \n\n> Like what?  \n\nJensen:  \n\n> Well, alright. I'll give you a couple.  \n\nLukas:  \n\n> Awesome. Thank you.  \n\nJensen:",
    "75efdace-056c-4468-830c-0b04af2705bd": "## Transcript\n### Leadership lessons that Jensen has learned\nWe can create the conditions by which great work can be done. We can be good listeners and eliminate obstacles for the team. We could be part of the solution by highlighting issues, recruiting. All kinds of things that we can do to help them reason about priorities, help them reduce the scope of their work, and try to seek the minimum viable product instead of building such giant things.  \n\nThere are a lot of different skills that we could've instilled into the organization, but the one thing that it doesn't really need is a tapeout bonus, an achievement bonus. Because everybody's trying to do their best.  \n\nThat's one example.  \n\nLukas:  \n\n> That's a great one. What else? If you've got others, I'd love to hear them.  \n\nJensen:  \n\n> Okay. Here's another one. Well, I want to be diplomatic as well, because there's so many CEOs that are out there. They could be using some of these techniques, and I hate to be critical of them. So this is not a criticism, this is just my style.",
    "eb47cb2e-ec63-4676-94f0-440f353088a8": "## Transcript\n### Logging GPT-3 fine-tuning projects to W&B\nIt's like, start with a small set of examples, then double it and see how much of improvement you get. You usually...if you double your amount of training data, then you get to see some linear improvement in your error rates. So if you have 10% error rate or something, and you double your training data, you're going to get down to maybe 8% error rate. And then you double it again, you get down to 6% error rate, and so on.  \n\nIf you can start seeing that trend, then you can suddenly get a sense of, \"How much would it actually cost me \u2014 in terms of labeling more data and so on \u2014 to get the result that I want?\" and so on.  \n\nIt's a very powerful thing to do.  \n\nLukas:  \n\n> Are the results of training these models reproducible? How much variability is there each time you fine-tune it?  \n\nWould you get the same model if you fine-tuned on the same data two different times?  \n\nPeter:  \n\n> In principle, you can set it up to be quite reproducible.",
    "56d90d03-78c0-4a5c-9cce-ad4ed1c94306": "## Transcript\n### Lukas' advice to new entrepreneurs\n\nChris:  \n\n> All right, Luke. If there are other entrepreneurs listening to the podcast and wanting to build a startup that achieves a billion-dollar valuation, what advice as a startup CEO would you give them?  \n\nLukas:  \n\n> I feel like the advice probably depends on who that person is. Let's picture someone. Who are you thinking of here?  \n\nChris:  \n\n> All right. It's someone that looks a little like us, right? They're programmers. They're interested in starting a company, but maybe don't have a ton of experience on the business side of things, but they're passionate about the product they're creating.  \n\nLukas:  \n\n> There's so much advice out there that I think is really good these days. I feel like when we were all starting our companies the first time, being an entrepreneur wasn't a thing.",
    "c9ad2298-e2d4-42fa-88cf-96d72b90d556": "## Transcript\n### NVIDIA Base Command and DGX SuperPOD\n> In a way, think of NVIDIA Base Command as your one-stop shop for all of your AI development. It's a SaaS offering from NVIDIA where you log on directly, or you log on via an integration partner, and you leverage the capabilities of Base Command to schedule jobs across a variety of infrastructures.  \n\nYou do that in a secure manner. You gain access to your data and retain access to your data and data sovereignty, across the infrastructure that you're scheduling the jobs on. Then it's really just a matter of optimizing that job run on NVIDIA infrastructure. That's really what Base Command aims to do.  \n\nLukas:  \n\n> These jobs, they're model training jobs exclusively, or is it broader than that?  \n\nStephan:  \n\n> Model training jobs are generally the ones that we focus on, but we also do model validation, for example. You could have single-shot inference runs as well.  \n\nLukas:  \n\n> Are there other pain points of model of development that Base Command aspires to solve or tries to solve?  \n\nStephan:",
    "8dd6a2aa-c771-4e21-a30f-edcd64d11327": "## The Reality Behind Optimization of Imaginary Variables - II\n### Strictly Linear vs Widely Linear Networks\nWidely linear networks on the other hand are special projections of complex layers that are used for specific tasks. The primary distinction between a strictly linear and a widely linear network is that the former is holomorphic and the latter is not. This gives them a special advantage of fitting to second-order non-circular data (Zeyang Yu, Shengxi Li and Danilo Mandic, 2019).  \n\nA complex random variable  is said to be circular if for any rotation angle  both  and , have the same probability distribution. This means that second order circular signals are invariant to phase transformations (Danilo Mandic and Vanessa Su Lee Goh, 2009). In actuality, the availability of such circular distributions is rare and most distributions are non-circular. This is why widely linear neural networks were suggested as a practical solution.  \n\nA widely linear network uses conjugates of the imaginary inputs along with two weight matrices, one for the standard and the other for the conjugate representation of the input.",
    "f908a4d4-e8a7-4720-825f-288d3f9b1be4": "  \n\n# How Capella Space Produces World Class Satellite Data with the Help of Weights & Biases  \n\nDescription: Learn about synthetic aperture radar, why Capella Space approaches satellite data differently, and how W&B helps accelerate their machine learning projects  \n\nBody:  \n\n# About Capella Space  \n\nFounded in 2016, Capella Space is an aerospace and satellite imaging company specializing in synthetic aperture radar (SAR). They launch and maintain their own satellites, collecting SAR data for a wide array of use cases, including disaster response, maritime domain awareness, defense, intelligence, and more. To understand why they\u2019re so successful, you have to first understand the data they collect.  \n\n# Why SAR is Different  \n\n# Accelerating Deep Learning with W&B  \n\n# What\u2019s Next for Capella Space",
    "64272d3d-305f-4602-8a86-7bbcb0b47bf4": "## 2. The PTI Method\nThe above equation is the closed-form equation which in reality is an optimization approach. So a typical Generator  undergoing the tuning procedure should go through the following equations:  \n\n$$\nG(w_{pivot}) = X_{pivot}\n$$  \n\n$$\n\\mathrm{L}_{pt} = \\mathrm{LPIPS}(X,X_{pivot}) +\\lambda_{reg}*\\mathrm{L2}(X,X_{pivot})\n$$  \n\n$$\nw^* = w - \\frac{\\partial L_{pt}}{\\partial X}\n$$  \n\nwhich gives us Generator .  \n\n### Note  \n\nmeans GAN Inversion method and does not mean an invertible generator function.  \n\nNow? Let's see this in action.",
    "b6eb89d1-cba5-495d-a2c3-8fba69d1a3f6": "## Watch on YouTube\n### Transcript\n#### Why benchmarks are limited\nYou asked me before if I could summarize the problems with benchmarks, and it's not so much benchmarks I have a problem, but the way that they're used. I think this is an example of \"the map is not the territory\".  \n\nPeople will tend to say, \"Oh, here's this benchmark about computer vision.\" ImageNet is that. Or, \"Here's a benchmark about natural language understanding of English,\" and that's GLUE and SuperGLUE.  \n\nPeople will say...I've actually seen this in like a PR thing that came out of Microsoft saying that computers understand English better than people now, because this one setup scored higher than some humans on the GLUE benchmark. That's just a wild overclaim. and it's a misuse of what the benchmark is for.  \n\nSo, what's the problem with the overclaims? Well, it kind of messes up the science. We're not doing science if we're not actually matching our conclusions to our experiments.",
    "8e66172b-6ee8-4bf2-b49e-f13144291c96": "  \n\n# Jeff Hammerbacher \u2014 From data science to biomedicine  \n\nDescription: Jeff talks about building Facebook's early data team, founding Cloudera, and transitioning into biomedicine with Hammer Lab and Related Sciences.  \n\nBody:  \n\n# Listen on these platforms  \n\n[Apple Podcasts](http://wandb.me/apple-podcasts)[](http://wandb.me/apple-podcasts)    [Spotify](http://wandb.me/spotify)[     Google Podcasts](http://wandb.me/google-podcasts)    [YouTube](http://wandb.me/youtube)    [SoundCloud](http://wandb.me/soundcloud)  \n\n# Guest Bio  \n\nJeff Hammerbacher is a scientist, software developer, entrepreneur, and investor. Jeff's current work focuses on drug discovery at Related Sciences, a biotech venture creation firm that he co-founded in 2020.  \n\nPrior to his work at Related Sciences, Jeff was the Principal Investigator of Hammer Lab, a founder and the Chief Scientist of Cloudera, an Entrepreneur-in-Residence at Accel, and the manager of the Data team at Facebook.  \n\n## Connect with Jeff  \n\n*\n*  \n\n# Show Notes",
    "fb78dab8-8c32-4b9e-8e72-4719fc9b75ae": "## DALL-E Mini Explained\n### How Do Our Results Compare To \"Generator + CLIP\"\n\nThere are several models available which consist of a generator coupled with CLIP to create images (such as \"VQGAN + CLIP\").  \n\nThese models have a completely different approach. Each image prediction is actually the result of an optimization process where we iterate over the latent space of the generator (image encoding space) to directly maximize the CLIP score between generated image and description.  \n\nAn interesting aspect of this method is that we can iterate either from a random image or from a pre-selected image. Also it can be used with any image resolution, constrained only by GPU RAM and time to train.  \n\nThis technique is slower and mostly used for generating artistic images which could be unrealistic but of a higher resolution.",
    "be4194f8-9692-4161-afc0-d17c1ccbdf49": "## Model Architecture\nThe main component of the ResMLP architectures are the ResMLP layers. These consist of a linear sublayer applied across patches followed by a feedforward sublayer applied across channels. Similar to Transformers, each sublayer (crosspatch and crosschannel) is paralleled with a skip-connection to achieve training stability.  \n\nThe authors report that the lack of self-attention layers made it possible to replace the Layer Normalization by a much simpler Affine transformation:  \n\n$$\nAff_{\\alpha, \\beta} (x) = Diag(\\alpha)x \\,\\, + \\beta\n$$  \n\nHere,  and  are learnable weight vectors. Like Normalization, this operation rescales the input and shifts the input element-wise. As opposed to other normalization:  \n\n* Affine Transformations have no cost at inference time.\n* Affine Transformation doesn't depend on batch statistics.",
    "cad7d32b-3088-4b87-9eb3-fa86be58aae0": "## Model Architecture\n### \ud83c\udfe0 The Complete Model Architecture\nThe key differences as listed in the paper between ResMLP and ViT are :-  \n\n* Lack of Self-Attention Blocks: Linear layers are instead used\n* No Positional Embeddings: Linear Layers encode patch information\n* No extra 'Class' Tokens: Instead Average Pooling is used on patch embeddings\n* No Normalization based on batch statistics: A learnable Affine Operator is used",
    "d30cef77-0057-4fd8-b287-5c149cfb10a7": "## MLP Mixer in PyTorch\nAs can be seen, the MLP implementation above exactly follows figure-5. It consists of two fully connected (or nn.Linear layers) separated by an activation layer.  \n\nNext, let's see how the mixer layer can be implemented assuming we already have the input image converted to a 196x512 patch embedding matrix.  \n\nAs you can see, the first step is to perform the normalization using nn.LayerNorm. Next, we transpose the input matrix and pass it through the MLP and transpose it back. This is the token-mixing operation. In the implementation below, the self.mlp_tokens represents the token-mixing MLP. And as can be seen in forward method, x = x + self.drop_path(self.mlp_tokens(self.norm1(x).transpose(1, 2)).transpose(1, 2)) represents the token-mixing MLP operation with transposition of the matrix.  \n\nNext, we again perform normalization using nn.LayerNorm and feed the outputs to the channel-mixing MLP with skip connections to get the final outputs from the mixer layer.",
    "2996ff57-48e0-4574-9a86-1aaae6ecdfab": "## Log & Explore a Single Table\n* run.log(): associate a Table with a particular experiment run. This is the recommended default and most useful for saving a model's predictions or samples generated during training or testing. It's ideal for faster exploration, and we'll focus on this method in this report.\n* artifact.add(): associate a Table with a particular dataset or model version. This is most useful when you plan to reference/reuse a specific dataset version in future runs. Refer to this section of the docs for .  \n\n## Run.log() to Create a Table in the Workspace  \n\nTo log a Table to the workspace, construct the Table as usual ([by adding rows or from a Pandas DataFrame](https://docs.wandb.ai/guides/data-vis/log-tables#create-tables)) then call:  \n\n## Include a Logged Table in a Report  \n\nThere are two ways to add a Table like the one above to a Report:  \n\n## Helpful Tips  \n\n### The run workspace view is stateful and shared for all Tables under the same key  \n\n### Unique IDs are optional  \n\n### Save class labels as strings",
    "9a5a7567-3a2c-4e2a-8c1c-3a0b5f8d70bd": "## About SBX Robotics\n\nWorking on a computer vision problem? We can help.  \n\nAt [SBX Robotics](http://sbxrobotics.com/) we are experts in using synthetic data to bootstrap and improve computer vision systems.  \n\nOur clients send us ~25 images from their production setting, and we generate 25,000 synthetic training samples proven to work on the original validation data. All of our datasets ship with:  \n\n* Our best benchmark model trained on the synthetic data, tested on your validation data, and loaded into a Google Colab.\n* Code to evaluate the benchmark model, , and our  with code snippets  \n\n### Ready to try synthetic data for your project?  \n\n[Use this link](https://www.sbxrobotics.com/?ref=wandb1#get-started) to submit 25-50 images from your production setting, or contact us at info@sbxrobotics.com.  \n\n>\nMention  \"Weights & Biases Tables Tutorial\" for 20% off your first synthetic dataset.",
    "d78aff72-a438-4eae-9fec-9ecaa1a108bd": "## Transcript\n### Tackling the robotic hand problem\n\nLukas:  \n\n>  Why did you choose to manipulate a hand? I feel like if I was trying to build a general-purpose robot, I might even leave out the hands.  \n\nIt seems like the hand has got to be the most complicated thing and I feel like in the movies robots don't even have hands. Maybe they don't even need them, I don't know.  \n\nPeter:  \n\n> Yeah. You know, it's interesting how that started. The first problem we tackled, we didn't use a robotic arm.  \n\nWe had one of these fetch robots, which is basically a mobile robot with a robotic arm and a two-finger gripper. It's a super simple robot and we would even just screw it into the floor so it couldn't move. So it was just like a robot arm, basically. A very expensive robot arm. That's how we started.",
    "637daa3a-0bb9-46da-9b06-a59b646eb55e": "## Explained: Characterizing Signal Propagation to Close the Performance Gap in Unnormalized ResNets\n### Scaled Weight Standardization\n> \ud83d\udcad: As I've mentioned before, it is considered good propagation if the activations have a zero mean and unit variance throughout the network. Thus, having an average value of the squared channel mean that grows rapidly with depth & values of variances that are consistently smaller than one represents instability in the network.  \n\nTo prevent the emergence of a mean shift and to ensure that the residual branch   is variance-preserving, the authors proposed Scaled Weight Standardization.  \n\n> \ud83d\udcad: Scaled Weight Standardization is an extension of   . Essentially, weight standardization normalizes weights in convolution layers, i.e., making the weights have zero mean and unit variance. Note that this is different from Batch Normalization (BatchNorm) which standardizes the hidden activations instead. For the curious reader,    is a wonderful video by Yannic Kilcher, that explains Weight Standardization.  \n\nScaled Weight Standardization has been formulated as:",
    "e26de47d-2c15-4965-99d0-d002c3f879a3": "  \n\n# Chris Anderson \u2014 Robocars, Drones, and WIRED Magazine  \n\nDescription: Chris shares his journey starting from playing in R.E.M, becoming interested physics to leading WIRED Magazine for 11 years. His robot fascination lead to starting a company that manufactures drones, and creating a community democratizing self-driving cars.  \n\nBody:  \n\n# Listen on these platforms  \n\n[Apple Podcasts](http://wandb.me/apple-podcasts)[](http://wandb.me/apple-podcasts)    [Spotify](http://wandb.me/spotify)[     Google Podcasts](http://wandb.me/google-podcasts)    [YouTube](http://wandb.me/youtube)    [Soundcloud](http://wandb.me/soundcloud)  \n\n# Guest Bio  \n\n# Show Notes  \n\n## Topics Covered  \n\n0:00\u200b sneak peek and intro  \n\n1:03\u200b Battle of the REM's  \n\n3:35\u200b A brief stint with Physics  \n\n5:09\u200b Becoming a journalist and the woes of being a modern physicis  \n\n9:25\u200b WIRED in the aughts  \n\n12:13\u200b perspectives on \"The Long Tail\"  \n\n20:47\u200b getting into drones  \n\n25:08\u200b \"Take a smartphone, add wings\"  \n\n## Transcript",
    "0614a9f9-81c4-4085-b729-c5db0d5e37ec": "  \n\n# Robert Nishihara \u2014 The State of Distributed Computing in ML  \n\nDescription: The story of Ray and what lead Robert to go from reinforcement learning researcher to creating open-source tools for machine learning and beyond.  \n\nBody:  \n\n# Listen on these platforms  \n\n[Apple Podcasts](http://wandb.me/apple-podcasts)[](http://wandb.me/apple-podcasts)    [Spotify](http://wandb.me/spotify)[     Google Podcasts](http://wandb.me/google-podcasts)    [YouTube](http://wandb.me/youtube)    [Soundcloud](http://wandb.me/soundcloud)  \n\n# Guest Bio  \n\nRobert is currently working on Ray, a high-performance distributed execution framework for AI applications. He studied mathematics at Harvard. He\u2019s broadly interested in applied math, machine learning, and optimization, and was a member of the Statistical AI Lab, the AMPLab/RISELab, and the Berkeley AI Research Lab at UC Berkeley.  \n\n## Connect with Robert  \n\n*\n*\n*\n*\n*  \n\n# Show Notes  \n\n## Topics Covered  \n\n0:00\u200b sneak peak + intro  \n\n1:09\u200b what is Ray?  \n\n## Transcript",
    "ed27b48e-1826-4ab1-892e-ca6470332bec": "## Scope of Reproducibility\n\nWe focus on the following target questions:  \n\n* Does RigL outperform existing sparse-to-sparse training techniques\u2014such as  and \u2014and match the accuracy of dense-to-sparse training methods such as iterative pruning ()?\n* We investigate the sensitivity of RigL to two additional hyperparameters which it requires.\n* How does the choice of sparsity initialization affect the final performance for a fixed parameter count and a fixed training budget?\n* Does redistributing layer-wise sparsity during connection updates improve RigL\u2019s performance? Additionally, can the final layer-wise distribution serve as a good sparsity initialization scheme?",
    "e67439ea-a7c7-49a4-8958-8ea24b8fbc20": "## Introduction\n### Methodology\nTo better exploit the dynamic property of the training procedure, the authors propose Dynamic R-CNN which as shown in the above figure. The key insight is adjusting the second stage classifier and regressor to fit the distribution change of proposals. As described earlier, Dynamic RCNN is made up of two primary components: 1. Dynamic Label Assignment (DLA) and 2. Dynamic SmoothL1 Loss (DSL) where the former is structured for the classifier and the latter for the regressor. We formulate these two components in details in the subsequent sections.  \n\n### Dynamic Label Assignment (DLA)  \n\nThe DLA module can be represented by the following mathematical formula:  \n\n$$\nlabel = \\begin{cases} 1 & \\text{if max} \\ IoU(b, G) \\geq T_{now} \\\\  0 & \\text{if max} \\ IoU(b, G) < T_{now},\\end{cases}\n$$  \n\nwhere  refers to the current IoU threshold.  \n\n### Dynamic SmoothL1 Loss (DSL)  \n\nThe DSL module can be represented by the following mathematical formula:",
    "77accba9-7ae5-4a4a-a570-3687fa882f2a": "  \n\n# Information Extraction From Documents Using Machine Learning  \n\nDescription: In this article, we'll extract information from templated documents like invoices, receipts, loan documents, bills, and purchase orders, using a model.  \n\nBody:  \n\nIn this article, we'll discuss how to extract information from structured or unstructured documents. Specifically, we will be discussing [Representation Learning for Information Extraction from Form-like Documents](https://research.google/pubs/pub49122/) paper by Google. This paper is also accepted at [ACL 2020](https://acl2020.org/).  \n\nImage Source: [Nanonets blog](https://nanonets.com/blog/extract-structured-data-from-invoice/)  \n\nIf we can add all contact details like Contact No, Email Id, Address, etc.. directly by scanning the business card. Interesting isn't it? This small feature saves a lot of time.  \n\nExtracting Information from documents is a cumbersome task for humans and of course, it is also expensive.  \n\n# Various Approaches  \n\n# Dataset Details",
    "499cccc7-b4f9-4931-a8a6-21ff572a0650": "## [Wav2Vec](https://arxiv.org/pdf/1904.05862.pdf)\nWav2Vec is an unsupervised pre-training approach for speech recognition by learning representations of raw audio. In general, we use log mel filter banks or mfcc features to train speech recognition systems. Instead, if we use the encoding or representation of Wav2Vec we can achieve almost similar results while using two orders of magnitude less labeled training data.  \n\n## Data Used For Pre-training:  \n\n1. Wall Street Journal (WSJ): 81hrs of transcribed data\n2. Libri Speech: 960hr full training-data\n3. Libri Speech: 80hr subset of cleaned data  \n\n## Model  \n\nThe Wav2Vec model is Fully Convolutional. It consists of two networks:  \n\n1. Encoder Network\n2. Context Network  \n\nBefore going further, let us get familiarized with the term Latent Space.  \n\n## Training Objective:  \n\nThese contextualized representations are then used to predict the future by contrasting the two latent speech representations from a set of negative samples.  \n\n## Results:  \n\nThe author\u2019s used wav2letter as an acoustic model.  \n\n## Model:",
    "e24ec41d-81e8-401c-be4e-673ef4ddadc0": "  \n\n# What does it take to write a NeurIPS paper?  \n\nDescription:  \n\nBody:  \n\nThis is how participants at NASA's Frontier Development Lab (FDL) \u2013 [Sairam Sundaresan](https://twitter.com/DSaience) and [J. Emmanuel Johnson](https://twitter.com/jejjohnson), published a paper at NeurIPS.  \n\nYou can read their paper at: [\"RotNet: Fast and Scalable Estimation of Stellar Rotation Periods Using Convolutional Neural Networks\"](https://arxiv.org/abs/2012.01985).  \n\n# NASA's Frontier Development Lab  \n\nThe Frontier Development Lab (FDL) applies AI technologies to science to push the frontiers of research and develop new tools to help solve some of the biggest challenges that humanity faces. Researchers from different disciplines of science come together to work on this 8-week long challenge.  \n\n# Problem Statement  \n\n>  \n\nThe goal of the team was to accurately predict stellar rotation periods from Kepler light curves.  \n\n# The Two Aha Moments  \n\n# Debugging and Reproducibility  \n\n>  \n\n# Result  \n\n>",
    "5dd18976-4dd0-467b-acaf-4acd9516dd68": "## Circular Dendrogram\n### Display Configuration\n* labels: show/hide node names\n* radius: spacing between concentric layers\n* extent: 0-360 degrees of the circle in which to render the chart\n* rotate: circle orientation (pivot chart around the root)\n* layout: visual spacing of child nodes\n* links: how the edges are rendered (lines, curves, orthogonal)",
    "c8b32ffe-f28e-4960-b259-5c759eb1f362": "## How To Use GPU with PyTorch\n### Torch CUDA Package\n\nIn PyTorch, the torch.cuda package has additional support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.  \n\n* If you want a tensor to be on GPU you can call `.cuda()`.\n*    If you have a tensor on GPU and you would like to bring it to CPU then you can call `.cpu()`. This is usually used to bring the output(tensor) of the model to the CPU.\n* To get the index of the currently selected device.\n* To get the number of GPUs available.\n* To get the name of the device.",
    "1c8d7037-104b-40a6-bdcf-545865201252": "## \ud83c\udfba LightningModule - Define the System\n\nThe LightningModule defines a system and not a model. Here a system groups all the research code into a single class to make it self-contained. LightningModule organizes your PyTorch code into 5 sections:  \n\n* Computations (`__init__`).\n* Train loop (`training_step`)\n* Validation loop (`validation_step`)\n* Test loop (`test_step`)\n* Optimizers (`configure_optimizers`)  \n\nOne can thus build a dataset agnostic model that can be easily shared. Let's build a system for Cifar-10 classification.  \n\n### 1. Computations  \n\nThis component of the LightningModule encompasses the model architecture and the forward pass. This code snippet might look familiar to your normal PyTorch code.  \n\n### 2. Training Loop  \n\n### 3. Validation Loop  \n\n### 4. Test Loop  \n\n### 5. Optimizer  \n\nWe can define our optimizer and learning rate schedulers using the configure_optimizer method. One can even define multiple optimizers like in the case of GANs.",
    "e1d5046c-99f9-4018-98c3-385dc6d1643d": "## Overview of the Proposed Method\n### II. Domain Guided Encoder\n* We are using a pre-trained GAN model. The authors have used pre-trained StyleGAN models.\n* In this proposed adversarial learning strategy, the generator is fixed(weights are frozen), and the encoder and the discriminator are updated accordingly.\n* For the perceptual loss, the `conv4_3` layer of the VGG16 model is used.  \n\nClick on the \u2699\ufe0f icon and move the slider to get the original image. The encoder image is the reconstruction of the original images made by the generator while using the latent code, which is the domain-guided encoder's output.",
    "5961e512-876f-44db-a6e2-7871d09af74e": "## Free for Academics and Individuals\n\nW&B is free for individuals and academic teams, and will always be. We are committed to staying free to support the advancement of machine learning, and we make it easy to export data with our [Export API](https://docs.wandb.com/library/api).  \n\n* : Accounts for individuals, startups, teams, and enterprises\n* : Apply for access to academic teams to share results with collaborators\n* : We have options for on-prem and private cloud installations\n* : We're happy to get on a call and talk about custom enterprise plans",
    "10d10d03-b36d-4f9f-88ab-6cd5095c901d": "## Ludwig \ud83d\udc9c W&B\nThink of W&B like GitHub for machine learning models \u2014 save machine learning experiments to your private, hosted dashboard. Experiment quickly with the confidence that all the versions of your models are saved for you, no matter where you are running your scripts.  \n\nW&B lightweight integrations work with any Python script, and all you need to do is sign up for a free W&B account to start tracking and visualizing your models.  \n\nWe have instrumented the Ludwig repo to automatically log training and evaluation metrics to W&B at each logging step.  \n\n### Using W&B with Ludwig  \n\nTo use Ludwig\u2019s new Weights & Biases integration, just add the \u2013wandb parameter to your Ludwig commands. This will allow training and experiments to be tracked and interacted with on the corresponding Weights & Biases page.  \n\nAnd here an example:\nludwig train --dataset <DATASET_PATH> --config_file <CONFIG_FILE_PATH> --wandb",
    "6ccac525-ad9e-44c8-a250-359b0e66cc7b": "  \n\n# Metric Learning for Image Search With Weights & Biases  \n\nDescription: In this article, we will explore supervised metric learning and extend it to image similarity search using Weights & Biases to track the results of our experiments.  \n\nBody:  \n\nMetric learning is a broad field with many definitions to define it. Primarily,  it aims to measure the similarity among data samples and to learn embedding models. In a familiar classification setting, we give our model some  and learn to predict its class.  \n\nIn the context of metric learning to learn embedding models, the motivation is to embed  in an embedding space such that similar  are close together in that space while dissimilar ones are far away. We are often not interested in how the embedding space looks as long as the  we want to be close together(similar) form a cluster in that space.  \n\n# The Dataset  \n\nFor simplicity, we will be using the CIFAR-10 dataset. There are 10 classes as mentioned by CLASS_NAMES.  \n\n# The Model  \n\n# Conclusion",
    "20120d33-1064-46ff-a26b-88d3d1993764": "## What Is Image-to-Image Translation?\n* Deep Image Inpainting - Here, the input image is corrupted with missing pixel values, and the task is to fill those patches as the output. Here is an .\n* Colorize image - We feed black & white images and want to get them colorized most realistically. Here is .\n* Sketch to Image - We feed an object's sketch and want to get a realistic image as output. The paper that this article is covering is an example to sketch to image translation. Let us dive into it.\n*  \n\nFigure 1: Examples of image-to-image translation tasks. ([Source](https://arxiv.org/abs/1611.07004))  \n\nRecent developments in image-to-image translation allow fast generation of face images from freehand sketches. However, all these techniques quickly overfit to input sketches. Thus, it requires professionally drawn sketches, limiting the number of people using applications based on these techniques.",
    "ef31453f-0426-4e45-8c52-e05339144a58": "## Data Preparation\n### Generate Dataset - Labels\n\nWe need to integer encode our sentence(made of characters). We will use an inverse_mapping dictionary where the key is the character, and the value is its corresponding integer encoding.  \n\nFig 2: An example of integer encoding.  \n\nWe will build an input pipeline using tf.data.  \n\nNow on to a more exciting part of this application.",
    "012273f7-4bf7-47d6-8adb-97a486d3eb18": "## A Guide to Multi-Label Classification on Keras\nThe output of the neural network is a probability distribution modeling the approximate true distribution. In a multi-class classification, our true label usually corresponds to a single integer. However, in multi-label classification, inputs can be associated with multiple classes. For example, a movie poster can have multiple genres.  \n\nLet's take a quick look into a few of the key ingredients of multi-label classification. Here's what we'll be covering:  \n\n### Table of Contents  \n\nLet's dive in!  \n\n## Multi-Label Binarizer  \n\n## Output Activation and Loss Function  \n\nLet's first review a simple model capable of doing multi-label classification implemented in Keras.  \n\n## Resources  \n\nIn this article, I have only touched on the key ingredients, so this is suited for someone with prior experience in this topic. However, if you are new to this, here are some useful reads:  \n\n*\n*\n*  \n\n\"",
    "03724ae1-30a4-48a6-b315-12b751e7572f": "\"  \n\n# Comparing Sigmoid-MSE With Softmax Cross-Entropy for Image Classification  \n\nDescription: In this article, we look at the results from an experiment to compare sigmoid with MSE and softmax with cross-entropy for image classification.  \n\nBody:  \n\nIn this article, we're going to look at what happens when we use the sigmoid activation function along with Mean Square Error(MSE) loss function instead of the usual choice of using softmax activation function along with categorical cross-entropy loss function for image classification. Let's get right to it! [](https://colab.research.google.com/drive/1HnbLqwXwarbpu45DiAQNWuj958bMZ2rF?usp=sharing)  \n\n# Experiment  \n\nLet's train a simple vanilla [convolutional neural network (CNN)](https://wandb.ai/site/tutorial/convolutional-neural-networks) on the CIFAR-10 dataset to perform image classification, and then compare:  \n\n# Observations  \n\n# A Few Things We Learned",
    "ee85c66b-c7c8-47c0-8c49-078d05ab4bbb": "  \n\n# How to Prevent TensorFlow From Fully Allocating GPU Memory  \n\nDescription: In this report, we see how to prevent a common TensorFlow performance issue  \n\nBody:  \n\n# Problem  \n\nI work in an environment where computational resources are shared, i.e., we have a few server machines equipped with a few NVIDIA Titan X GPUs each.  \n\nFor small to moderate size models, the 12 GB of the Titan X is usually enough for 2\u20133 people to run training concurrently on the same GPU. If the models are small, such that a single model does not fully utilize the GPU, it can result in a speedup compared to running one training process after the other. Even in cases where the concurrent access to the GPU does slow down the individual training time, it is still nice to have the flexibility of having multiple users simultaneously train on the GPU.  \n\n# Solution  \n\nTensorFlow, by default, allocates all the GPU memory to your model training. However, to use only a fraction of your GPU memory, your solution should have two things:",
    "33af4cbc-71f2-4503-ab7a-1e7dfdd45bcf": "## Revisiting the Optimization Landscape of Neural Networks\n$$\n\\operatorname{minimize}_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} \\ell\\left(h_{\\theta}\\left(x_{i}\\right), y_{i}\\right)\n$$  \n\nwhere,  \n\n*  is the parameter vector,\n*  is the number of training examples,\n*  is the model (neural network) parameterized over\n*  is our loss function in which  and  correspond to an input and a label from the training dataset  \n\nConsider the figure below that shows a sample non-convex loss landscape (typical for neural networks). As we can see, there are multiple local minima in there. A neural network can only reach one of these local minima at one time after they are trained. The same neural network can end up in different landscapes each time they are trained with different random initializations exhibiting high variance in predictions.  \n\nWe can also see that these local minima lie at the same level in the loss landscape, which further suggests that if a network ends up in one of these local minima, it will yield the same kind of performance more or less.",
    "6e87324d-1df8-4e4a-8c17-72169b17d75a": "## Our Model\nIn this article, we use this research paper as the inspiration and reference: [Hiding Images in plain sight: Deep Steganography](https://papers.nips.cc/paper/6802-hiding-images-in-plain-sight-deep-steganography.pdf).  \n\nOur goal is to automate the task of hiding as well as revealing the secret image (or message).  \n\nThe architecture of the network is similar to auto-encoders.  \n\nThe network consists of three sections:\nThe preparation network\nThe hiding network\n*The revealed network.\nWe will combine these three parts collectively to form an end-to-end system for hiding as well as revealing the hidden image.  \n\nHere is how the research paper describes the functionalities of the networks:",
    "7265a57e-be3b-4ef4-918f-6e2f0ac90705": "## Models\n### MemRNN: RNNs with Self-Attention\n\nRNNs can be augmented to contain a memory cell, holding past hidden states, and can use an additional neural network to create connections to these memories.  We modify the RNN above as follows:  \n\nwhere:  \n\nwhere:  \n\nwhere s are defined as:  \n\nand  \n\nhere is called an alignment function and is often done using the aforementioned additional neural network. Additionally,  can be any number of functions,including addition and concatenation (we use addition here). An example of one of these networks is shown below for common sequence-to-sequence problems:  \n\n(source: [https://ift6135h18.files.wordpress.com/2018/03/10_26_memory_and_attention.pdf](https://ift6135h18.files.wordpress.com/2018/03/10_26_memory_and_attention.pdf))  \n\nSo the MemRNN works by taking a linear combination of all past states to update the current hidden state, where the strength of each contribution of each past hidden state is defined by the alignment function.",
    "13af484a-9803-484c-a3ec-1da6028d87c3": "## What Are the Different Activation Functions?\n### The ReLU Activation Function\n#### Disadvantages of the ReLU Activation Function\n* It suffers from the dying ReLU problem. ReLU is always going to discard the negative values (i.e. the deactivations by making it 0).  Because of this, the gradient of these units will also become 0 and by now we all know that 0 gradient means no weight updation during backpropagation. Simply speaking, the neurons which will go to this state will stop responding to the deviation of input or the error.  This, as a result, hampers the ability of the model to fit the data properly.\n* It is non-differentiable at 0.",
    "38f304c7-e5f0-4280-a590-2f80f749974a": "## What Are the Different Activation Functions?\n### The Leaky ReLU Activation Function\n\nIt is a variant of ReLU. The equation for Leaky ReLU is f(x) = max(\u03b1x,x) where \u03b1 is a small constant(normally 0.01).  \n\nBelow is the image of Leaky ReLU.  \n\n### Advantages of the Leaky ReLU Activation Function  \n\n* It tries to remove the dying ReLU problem. Instead of making the negative input 0, which was the case of ReLU, it makes the input value really small but proportional to the input. Because of this, the gradient doesn't saturate to 0. If the input is negative, the gradient will be \u03b1. As a result, there will be learning for these units as well.  \n\nApart from this, it enjoys similar advantages to ReLU.  \n\n### Disadvantages of the Leaky ReLU Activation Function  \n\n### Uses of the Leaky ReLU Activation Function  \n\nSimilar to ReLU, Leaky ReLU should also be used in the hidden layers. But Leaky ReLU should always be used as an alternative to ReLU because it doesn't necessarily perform better.",
    "1f947411-6311-4270-9341-fd9152fdb087": "#### Advantages of the Swish Activation Function\n* Just like ReLU, Swish is unbounded above. This means that for very large values, the outputs do not saturate to the maximum value(which was the case of sigmoid and tanh). This means for any value the gradient doesn't become 0. Hence making the learning more capable.\n* It is bounded below. This means as the input tends to negative infinity, the output tends to some constant. It is really important in the beginning that as many negative values are conquered as possible. With this power, Swish forgets the very large negative values which are nothing but the deactivations. This feature of Swish introduces regularization in the model.\n* It is non-monotonic in nature i.e. it doesn't move in one direction. In the positive region, the nature of Swish is somewhat similar to that of ReLU. It is the negative region where the difference lies, and this difference is nothing but the non-monotonicity. Because of this feature, it is possible for the output to still fall even if the input increases. This in return increases the information storage capacity of the model and of course the discriminative capacity. But how does it happen? It happens because Swish has negative derivatives at some points and positive derivatives at others, instead of all positive or all negative derivatives. This has a huge role in the success of swish over ReLU as the expressivity of the model increases.\n* This function is self-gated.  I know you are thinking about LSTM now. Well, that is completely justified because this feature was actually inspired by LSTM.  In LSTM we had Sigmoids as the gates. These gates controlled the quantity of a vector that was passed to the next stage and this was achieved by multiplying it by the output of the sigmoid which is nothing but a number between o and 1. Similarly, in Swish, the sigmoid controls the quantity of information going to the next layer. In the case of activation, the value remains the same. But in the case of deactivations, the value is reduced by multiplying with sigmoid of the value.\nSo, self-gated means that the gate is actually the sigmoid activation of itself. where the gate is \u03c3(x) and the value to pass through is x.",
    "91d05187-3ba0-4981-b229-4ead2efe59cd": "## Visualize Latent Space\n#### Observations\nIt\u2019s worth noting that even though the digit labels were never shown to the model during training, the autoencoder naturally groups digits that look alike into the same part of the latent space. There are some really interesting takeaways from this plot:  \n\n* The plot is not symmetrical around the point (0, 0). Some points are far apart.\n* There are some digits which are represented in a smaller, more densely packed area. Some digits span a larger area in the latent representation.\n* Even within the same representation area, some points have large gaps between them.  \n\nOur aim here was to use an autoencoder as a generative model. We can certainly do so by sampling points from the latent space and passing them through the decoder. The decoder will decompress the points and we'll have an image. Thus the performance of an autoencoder as a generative model depends on how well the data is represented in the latent space.",
    "770df93c-37b0-4e5d-845f-4c17dd5290a1": "## Community Submissions So Far\n### Community Improved Baseline By 2%\nThe chart below shows the validation accuracy of model variants submitted to the benchmark. You can see the baseline in black for contrast. Several runs converge to 1-2% higher final values than the baseline.  \n\n### What Helps: More Layers, More Sophisticated Base Network Architecture  \n\nSome techniques that appear to help, in the order they exceeded the baseline in the [benchmark leaderboard:](https://wandb.ai/wandb/droughtwatch/benchmark/leaderboard)  \n\n*  : using  with custom activation functions and clocked learning rate\n* : using ResNet 50\n* : modifying the default model to use extra convolutional and dropout layers\n* : using EfficientNet (currently the highest accuracy model on the benchmark validation set)  \n\nThese improvements are amazing to see, and the community is just getting started. The full training curve is not available for all submissions because of different logging settings, hence some of the submissions appear as dots in the top left at epoch 0.",
    "8dfa40b7-5e01-42df-80c6-fbe5dd811ac7": "## Method 3: Weights initialized with values sampled from a uniform distribution\nIn the earlier experiments, we saw that initializing our model with constant values is not a good idea. So, let\u2019s try initializing them with unique small numbers having [0,1]  range. We can do this by sampling values from a [uniform distribution](uniform distribution). A uniform distribution looks like so:  \n\nA uniform distribution within [-5, 5] range  \n\nHere\u2019s the catch with uniform distributions - the values from a uniform distribution have an equal chance of being sampled.  \n\nInitializing a tf.keras Dense layer with a uniform distribution is a bit more involved than the previous two schemes. We would make use of the tf.keras.initializers.RandomUniform(minval=min_val, maxval=max_val, seed=seed) class here. In this case, we would be supplying 0 as the minval and 1 as the maxval. seed could be any integer of your choice. Let\u2019s see how it performs!  \n\n## Let\u2019s see how it performs!  \n\n## The Results So Far - Ones vs Uniform Weights Accuracy Trade-Off  \n\n## Quick Tangent: The recipe for initializing weights",
    "4856940f-3b17-433d-ba1d-a0551e87fd10": "  \n\n# Classify the Natural World with Weights & Biases  \n\nDescription: This article explains how to train and fine-tune convolutional networks (CNNs) to identify species beyond ImageNet using Weights & Biases  \n\nBody:  \n\nIn this project, I explore different models to identify plant and animal species from photos. I've trained small convnets and compared fine-tuned versions of existing architectures (Inception V3, [ResNet](https://wandb.ai/site/articles/exploring-resnets-with-w-b), Inception ResNet V2, Xception) with different freeze layers. I also analyzed performance on higher-level categories (e.g. mammals versus insects versus birds).  \n\n## Table of Contents  \n\nThis is my log of progress as I try different approaches.  \n\nHere's a sample of the images from the [iNaturalist 2017 dataset](https://github.com/visipedia/inat_comp/tree/master/2017):  \n\n# Fine-Tune Standard CNNs on Small Data  \n\n# Findings  \n\n## 3-10 Epochs of Pre-Training and More Data Helps  \n\n## Notes Fri Jan 18  \n\n# Fine-Tuning Inception V3",
    "26e83cf5-86cd-4048-b0a5-9474ec9a5099": "## Autoencoder Networks for FashionMNIST\n### Training\n#### Exercises\n##### 3. Convolutional Autoencoders\nTry out the convolutional version of the autoencoder\n(`encoder = EncoderConv` and `decoder = DecoderConv`).\nCompare its performance on the usual autoencoder task (`erase=0.`)\nand on the in-filling task (`erase=1.`)\nto that of the fully-connected network.\nWhich style of network does better?",
    "ae5c27ac-2627-4c35-ba8a-d1885015ac77": "## Autoencoder Networks for MNIST\n### Training\n#### Exercises\n##### **Challenge**: Skip Connections\nA common technique for improving performance in computer vision models is adding\nskip connections --\ntransformations that \"skip over\" intervening layers.\nThese \"shortcuts\" allow information to flow more smoothly through the network\nand stabilize training -- enabling more choices of optimizer,\nlayer size, and nonlinearity to reach good performance.  \n\nIn a small autoencoder like this one, we might write one skip connection\nfrom the input to the hidden layer\nand another from the hidden layer to the output.\nTry adding these to `EncoderConv` and `DecoderConv`.",
    "195dc1ac-337d-426f-bfc6-1b04182a26b9": "## Exercises\n\n#### 1. Training a Network with 99% Sparsity  \n\nWith the default settings above\n(unstructured pruning applied globally\nand feature-wise pruning applied to linear and convolutional layers,\neach with a target sparsity of `0.9`),\ntypical final sparsities are close to 99%.  \n\nReview the Weights & Biases dashboard for a training run with these settings.  \n\nDo you notice anything interesting in the loss and accuracy traces?\nEspecially on the training set, these are typically monotonic.  \n\nThe \"sparsity\" section tracks the degree of sparsity for\neach layer and for the network as a whole.  \n\nWith a typical multiplicative, iterative pruning strategy,\nthe largest absolute increase in sparsity is at the end of the first epoch,\nwhen it increases from `0` to `0.33`,\nwhile the fraction of remaining weights pruned\nremains constant.\nThe magnitude of pruned weights in each step\ngenerally increases throughout training.  \n\n#### 2. Train Longer, Get Sparser?  \n\n#### 3. Structured Pruning Considered Harmful",
    "27c71455-e901-4183-84d1-5e22172f148f": "## Applying Structured Output to RAG applications\n\n**What is RAG?**  \n\nRetrieval Augmented Generation (RAG) models are the bridge between large language models and external knowledge databases. They fetch the relevant data for a given query. For example, if you have some documents and want to ask questions related to the content of those documents, RAG models help by retrieving data from those documents and passing it to the LLM in queries.  \n\n**How do RAG models work?**  \n\nThe typical RAG process involves embedding a user query and searching a vector database to find the most relevant information to supplement the generated response. This approach is particularly effective when the database contains information closely matching the query but not more than that.  \n\n**Why is there a need for them?**",
    "e0241052-fe94-4c56-871f-0f0ce553a979": "## Visualizing and Minimizing Surprises\n### Artisanal, Hand-Tuned, Small-Batch Data Science\n##### Q Is the surprise higher or lower than the surprise you found by hand? Relate this finding to the statement above, that every time we summarize data by the mean and standard deviation, we've minimized the surprise of a model.\n\nIncrease the value of `N`,\nwhich sets the total number of values in `data`,\nby at least an order of magnitude, e.g. to `200`.  \n\nThen fit the model again, by minimizing the surprise.",
    "6c5a2c6c-75f2-4680-a912-e4fc0858309c": "## GATE: Automatic Drift Detection\n### Iterate through corruptions\n|  | corruption\\_name | precision | recall |\n| --- | --- | --- | --- |\n| 0 | corrupt\\_null | 1.000000 | 1.000000 |\n| 1 | corrupt\\_nonnegative | 0.972973 | 0.947368 |\n| 2 | corrupt\\_typecheck | 1.000000 | 0.400000 |\n| 3 | corrupt\\_units | 0.978723 | 0.821429 |\n| 4 | corrupt\\_average | 0.981818 | 0.964286 |\n| 5 | corrupt\\_pinned | 0.968750 | 0.553571 |",
    "d8093732-c7b0-4442-887b-f1d4c1b56116": "---  \n\n## description: Guide to using Artifacts for dataset versioning  \n\n# Dataset Versioning  \n\nVersion datasets with W&B Artifacts.  \n\nW&B Artifacts help you save and organize machine learning datasets throughout a project's lifecycle.  \n\n### Common use cases  \n\n1. **Version data seamlessly**, without interrupting your workflow\n2. **Prepackage data splits**, like training, validation, and test sets\n3. **Iteratively refine datasets**, without desynchronizing the team\n4. **Juggle multiple datasets**, as in fine-tuning and domain adaptation\n5. **Visualize & share your data workflow**, keeping all your work in one place  \n\n### Flexible tracking and hosting  \n\nBeyond these common scenarios, you can use core Artifact features to upload, version, alias, compare, and download data, supporting any custom dataset workflow on local or remote file systems, via S3, GCP, or https.  \n\n## Core Artifacts features  \n\nW&B Artifacts support dataset versioning through these basic features:  \n\n## Version data seamlessly",
    "1b1b0683-a388-4177-8b2e-55568efbba4f": "## \ud83e\ude7a Medical Image Classification Tutorial using MonAI and Keras\n\nThis notebook demonstrates\n- an end-to-end training using MonAI and KerasCore.\n- how we can use the backend-agnostic Keras callbacks for Weights & Biases to manage and track our experiment.  \n\nOriginal Notebook: <https://github.com/Project-MONAI/tutorials/blob/main/2d_classification/mednist_tutorial.ipynb>  \n\n## Installing and Importing the Dependencies  \n\n* We install the `main` branch of KerasCore, this lets us use the latest feature merged in KerasCore.\n* We install monai, a PyTorch-based, open-source framework for deep learning in healthcare imaging, part of the PyTorch Ecosystem.\n* We also install wandb-addons, a library that hosts the backend-agnostic callbacks compatible with KerasCore  \n\n## Setup data directory  \n\nYou can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.\nThis allows you to save results and reuse downloads.\nIf not specified a temporary directory will be used.  \n\n## Download dataset",
    "77beb005-d459-4b3e-ad43-8711cb625e24": "## \ud83e\ude7a Medical Image Classification Tutorial using MonAI and Keras\n### Define MONAI transforms, Dataset and Dataloader to pre-process data\nWe typically define a model in PyTorch using `torch.nn.Module`s which act as the building blocks of stateful computation. Even though Keras supports PyTorch as a backend, it does not mean that we can nest torch modules inside a `keras_core.Model`, because trainable variables inside a Keras Model is tracked exclusively via Keras Layers.  \n\nKerasCore provides us with a feature called `TorchModuleWrapper` which enables us to do exactly this. The `TorchModuleWrapper` is a Keras Layer that accepts a torch module and tracks its trainable variables, essentially converting the torch module into a Keras Layer. This enables us to put any torch modules inside a Keras Model and train them with a single `model.fit()`!  \n\nThe idea of the `TorchModuleWrapper` was proposed by Keras' creator Fran\u00e7ois Chollet on this issue thread.",
    "4d334c5b-be70-42fc-9b22-feaff1c354b1": "## 3.Audio input: Record, upload, or URL\n\nYou have several options for audio input:\n1. **Record** audio from your microphone (NOTE: allow microphone access in your browser to do this)\n2. **Upload** audio from a file (.mp3 or .wav)\n3. **Download a URL** (this is hardcoded for the demo, and you can change SONG\\_URL in Step 2 to edit this)  \n\nAdditional notes:\n\\* Audio should be monophonic (single instrument / voice)\n\\* Extracts fundmanetal frequency (f0) and loudness features.",
    "bd9afbdf-63ca-4b67-a54c-36a3ed48fcad": "## Custom Logging for SimpleTransformers\n\nSimpleTransformers automatically logs important metrics to W&B.  \n\nYou can also customize what you log using two methods:  \n\n1. Resuming the run,\n\"restarting\" the experiment so that you can log additional stuff, including more training.\n2. Using the `wandb.api` to update existing runs with additional metadata.  \n\nWe show both below.  \n\n## Use resuming to add model checkpoints  \n\nRuns that have finished can be resumed\nso that additional information can be added to an experiment.\nFor example, you might be using\npre-emptible compute\nwhere training runs can be stopped prematurely.  \n\nHere we use it to log the model checkpoints to the training run,\nsince it was responsible for creating them.  \n\n## Use resuming to add evaluation results as a `Table`  \n\nTo evaluate models and their performance,\nit's important to be able to visualize and analyze model predictions.\nW&B supports this workflow with\nTables.  \n\n## Use the API to attach the train-test splits to the training run",
    "7178c033-7049-4ab8-b4bb-2d7b71a495a8": "## What is W&B?\n\nWeights & Biases (W&B) is the AI developer platform, with tools for training models, fine-tuning models, and leveraging foundation models.  \n\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your models and data are tracked and versioned in a reliable system of record.  \n\nThis diagram outlines the relationship between W&B products.  \n\n**W&B Models** is a set of lightweight, interoperable tools for machine learning practitioners training and fine-tuning models.\n- Experiments: Machine learning experiment tracking\n- Model Registry: Manage production models centrally\n- Launch: Scale and automate workloads\n- Sweeps: Hyperparameter tuning and model optimization  \n\n**W&B Prompts** is for LLM debugging and monitoring, including usage of OpenAI's GPT API.",
    "0bc981f0-490a-4fbf-82e2-71e4479d24a1": "## Embedding Projector\n### Logging Options\n1. Directly from a **dataframe** using `wandb.Table(dataframe=df)`\n2. Directly from a **list of data** using `wandb.Table(data=[...], columns=[...])`\n3. Build the table **incrementally row-by-row** (great if you have a loop in your code). Add rows to your table using `table.add_data(...)`\n4. Add an **embedding column** to your table (great if you have a list of predictions in the form of embeddings): `table.add_col(\"col_name\", ...)`\n5. Add a **computed column** (great if you have a function or model you want to map over your table): `table.add_computed_columns(lambda row, ndx: {\"embedding\": model.predict(row)})`",
    "62279fcb-9801-4c34-82de-45faafca0b2d": "## Line Plot\n### Settings\n* **X axis**: Select default x-axes including Step and Relative Time, or select a custom x-axis. If you'd like to use a custom x-axis, make sure it's logged in the same call to `wandb.log()` that you use to log the y-axis.\n* **Relative Time (Wall)** is clock time since the process started, so if you started a run and resumed it a day later and logged something that would be plotted a 24hrs.\n* **Relative Time (Process)** is time inside the running process, so if you started a run and ran for 10 seconds and resumed a day later that point would be plotted at 10s\n* **Wall Time** is minutes elapsed since the start of the first run on the graph\n* **Step** increments by default each time `wandb.log()` is called, and is supposed to reflect the number of training steps you've logged from your model\n* **Y axes**: Select y-axes from the logged values, including metrics and hyperparameters that change over time.\n* **Min, max, and log scale**: Minimum, maximum, and log scale settings for x axis and y axis in line plots\n* **Smoothing and exclude outliers**: Change the smoothing on the line plot or rescale to exclude outliers from the default plot min and max scale\n* **Max runs to show**: Show more lines on the line plot at once by increasing this number, which defaults to 10 runs. You'll see the message \"Showing first 10 runs\" on the top of the chart if there are more than 10 runs available but the chart is constraining the number visible.\n* **Chart type**: Change between a line plot, an area plot, and a percentage area plot",
    "d61face7-0c43-4b81-a4c7-3c3fd1e28cb0": "## Triggering CI/CD events with model registry changes\n### Delete an automation\n\nDelete an automation associated with a model. Actions in progress are not affected if you delete that automation before the action completes.  \n\n1. Navigate to the Model Registry App at <https://wandb.ai/registry/model>.\n2. Click on a registered model.\n3. Scroll to the bottom of the page to the **Automations** section.\n4. Hover your mouse next to the name of the automation and click on the kebob (three vertical dots) menu.\n5. Select **Delete**.",
    "c3a8a128-ca19-4259-bf60-62f60f7f6cc0": "## Queue monitoring dashboard (beta)\n### Dashboard and plots\nThe dashboard contains a number of plots answering common questions about performance and efficiency. The proceeding sections describe UI elements of queue dashboards.  \n\n### Job status  \n\nThe **Job status** plot shows how many jobs are running, pending, queued, or completed in each time interval. Use the **Job status** plot for identifying periods of idleness in the queue.  \n\nFor example, suppose you have a fixed resource (such as DGX BasePod). If you observe an idle queue with the fixed resource, this might suggest an opportunity to run lower-priority pre-emptible launch jobs such as sweeps.  \n\nOn the other hand, suppose you use a cloud resource and you see periodic bursts of activity. Periodic bursts of activity might suggest an opportunity to save money by reserving resources for particular times.  \n\nTo the right of the plot is a key that shows which colors represent the status of a launch job.  \n\n### Queued time  \n\n### Job runs  \n\n### CPU and GPU usage  \n\n### Errors",
    "4304d321-d00b-467a-bff2-a35999cb7ad3": "## TensorFlow\n### Examples\n\nWe've created a few examples for you to see how the integration works:  \n\n* Example on Github: MNIST example Using TensorFlow Estimators\n* Example on Github: Fashion MNIST example Using Raw TensorFlow\n* Wandb Dashboard: View result on W&B\n* Customizing Training Loops in TensorFlow 2 - Article | Colab Notebook | Dashboard",
    "a4531e43-e6fe-431a-8da1-87776bca67c7": "## PyTorch Ignite\n### The basic PyTorch setup\nUsing WandBLogger in ignite is a 2-step modular process: First, you need to create a WandBLogger object. Then it can be attached to any trainer or evaluator to automatically log the metrics. We'll do the following tasks sequentially: 1) Create a WandBLogger object 2) Attach the Object to the output handlers to:  \n\n* Log training loss - attach to trainer object\n* Log validation loss - attach to evaluator\n* Log optional Parameters - Say, learning rate\n* Watch the model",
    "ed049abf-2315-4b12-8ae4-007ea332c65b": "## Single Sign-On (SSO) setup\n### Configure SSO on the W&B App\nOnce you have everything configured you can provide the Issuer, Client ID, and Auth method to `wandb/local` on the W&B App or set environment variables. The following procedure walks you through the steps to configure SSO with the W&B App UI:  \n\n1. Sign in to your Weights and Biases server\n2. Navigate to the W&B App.  \n\n1. From the dropdown, select **System Settings**:  \n\n1. Enter your Issuer, Client ID, and Authentication Method.\n2. Select **Update settings**.  \n\n:::info\nIf you're unable to log in to your instance after configuring SSO, you can restart the instance with the `LOCAL_RESTORE=true` environment variable set. This will output a temporary password to the containers logs and disable SSO. Once you've resolved any issues with SSO, you must remove that environment variable to enable SSO again.\n:::",
    "932fd31a-1935-444b-a178-b28bbf0cd1d1": "## On Prem / Baremetal\n### Infrastructure Guidelines\n#### Object Storage\nW&B is compatible with an object storage that supports S3 API interface, Signed URLs and CORS. We recommend specing the storage array to the current needs of your practitioners and to capacity plan on a regular cadence.  \n\nMore details on object store configuration can be found in the how-to section.  \n\nSome tested and working providers:\n- MinIO\n- Ceph\n- NetApp\n- Pure Storage  \n\n##### Secure Storage Connector  \n\nThe Secure Storage Connector is not available for teams at this time for bare metal deployments.",
    "2a556476-460a-4f8b-aa3b-da26d7b86350": "## XGBoost Sweeps\n### Visualize your results\n\nNow that your sweep is finished, it's time to look at the results.  \n\nWeights & Biases will generate a number of useful plots for you automatically.  \n\n### Parallel coordinates plot  \n\nThis plot maps hyperparameter values to model metrics. It\u2019s useful for honing in on combinations of hyperparameters that led to the best model performance.  \n\nThis plot seems to indicate that using a tree as our learner slightly,\nbut not mind-blowingly,\noutperforms using a simple linear model as our learner.  \n\n### Hyperparameter importance plot  \n\nThe hyperparameter importance plot shows which hyperparameter values had the biggest impact\non your metrics.  \n\nWe report both the correlation (treating it as a linear predictor)\nand the feature importance (after training a random forest on your results)\nso you can see which parameters had the biggest effect\nand whether that effect was positive or negative.",
    "b7f49724-1b96-4ab5-91b6-7ca23a32e78f": "  \n\n# A Beginner\u2019s Guide To Prompt Engineering  \n\nDescription: Explore the world of prompt engineering in AI through this beginner's guide. Learn its significance, distinctions from fine-tuning, and the art of crafting effective prompts for enhanced AI interactions.  \n\nBody:  \n\n# Introduction  \n\nAs AI systems become more popular, particularly sophisticated language models like GPT-3 and GPT-4, become integral to a diverse range of applications, the skill of crafting precise and effective prompts has emerged as a key competence in the realm of AI.  \n\nThis beginner\u2019s guide to prompt engineering offers an insightful journey into the world of AI interaction, delving into what prompt engineering is, its significance, the nuances of crafting prompts, and the role of a prompt engineer.  \n\n# Table of Contents  \n\n# What Is Prompt Engineering in AI  \n\n# What Is a Prompt in Deep Learning?  \n\n# What Is the Difference Between Fine-Tuning and Prompt Engineering?  \n\n## Fine-Tuning  \n\n## Prompt Engineering",
    "b3dcd7ab-5ee4-4417-ad87-ffa36009de45": "## 3) GPT-4 translation refinement\nIf you speak any of the HBS languages (Serbian et al) you can check out how I compare these 2 in my Discord server [here](https://discord.com/channels/875382728093216809/1182322196065030195/1183414523835715684).  \n\nThe snapshot of the eval at this point is labeled with v1 and can be found here:  \n\nhttps://huggingface.co/datasets/gordicaleksa/serbian-llm-eval-v1  \n\nThis part of the pipeline was generously sponsored by [Weights & Biases](https://wandb.ai/site) \u2764\ufe0f also, a bit later, some local companies and individuals were also kind to help financially - you can see the list of sponsors [here](https://github.com/gordicaleksa/lm-evaluation-harness-serbian). I burned through a lot of GPT-4 credits. :)  \n\nFinally, the only thing left was to consider whether we want to have human annotators improve the evals even further, and that brings me to the last section.",
    "8659a723-2bcf-4b30-b091-6d134d3d7d7f": "## Day 12: Data Analysis & Refinement in LLMs\n### Chapter Highlights\n* Navigating the Dataset: Discover techniques for effectively using the Weights & Biases dashboard to examine synthetic datasets.\n* Evaluating Q&A Combinations: Delve into assessing the quality of synthetic question-answer pairs created by LLMs.\n* Enhancing Prompt Crafting: Acquire knowledge about how continuous refinements in prompt crafting can result in more authentic and precise datasets.\n* Practical Implications: Learn about the real-life application of this data in developing a question-answering app, relating it to practical scenarios.\n* Advanced Evaluation Techniques: Investigate various approaches to further improve and verify the quality of the datasets generated.  \n\n[Access the curriculum for Building LLM-Powered Apps here](http://wandb.me/building-llm-powered-apps).",
    "948b18cc-b1fd-40fc-ab90-30e74aecb50f": "  \n\n# 30 Days of LLMs: Day 5 \u2014 Hands-On LLM Experiments with Jupyter & OpenAI API  \n\nDescription: In Day 5 of the Weights & Biases 30 Days of LLMs, we Explore Jupyter Notebooks & OpenAI API with Darek Kleczek. Dive into practical tokenization demonstrations.  \n\nBody:  \n\nWelcome to [Day 5 of the Weights & Biases 30 Days of LLMs](https://youtu.be/AQqSLPm-504)  \n\nWe'll be taking a day-by-day look at our Building LLM-Powered Applications course\u00a0\u2014 and giving you the chance to win some great prizes!  \n\n# 30 Days of LLMs Contest  \n\nBy [enrolling in our free Building LLM-Powered Applications course](http://wandb.me/building-llm-powered-apps), you will automatically be entered into a prize draw to win the coveted W&B socks. Complete the course, and you'll be entered into the draw to win a pair of Apple AirPods Pro!  \n\n[Find full details on the contest here. ](https://wandb.ai/site/contest-building-llm-powered-apps)  \n\n# Day 5: Hands-On LLM Experiments with Jupyter & OpenAI API",
    "a5c8f29f-ec32-4b7f-8108-ef30282cd6c0": "## Testing Large Language Models with W&B and Giskard\n### The integration of W&B and Giskard\n#### Langchain prompt chaining as a use-case \u26d3\ufe0f\nLet's walk through a real-world use case as a demonstration: generating product descriptions.  \n\nBroadly speaking, LLMs tend to be better at things like product descriptions and ad copy than long form prose. Part of this is that this kind of copy is short and punchy and doesn't require deep expertise to spin up.  \n\nHere, we're looking to generate comprehensive product descriptions to enhance visibility, attract quality leads, and build a strong brand image. Yet, manually writing these product descriptions can be time-consuming and incredibly repetitive.  \n\nWe'll walk through a basic example how this process can be simplified. Given a product name, we'll ask the LLM to process two chained prompts using langchain in order to provide us with a product description. The 2 prompts:  \n\n1. keywords_prompt_template: Based on the product name (given by the user), the LLM has to provide a list of five to ten relevant keywords that would increase product visibility.",
    "a64b13c5-493a-4720-b8cd-2161375769c4": "## How to Evaluate an LLM, Part 2: Manual Evaluation of Wandbot, our LLM-Powered Docs Assistant\n### How to analyze the results from manual evaluation\n#### How accurate are wandbot's responses?\n* Wrong language: The user queries are all in English but a few responses were generated in Japanese. The wandbot ingested both our English and Japanese documentation and a single retriever was used to get the context. Sometimes the retrieved context is in Japanese leading to Japanese response. This is annotated as incorrect because a person asking a query in English will expect the answer in the same language. Our initial hypothesis was that GPT can handle it by itself without us needing to have two separate retrievers. We have since updated wandbot to avoid this language mix up from happening.\n* Documentation limitations: Some of the responses were incorrect because of missing or confusing documentation. This is not exactly wandbot's issue but data source is a crucial part of an LLM-based system. In some responses wandbots suggestion (choice of API, etc.) works, but is a usage pattern we discourage and thus have marked such responses as incorrect. Manual evaluation showed us a few holes in our documentation which should be fixed.\n* Broadly asked question: A few queries are very broad. Ideally the system should be able to judge it and ask for more information from the user. The wandbot's retriever confuses multiple keywords in such broad queries and generates a patched up answer which in practice is not correct.\n* Out of scope: Sometimes user will ask a question that's not directly related to W&B. In such a case, wandbot's response should be to politely inform the user to ask W&B specific question - this is something we have mentioned in the system prompt. We have seen instances where wandbot will retrieve context that to some extent is connected to the question and will make up stuff.\n* Hallucination: We have seen multiple example of hallucinated responses. Either the code snippet is hallucinated or the response is made up by stitching irrelevant contexts. Either way, there is room to improve the quality of our retriever and overall prompt design of wandbot.",
    "aebc0a83-435d-4c91-9dcc-b000c87b7c36": "  \n\n# Crafting Superior Summaries: The ChatGPT Fine-Tuning Guide  \n\nDescription: This article details the fine-tuning of ChatGPT for dialogue summarization, showcasing marked improvements using Weights and Biases for performance tracking and optimization  \n\nBody:  \n\n# Introduction  \n\nDrowning in a sea of endless information? You're not alone. One communication expert estimates that the average knowledge worker must process, consciously or subconsciously, the equivalent of [174 newspapers](http://annenberg.usc.edu/news/future-media/phd-student-calculates-how-much-information-world) of information every day. So, how do we make sense of it all? One word: Summarization.  \n\n# Table of Content  \n\n# Understanding ChatGPT and Summarization  \n\n## ChatGPT Architecture  \n\n## The Importance of Text Summarization  \n\n## Why Do We Need To Fine-Tune ChatGPT for Text Summarization  \n\n# An Overview of Weights and Biases  \n\n# Data Preparation and Annotation  \n\n## The Significance of High-Quality Training Data for Summarization",
    "335bd2cf-4162-47a7-b1d8-ff99cd70dd59": "  \n\n# W&B Product Newsletter: Updates and New Features for September 2023  \n\nDescription: A quick rundown of the big changes in September  \n\nBody:  \n\nWe know how fast the pace of development and evolution with LLMs is, and we\u02bcre cooking up some exciting new tools as part of our W&B Prompts product to serve software developers and ML engineers working with LLMs - stay tuned!  \n\nIn the meantime, data governance continues to be a top priority for ML teams and AI companies we talk to and work with. To that end, we\u02bcre very excited to release Time-to-Live (TTL) policies for W&B Artifacts.  \n\nCheck it out and see what else is new at Weights & Biases!  \n\n# Time-to-live (TTL) policies for W&B Artifacts  \n\n# Miscellaneous Updates  \n\nWe've made a host of improvements and updates across W&B Models, W&B Prompts, and our W&B Core platform over the past month!  \n\n### Weave  \n\n### Model Registry updates  \n\n### Performance and quality-of-life improvements  \n\n## Recent Update Newsletters:  \n\n'",
    "e25b4a63-6479-4e78-93b8-cb9d1046f7bd": "## Reinforcement Learning: An Example\nThe process would involve you (the agent) starting by riding a bicycle. You take actions like pedaling, steering, and maintaining balance while attempting to ride. The environment responds by providing feedback, such as successfully riding for a certain distance without falling or losing balance and falling. Based on the feedback, you adjust your actions and strategy (the policy), gradually learning to pedal more smoothly, steer accurately, and maintain better balance, leading to longer periods of successful riding without falling.  \n\nThrough this process, you learn from your experiences and refine your approach based on the feedback from your actions in the environment, eventually mastering the bicycle-riding skill.  \n\n## Reinforcement Learning: Optimizing Rewards  \n\n## CartPole: A Common RL Challenge  \n\n## Finding the optimal policy algorithm  \n\n## Bellman equation and Markov Decision Process  \n\nThe Bellman equation simplifies the state value or state-action value calculation.",
    "d2451d99-7930-4563-b2b7-f676877a4845": "\"  \n\n# OpenAI Targets Fully Multi-Modal Assistant  \n\nDescription: OpenAI has introduced exciting (and much-anticipated) updates to ChatGPT, which will soon be featuring voice and image recognition capabilities in addition.  \n\nBody:  \n\nOpenAI has introduced exciting updates to [ChatGPT](https://wandb.ai/sundar/Unboxing%20ChatGPT%20-%20A%20Deep%20Dive%20on%20How%20This%20AI-Driven%20Chatbot%20Was%20Trained/reports/Unboxing-ChatGPT-A-Deep-Dive-on-How-This-AI-Driven-Chatbot-Was-Trained--VmlldzozNDA5NzAx), which will soon be featuring voice and image recognition capabilities. Initially, a text-based interaction platform, ChatGPT's leap into voice and image modalities is a super exciting shift in AI-human interactions, adding a layer of depth and context that was previously lacking.  \n\n### Early Access to GPT-4 Image Capabilities: A Brief Episode  \n\n### A New Era of Voice Interactions  \n\n### Image Recognition Makes a Comeback  \n\n### Collaboration",
    "dfd45887-504b-4dd4-926f-4ebb4e87a283": "## ML Best Practices: Test Driven Development at Latent Space\n### Testing with Rigor\n#### Finding the new SOTA\nIf you run the same config many times, you\u2019ll get different results depending on model initialization, randomized shuffled data-loader reads, or the parallel nature of GPU operations. Moreover, they realized they don\u2019t just care about improving the average, or even the min/max. If a change didn\u2019t affect the mean/min/max, but the variance significantly reduced, it would be a win. These kinds of changes make the signal much clearer and reduce the number of runs needed.  \n\nEven during development, the team triggers a training run over 10 times for each of their changes. Using an in-house cloud setup, they can run them all in parallel, then analyze their changes in W&B.  \n\nTo communicate how changes improve their models, all their code pull requests use the following template:  \n\nThey use one or more W&B tags per config, and then use different run sets (with their corresponding tags) in the same section of a report to pull the different arms of the experiment together.",
    "338439ff-7cc1-440b-a61b-acc6358019a1": "## Part I: Best Practices for Picking a Machine Learning Model\n#### What We'll Cover\n* A Royal Rumble of Models\n* Comparing Models\n* Let's get started!  \n\nUnlike Lord of the Rings, in machine learning there is no one ring (model) to rule them all. Different classes of models are good at modeling the underlying patterns of different types of datasets. For instance, decision trees work well in cases where your data has a complex shape:  \n\nWhereas linear models work best where the dataset is linearly separable:  \n\nBefore we begin, let\u2019s dive a little deeper into the disparity between model selection in the real world vs for competitive data science.",
    "a7f48ebe-c78b-4d61-b122-441f05d8aac3": "## Part II: A Whirlwind Tour of Machine Learning Models\n### Regression \u2192 Linear Regression \u2192 Lasso, Ridge, Elastic-Net Regression\n#### Regression \u2192 Regression Trees \u2192 Decision Tree\n* Low(er) prediction accuracy\n* Requires some parameter tuning\n* Doesn't do well on small datasets\n* Doesn't separate signal from noise well\n* Not easy to update the model when new data comes in\n* Used very rarely in practice, use ensembled trees instead\n* Can overfit (see ensembled models below)",
    "2736c9e1-6dbc-4ace-ab90-ccb9bf7e8d72": "## What Is Baysian Hyperparameter Optimization?\n\nBayesian hyperparameter optimization is a  technique for finding the best settings for the \"knobs\" of your machine learning model \u2013 the hyperparameters \u2013 that control its performance. Unlike traditional methods like those noted above, which try every possible combination blindly, Bayesian optimization uses a smart and efficient approach to guide its search, informed by previous evaluations.  \n\n## Bayesian Hyperparameter Tuning: Nuts & Bolts  \n\nWhen it comes to using Bayesian principles in hyperparameter tuning the following steps are generally followed:  \n\n* Pick a combination of hyperparameter values (our belief) and train the machine learning model.\n* Get the evidence (i.e., the score of the model)\n* Update our belief that can lead to model improvement.\n* Terminate when a stopping criteria is met (generally when a loss a quantity is minimized or classification accuracy is maximized).  \n\n## Expected Improvement  \n\n## Surrogate Model",
    "f9267d9d-ef3a-471f-8933-8bd71601eace": "## Classifying Tweets with Weights & Biases\nNote: You can find the accompanying code in [this Colab Notebook](https://colab.research.google.com/drive/11d92aCLGsvcLTxymar2BPm3WEvSqYGBN). We highly encourage you to fork it, tweak the parameters, or try the model with your own dataset!  \n\n## Setup  \n\nStart out by installing the experiment tracking library and setting up your free W&B account:  \n\n* pip install wandb \u2013 Install the W&B library\n* import wandb \u2013 Import the wandb library\n* from wandb.keras import WandbCallback \u2013 Import the wandb  \n\nThe dataset is called \u201cDisasters on Social Media\u201d, which is [gathered from Figure Eight](https://www.figure-eight.com/data-for-everyone/). Contributors looked at over 10,000 tweets culled with a variety of searches like \u201cablaze\u201d, \u201cquarantine\u201d, and \u201cpandemonium\u201d, then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous).  \n\n## Prepare The Target  \n\n## Lemmatization  \n\n## Word Embeddings  \n\n## Feedforward Neural Network  \n\n## Comparison",
    "01df772e-3cbf-46cb-80f3-938f5438784f": "  \n\n# Fine-Tuning ChatGPT for Text Generation With W&B  \n\nDescription: An in-depth guide on fine-tuning ChatGPT for text generation using Weights & Biases, highlighting the importance of data quality and model adaptation for specific tasks.  \n\nBody:  \n\nFew companies have the financial or compute resources to train LLMs on their own, so it follows that we're seeing far more fine-tuning than training from scratch. What's more, fine-tuning allows us to adapt the general LLM to our specific task, potentially increasing its performance. But how does one navigate the complex terrain of a behemoth like [ChatGPT](https://wandb.ai/sundar/Unboxing%20ChatGPT%20-%20A%20Deep%20Dive%20on%20How%20This%20AI-Driven%20Chatbot%20Was%20Trained/reports/Unboxing-ChatGPT-A-Deep-Dive-on-How-This-AI-Driven-Chatbot-Was-Trained--VmlldzozNDA5NzAx)?  \n\n# What Is Fine-Tuning in Machine Learning?  \n\n# Fine-Tuning for ChatGPT  \n\n# Introduction to Weights & Biases (W&B)  \n\n# Preparing for Fine-Tuning  \n\n# Fine-Tuning ChatGPT  \n\n# Conclusion",
    "a57de493-81ee-4437-a83e-5e9a82143601": "  \n\n# Prompt Engineering for LLMs: A Practical, Conceptual Guide  \n\nDescription: Exploring what actually works in prompt engineering  \n\nBody:  \n\n## Introduction  \n\nWhat is prompt engineering? When I first heard of this field, it seemed a little funny. Engineering what you want to say to an AI model? Are you testing my social skills?  \n\nI had never really given much thought to prompt engineering. It seemed as simple as learning how to Google search and that wasn't something AI researchers actively explored. What, then, is this all about? Well, the Large Language Models (LLMs) of today like ChatGPT, GPT-4, and the hundreds of competitor models don't just ingest your \"search query\" and make do with it. Like a computer vision model vulnerable to noise or adversarial attacks, slight changes to the input of an LLM does affect its outputs.  \n\nSo what's prompt engineering? Simply put:  \n\n## Table of Contents  \n\n## Methods  \n\n## Prompt Problems  \n\n## Resources  \n\n## References",
    "13a49f0a-2db7-4614-b568-a7233b2baee1": "\"  \n\n# Meta Releases Code Llama  \n\nDescription: Meta continues to release LLM's to the public, with Code Llama, a LLM designed specifically for programming!  \n\nBody:  \n\nCode Llama, a large language model (LLM), is now available and has been specifically designed to generate code from textual prompts. It stands out as a leading solution for publicly available LLMs on coding tasks.  \n\nThe model is being offered in three distinct versions:  \n\n- Code Llama, the foundational code model  \n\n- Codel Llama - Python, tailored for Python coding  \n\n- Code Llama - Instruct, which has been fine-tuned to comprehend natural language instructions  \n\nIt's released free of charge for both research and commercial applications, and the models are built on the foundation of Llama 2.  \n\n### Several Versions  \n\n### Performance  \n\n### Open Source",
    "8a6a014f-3cf8-4f14-a20f-58067dedde4a": "## This week in AI: Meta LLaMA 2, Meta-Transformer, StabilityAI FreeWilly\n### Meta-Transformer\n\nMeta-Transformer, not from Meta, is a unified, multi-modal Transformer architecture!  \n\nIt's safe to say this transformer is really multi-modal, not just text and images. Their website has a great video, below, walking through their paper's method.  \n\nThe overall architecture of their Meta-[Transformer](http://wandb.ai/fully-connected/blog/transformer) consists of a data-to-sequence tokenizer layer which, itself, consists of multiple modality-specific tokenizers. The tokenized input enters a shared token space which can all be fed into the unified model. The output of this unified model is fed into task-specific models.  \n\nThey benchmarked their model across dozens of benchmarks and other models!",
    "3168e40f-c6ce-464b-a5ae-40054507c614": "## Part 2: Applications\n### Converting Text into Dataframes\n#### Defining the Data Structures\nThe RowData class represents a single row of data in the dataframe. It contains a row attribute for the values in each row and a citation attribute for the citation from the original source data.  \n\nThe Dataframe class represents a dataframe and consists of a name attribute, a list of RowData objects in the data attribute, and a list of column names in the columns attribute. It also provides a to_pandas method to convert the dataframe into a Pandas DataFrame.  \n\nThe Database class represents a set of tables in a database. It contains a list of Dataframe objects in the tables attribute.  \n\nNow we can define our own extraction function as usual and see what happens.",
    "36b6a970-c5ba-4946-97bc-d6cb391fe1ff": "  \n\n# Understanding the Difference in Performance Between Binary Cross-Entropy and Categorical Cross-Entropy  \n\nDescription: This article breaks down binary and categorical cross-entropy in simple terms, explaining their roles in machine learning. It also highlights their differences, especially how they handle different types of data and how that impacts the results.  \n\nBody:  \n\n# Introduction  \n\n# What is Binary Cross-Entropy?  \n\n[Binary cross-entropy](https://arize.com/blog-course/binary-cross-entropy-log-loss/#:~:text=What%20Is%20Binary%20Cross%20Entropy,equate%20to%20high%20accuracy%20values.) is a handy loss function used in binary classification tasks to measure how well the predicted probabilities align with the true binary labels. It quantifies the dissimilarity between the predicted probabilities and the actual labels, giving us a sense of how accurate the model's predictions are.  \n\n## How is Binary Cross-Entropy Calculated?  \n\n## Binary Cross-Entropy: Use Cases in Neural Network  \n\n# Conclusion",
    "470ecd30-128f-436b-9aef-35b1a2d8811e": "## \ud83c\udfc1 Conclusion\n* In this article, we present a comprehensive set of benchmarks of XLA-compatible pre-trained vision models in .\n* We analyze the possible gains from XLA across different image resolutions and different GPU devices (A100, V100, and T4) for all vision models shipped by , , and .\n* We briefly explore how XLA can be used to optimize TensorFlow programs using Operator Fusion and other techniques.\n*  We use , an open-source toolkit developed by the team at  for generating performant, interactive, and insightful data visualization panels to aid our analysis and present insights.\n* We are immensely grateful to\n* For further resources on XLA, we recommend the following resources:  \n\n'",
    "9e81ec30-8411-4d23-a916-4c66d3a3063d": "  \n\n# Devsisters Levels Up Game Development with W&B  \n\nDescription: Discover how Devsisters uses W&B to apply reinforcement learning and accelerate game production  \n\nBody:  \n\n# Keeping Players Engaged  \n\nWhen it comes to mobile games, you either become a user's break time essential or lose them in the first few weeks of download. With almost [700,000 mobile games available in the app stores](https://www.blog.udonis.co/mobile-marketing/mobile-games/mobile-gaming-statistics#:~:text=How%20Many%20Mobile%20Games%20Are,200%2C000%20on%20Apple%20App%20Store.), it's not surprising people feel compelled to explore the latest and greatest.  \n\nWith a franchise like the Cookie Run IP, loved by more than 200 million users worldwide, global entertainment and gaming company, Devsisters, knows a thing or two about the makings of a loyal fanbase.  \n\n# Harnessing Reinforcement Learning  \n\n# Deriving Insights in Real-Time  \n\n# Empowering Game Developers",
    "1208e94b-146a-4e77-893d-b8c12d9620db": "\"  \n\n# Join Us at Fully Connected 2023 in San Francisco on June 7th  \n\nDescription: We're throwing an in-person conference in two weeks. We'd love to see there.  \n\nBody:  \n\n---\nWe had a great time hosting our first virtual conference earlier this year. Such a great time, in fact, that we\u2019re throwing a one-day, in-person conference in San Francisco. We\u2019d love it if you joined us. Fully Connected 2023 is open to everyone and will be taking place June 7th in San Francisco. If you'd like to attend, click the button below. If you'd like to learn more, keep reading!  \n\n## What\u2019s the Fully Connected Conference about?  \n\nOur event is focused on the generative models and LLMs transforming the world today. You\u2019ll hear from the creators of the most exciting ML tools like LangChain and OpenAI, people building your favorite platforms like AWS, NVIDIA, Lightning, HuggingFace, Kaggle, and companies having a positive impact on the world through machine learning.  \n\n## Who's speaking at Fully Connected?",
    "b405ec05-3e47-4564-8eb0-b5e038c89b2f": "## CI/CD Triggers\n### 2) A model starts degrading in production\n* Gantry alerts your team that a model is performing very poorly.  Automation instructs SageMaker to fall back to a baseline model from Metaflow artifacts.\n* ML Engineers perform a variety of ad-hoc experiments to investigate the regressions and perform error analysis.  Many experiments and analyses are conducted, which are tracked and shared through W&B.\n* Subsequently, a peer review process facilitated by W&B can be used to select a new deployment candidate.\n* The winning experiment is then promoted from W&B to Metaflow, where a final production run is executed, and results are automatically verified for consistency with W&B.\n* The model deployment process continues in the same way as our first example.",
    "11174598-146b-4504-b3a7-ca2747ad0ca5": "  \n\n# Dialogue Summarization with Flan-T5  \n\nDescription: In this article, we explore model recycling, and train a T5 text-to-text transformer model to summarize dialogue and achieve some unexpected results.  \n\nBody:  \n\nRecently, [natural language processing (NLP)](http://wandb.ai/fully-connected/blog/nlp) has undergone significant transformations in how it 'understands' and interacts with the world.  NLP tasks like summarization were particularly difficult or costly just half a decade ago. These tasks often required hundreds of hours of subject-matter expertise to produce annotated training datasets \u2013 expert-authored summaries of long documents such as court cases or medical records \u2013 or countless hours spent validating and auditing the machine-produced summaries to ensure that they had high fidelity to the original, long-form texts.  \n\n# The Text-to-Text Transfer Transformer (T5)  \n\n# Summarization Before Transformers  \n\n## Extractive Summarization in the Past  \n\n## Abstractive Summarization",
    "62140942-fd9d-4c7d-8682-c5f7f15e88d0": "## Example Project: Fine-Tuning an Image Segmentation Model\n* Image Segmentation Walkthrough with SegFormer \ud83d\udcf8: Model usage on this specific domain task and how the dataset interacts with them.\n* Training + Weights & Biases experiment tracking \ud83e\ude84 + \ud83d\udc1d: Training, hyperparameter optimization, and experiment tracking using Weights & Biases (W&B).\n* Training script via command line \ud83d\ude80: A section to experiment with different model configurations using a training script.  \n\nThese three sections broadly follow the development of the project.  \n\nFirst, we'll understand how to use the model and the dataset manipulation to feed it. Next, we'll start working on training, namely what we want to track and record, and the hyperparameters configurations (such as the learning rate and batch size).  \n\nThese two initial sections work as an internal exploration and as documentation for anyone who wants to understand the project. The third and final step is an engineering effort to make life easier and get the job done.",
    "4054e145-16af-4ba8-9996-99a57f9c6e28": "## Applications of Named Entity Recognition\n### NER in Business\nCompanies often receive a large volume of unstructured data in the form of emails, customer feedback, social media posts, etc. NER can be used to extract important entities such as product names, company names, locations, and people mentioned in these texts. This can help businesses to better understand what their customers are talking about and identify key issues or trends.  \n\nThe example below uses SpaCy pre-trained NER model to:  \n\n* Extract named entities from the text and creates nodes for a knowledge graph.\n* Identifies relationships between entities and creates edges for the knowledge graph.\n* Creates a directed graph using the nodes and edges.\n* Plots the graph using the `networkx` and `matplotlib` libraries.",
    "f5268ae1-c49e-4921-97e4-befca1bc2698": "  \n\n# David Guetta Predicts the Future of Music is AI  \n\nDescription: World renowned EDM artist David Guetta sees the new technology as a catalyst for future musical innovation  \n\nBody:  \n\nMusic perhaps just experienced its own \u201ciPhone moment\" at a David Guetta Concert recently, as Guetta used an AI generated Eminem line within one of his songs, and the result was a strong approval from fans [1].  \n\n## Davids Tweet  \n\n## The Future of Music  \n\nGuetta sees AI as a major tool for artists, and mentioned that he believes every new music style has roots in a new technology. Looking back, one of the most revolutionary technologies of the century has been the internet.  \n\n## Company Analysis  \n\nDiving deep into company activity, one of Apples most recent Acquisitions was Music AI.  \n\n## References  \n\n[1] https://www.bbc.com/news/entertainment-arts-64624525  \n\n[2] https://www.macrumors.com/2022/02/07/apple-acquires-ai-music/",
    "2467eee9-eeb0-499f-a486-b0dd870a878e": "## But What's Going On In Search?\n\nSo far we've cover a few machine learning models and not much else. It's time to look at what's going on presently in search, as well as where I believe things are headed. You'll obviously want to take that part with a grain of salt, as things are moving fast.  \n\nLike, \"Obliterating Moore's Law\" fast.  \n\nWhat we're seeing right now is the beginning of a new \"arms race\" in search.  \n\nMicrosoft has been invested in OpenAI (and by extension GPT and ChatGPT) since 2019. At that time they invested $1 billion dollars and have added $2 billion more between then and the $10B they added on January 23, 2023, landing a 49% stake in the company.  \n\nIn short, they didn't see ChatGPT and decide they wanted to add it to their search engine, they saw the potential to help develop it and similar models and then did.  \n\nGoogle/Alphabet, rather than investing in outside companies, simply invested in developing their own Large Language Models (LLMs).",
    "f8449ee9-5cd0-4024-9959-fa7bc64e5011": "## Google's BARD and Other Feb 6 Happenings\n### References\nDastin, Jeffrey. \u201c[Google Unveils CHATGPT Rival Bard, Ai Search Plans in Battle with Microsoft.](https://www.reuters.com/technology/google-opens-bard-chatbot-test-users-plans-more-ai-search-2023-02-06/)\u201d Reuters, Thomson Reuters, 6 Feb. 2023  \n\nMegan, Smalley. \u201cRecycleye Raises $17M in Series A Funding.\u201d Recycling Today, https://www.recyclingtoday.com/news/recycleye-raises-17-million-series-a-funding/  \n\nKokalitcheva, Kia. \u201c[Latent Technology Bringing Generative AI to Video Games.](https://www.axios.com/2023/02/06/latent-technology-21-million-generative-ai-video-games)\u201d Axios, 6 Feb. 2023  \n\n[Magic.dev Raises $28 Million to Build Ai Software Engineer](https://www.prnewswire.com/news-releases/magicdev-raises-28-million-to-build-ai-software-engineer-301738983.html). 6 Feb. 2023",
    "8dbc92bd-c105-4ae1-a012-5d077e1e2368": "## Distilling Desired Values Through RLHF\n### Deep Dive Into the Three RLHF Steps\n#### 2. Reward Model (RM) training\n...where \u03b8 is the RM's parameters, y_w is the preferred response from the pair of y_w and y_l based on rankings. The difference here represents the log odds that one response will be preferred to the other by a human labeler. Since RM loss is invariant to shifts in reward (difference of two reward values), the RM is normalized using a bias so that the labeler demonstrations achieve a mean score of 0 before doing RL.  \n\n* Evaluation: Despite a larger 175B RM resulting in lower validation loss, a 6B model was chosen as its training was much stable and efficient.",
    "6e5c7362-185d-4016-af4d-4876151e2160": "## Example: Language Modeling and Question-Answering using HuggingFace and W&B\n### Question-Answering with pre-trained models using HuggingFace and W&B\n\nNext, we'll explore how to use the saved Masked Language Model to build an extractive question-answering system. Wait, what's an extractive QA system?  \n\nAs we saw earlier, Question-Answering systems can be of two types:  \n\n* Extractive: The answers will be extracted from a given context. For example, if we provide the context: `I'm Madhana and I'm from India` and ask the question: `Who am I?`, the QA system should extract the answer from the context we provided and answer `Madhana`.\n* Abstractive: Abstractive question-answering is used to produce answers for open-ended questions. Unlike extractive question answering, which involves selecting an answer from a given set of text passages, abstractive question answering creates answers by synthesizing information from multiple sources like document stores.  \n\n### Step 1: W&B Set-Up  \n\n### Step 2: Load the SQuAD Dataset  \n\n### Step 3: Data Preprocessing  \n\n### Step 4: Model Building",
    "2f0f0e02-6579-47bf-939d-db32e58eb095": "## Transcript (from Whisper)\nThe next thing I want to discuss is error analysis. And error analysis is one of the most critical parts of model evaluation. It's really the opportunity to improve your models and gain deep insights into where your model could be wrong or where it's struggling or where you can improve your model.  \n\nAnd honestly, this is where I personally have the most fun in the entire machine learning process because this is where I get to build more intuition about the domain and how my model is interacting with that domain.  \n\nSo what is error analysis and how does weights and biases fit into that?  \n\nSo the way error analysis works is we're going to log a table to two Weights & Biases.  \n\nYou may have already seen these tables in previous lessons, but essentially it's table with predictions and ground truth and comparing those ground truths to predictions with all the various metrics.",
    "939a9586-7685-4b4f-b2a2-2b4bf87289a7": "## Transcript (from Whisper)\nSo what I can do is if I click road here, well, you can see like this orange area is vehicles and you can expand this to see, okay, orange area is vehicles, road is not even in the ground truth.  \n\nHere is clearly road and this model is trying to predict road. And let's take a look, let's take a, let's stack it on top of each other. Let's even take this off.  \n\nThe model is doing a pretty reasonable job at the road part of it. See, these are the predictions.  \n\nI can see do none to unselect everything and just say road and that's pretty reasonable. There's this blip up here. So what is the takeaway here?  \n\nThe takeaway from this specific example is the labels are wrong in our ground truth clearly because we can, if I do this and select road, there isn't, they don't have road.  \n\nSo we need to go back to our labeling process and potentially figure out what's going on, why roads are not being labeled appropriately. Okay, let's look at another one.",
    "ee41f7de-ce09-4149-ac60-915c42cab0bf": "\"  \n\n# Hyperparameter Tuning with W&B Sweeps  \n\nDescription: Learn hyperparameter tuning using Weights & Biases. This video is a sampling from the free MLOps certification course from Weights & Biases!  \n\nBody:  \n\nHyperparameter tuning is a crucial step in the machine learning process, as it can significantly impact the performance of your model. However, finding the optimal hyperparameters can be a time-consuming and manual process involving the testing of multiple combinations and the tracking of results. In this video from [our MLOps course](https://www.wandb.courses/courses/effective-mlops-model-development), we show you how to use Weights & Biases Sweeps to automate the hyperparameter tuning process.  \n\n# Transcription (from Whisper)  \n\nHello again, let's take care of the original task, improving the model performance.  \n\nHow can we make the model better, and increase the intersection of a union metric?",
    "e2f3a16c-7ddc-49c8-acdd-a6565e2adc69": "## Conclusion\n\nIn this article, we explore how to build a model for segmenting 3D point clouds.  \n\n* We explored a Dynamic Graph CNN model for segmenting 3D point clouds as proposed by the paper .\n* We explore the EdgeConv module as the building block of our model and how we can build it easily using .\n* We trained and evaluated our model on different categories of the  dataset. In order to explore this dataset interactively and check the performance of various models on this dataset on a leaderboard, check out the following article:  \n\n* In order to check how to build a model for the classification of point clouds, check out the following report:  \n\n\"",
    "a53e4e7b-3e38-46e0-bbbb-a87228b6065c": "## Google For India 2022: Vaani, BINDI, Language Models, Search Enhancements, And More\n### Google For India 2022 AI Announcements\n#### New AI features in search and accessibility\nIndia is the largest adopter of Google's expanded search features like Google Lens, multisearch, and even simple voice search. To honor this high usage, Google is adding several new features to Google Search in India, as well as working on features to make it accessible for all users.  \n\n* Multisearch gets Indian language support. Google's multi-modality search method, , allows users to search using images and text together. Support for Indian languages is coming to multisearch, starting with Hindi, and will expand to several other Indian languages over the coming year.\n* Searching video content with natural language. Videos in search results will now include a search widget to search through the video with language input, eliminating the need for annoying and time-consuming video scrubbing.",
    "256e5ed1-b309-4b30-bcfc-1786a8e9ff3c": "## Google For India 2022: Vaani, BINDI, Language Models, Search Enhancements, And More\n### Google For India 2022 AI Announcements\n#### AI for reading doctor's notes\n\nDoctor's notes are notoriously difficult to read; Full of quick writing, shorthand phrases, and more, even pharmacists often have to work to figure out their meaning. To help pharmacists decipher handwritten doctor's notes, Google is working on an AI model that can take a look at a doctor's note and determine all the medications found on it.  \n\nGoogle Cloud is also working with [Apollo Hospitals](https://www.apollohospitals.com/) to bring machine learning compute to healthcare in India through a Clinical Intelligence Engine for AI-supported diagnosis, medical record management, and data-driven expertise.",
    "2d93765c-a883-46a9-8462-af99f43a6c92": "## Robotics Transformer: Google's Open-Source Architecture For Robotics Generalization\n### How Robotics Transformer works\n#### The model\nRT-1 takes in a set of images alongside natural language text instructions. This input data is immediately fed into the first section of the model (16 million parameters) which tokenizes the data into a small set of combined text-visual tokens. The second section of the model (19 million parameters), the core transformer network of RT-1, determines final action.  \n\nTokenizing the input data with a modified FiLM-EfficientNet:  \n\nRT-1 starts off with tokenizing the input data. The tokenization process involves a modified [ImageNet](https://www.image-net.org/) pretrained [EfficientNet](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html) convolutional neural net. By itself, an EfficientNet will modify an input image into a differently shaped feature map, but for RT-1 it's been modified in a few different ways.",
    "f3c81f4c-cebd-4108-be71-ec909626ec09": "## Is attention all you need to compensate for poor tokenization?\n### Comparing context and tokenization\n#### The colors of bluetooth\nIt looks like the similarity increased since the stem of the subword tokenization is \u201cblue\u201d and this obviously has a similarity with the color blue even though it is used in a very different context here. If \"Bluetooth\" were stored as a single token here, it is likely it would not be as easily mis-associated with the color blue.  \n\nWhat happens if we use a nonsensical sentence with the phrase \u201cblue teeth\u201d which actually has no valid meaning in this context?  \n\nThe similarity actually increased here since it also found a similarity with \u201cteeth\u201d and \u201c##tooth\u201d. To verify this, let\u2019s compare the other sentence where we referred to \u201cwireless technology\u201d:",
    "bcd47eef-757e-40b4-84a7-0fe011d731b0": "\"  \n\n# NeurIPS 2022, 36th Annual Conference For Machine Learning, Has Arrived  \n\nDescription: The 36th annual NeurIPS conference has arrived, bringing people across the ML and AI industry together with highlights on the most recent advancements and research.  \n\nBody:  \n\n[NeurIPS 2022](https://nips.cc/virtual/2022/index.html), the 36th annual conference for everything machine learning and artificial intelligence, has arrived today. The conference brings researchers, developers, and business partners alike to attend workshops, panels, social events, and more while highlighting the most recent advancements in the field with paper submissions.  \n\n## The schedule  \n\nThe in-person portion of the conference will be running from November 28th through December 3rd, hosted at the New Orleans Conference Center. This first week contains the bulk of experiences (panels, workshops, and more), and is also available online through a virtual pass for those who can't attend in person.  \n\n## Company presence and awards",
    "f0f84253-072e-4e46-99ec-4c27c12a5437": "## Model Reproducibility Using Activeloop Deep Lake and Weights & Biases\n### Improving the Model Performance\nAfter retraining the model using the same code above, we observe that the average IOU increased from 0.29 to 0.37, which is substantial given the simple increase in image resolution.  \n\nThe model is still not production-ready, and further opportunities for improvement are:  \n\n* Assessing model performance on a per-image basis, which helps identify mislabeled or difficult data. A full description of this workflow is available .\n* Adding random real-world images that do not contain the objects of interest. This helps the model eliminate false positives.\n* Adding more data to the training set. 3000 images is likely not enough for a high-accuracy model.\n* Strengthening of transformations that affect image color, blur, warping, and others\n* Exploring different optimizers, learning rates, and schedulers\n* Further increasing the transform resolution until diminishing returns are achieved",
    "a8daca8b-329e-4b1e-a90b-5b4443f8605c": "## Anomalous Data in Your Autonomous Data\n### Data Provenance: Why Use Datasets like BDD100K?\nQueue the BDD100K dataset: gathered using mobile phone cameras, under suboptimal environmental conditions, in densely-populated urban areas with large amounts of unpredictable pedestrian and vehicular traffic, the BDD100K dataset aims to fill a need faced by autonomous vehicle companies and their data providers. Namely, the need for 'challenging' data on which to train a self-driving vehicle model. After all, it's not a matter of if  but when.  \n\nAnd when a self-driving vehicle encounters these rare, adverse scenarios? The hope is that it will have already seen similarly-challenging examples. At that point, the model will either fail elegantly and safely\u2013requesting that the human driver briefly take control of the wheel\u2013or it will succeed in maintaining automated control of the vehicle.  \n\nNexar's cofounder and CTO Bruno Fernandez-Ruiz has this to say about the BDD100K dataset, the data collection method, and the challenges faced by AV model builders who have 'too clean' of training data:",
    "b16415d3-4307-412c-bb3c-605acf952d60": "## Google's Smell AI Can Predict Scents & Repel Mosquitoes\nVisual and auditory mediums have always taken the front stage when it comes to sensory-based AI projects, but the medium of smell, the olfactory sense, is rarely considered. Perhaps, like a taste or touch AI, the utility is not immediately apparent, but this new research makes the case that there is certainly a space for AI models dealing with smells.  \n\n## Understanding scent with AI and POM  \n\nOne of the big pieces to highlight in this new research on display today is the development of a scent map, POM, which can represent smells in relation to each other similar to how colors are shown on a color map or wheel. POM, however, maps smells within a high-dimensional space based on the vector representations present in the smelling model's embedding space.  \n\n## Predicting new scents & repelling mosquitos  \n\n## Find out more  \n\n[Read the full blog post for more details by clicking here.](https://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html)  \n\n\"",
    "8579658d-fa50-4239-9afb-91661f30f9e8": "## Breaking Down Autonomous Vehicle Tasks\n### Path Planning, Decision Making and Motion Control\n\nThe next step, now that we know what's around us, where the empty spaces are, where other cars are located, and where they may be located some time in the future?  \n\nNow we need to decide whether we need to keep going straight or steer or slow down to reach the destination. This process entails path planning and decision-making.  \n\nThe task of the path planning module in autonomous vehicles is used to select a route through the road network which takes it from its current position to the destination while avoiding obstacles seen at that instant.  \n\nIn a dynamic scenario where the autonomous vehicle must navigate in any environment and scenario, this is not always feasible.  \n\nThe alternative is using different algorithms to generate multiple possible future scenarios in which the vehicle takes different paths to reach the destination, and where one of these paths is chosen depending on multiple parameters like time taken and feasibility.  \n\n>",
    "d2950946-3fc1-4add-84e5-7c1916598b73": "## Assessing the Feasibility of The Project\n### Collecting our data\n\nNow it\u2019s time to collect the relevant data. Below is the data we have collected in this scenario. In practice, you might have to make educated guesses for some of these numbers with your business partners. This process may take some time; however, it is well worth it so you can properly assess the feasibility of the project.  \n\n* Number of lemons sold per month: 1.5 million\n* Cost of labor: $100,000 per month\n* Percentage of lemons with mold: 15%\n* Current false negative rate: 27%\n* Target false negative rate: 20%\n* Cost of false negative per lemon: $1.50\n* Cost of false positive per lemon: $0.25\n* Current false positive rate: 3%\n* Cost of Data Science time & infrastructure: $15,000",
    "6e875908-e104-422b-9019-59c01f8e02f4": "  \n\n# An Introduction to BERT And How To Use It  \n\nDescription: In this article, we will explore the architecture behind Google\u2019s revolutionary BERT model and implement it practically through the HuggingFace framework BERT NLP.  \n\nBody:  \n\nBefore we dive into our article on BERT, let's first look at what we'll be covering.  \n\n## Table Of Contents  \n\n# Natural Language Processing  \n\n[Natural Language Processing (NLP)](https://wandb.ai/fully-connected/blog/nlp) is a branch of machine learning that focuses on the interaction of computers with human language to perform specific tasks including [language generation](https://wandb.ai/fully-connected/blog/language-generation), summarization, [translation](https://wandb.ai/fully-connected/blog/translation), and [sentiment analysis](https://wandb.ai/fully-connected/blog/sentiment-analysis).  \n\n# What is BERT?  \n\n# How does BERT work?  \n\n# What Machine Learning Tasks Is BERT Used For?  \n\n# BERT-based Models  \n\n## 1. FinBERT  \n\n## 2. RoBERTa  \n\n## 3. DeBERTa",
    "61bb5b7a-b615-47d0-9334-2b31f4127522": "\"  \n\n# Building Diverse Skillsets for Video Game Characters With Adversarial Skill Embeddings  \n\nDescription: In this article, we explore using large-scale reusable adversarial skill embeddings for physically simulated characters.  \n\nBody:  \n\nHumans are capable of performing an awe-inspiring variety of complex tasks by drawing on our vast repertoire of motor skills. This repertoire is built over a lifetime of interaction with the environment, leading to general-purpose skills that can be widely reused to accomplish new tasks.  \n\n# Overview of the Framework  \n\nThe Adversarial Skill Embedding framework consists of two stages: pre-training and transfer.  \n\n# The Pre-Training Stage  \n\n# Task Training  \n\nAfter pre-training, the low-level policy can be reused to perform new downstream tasks by training a high-level policy to specify latent variables for directing the low-level policy toward completing the desired objectives.  \n\n## Demonstrations of Downstream Tasks  \n\n# Similar Reports  \n\n\"",
    "a9ab29a1-8a93-4938-8bab-7914f3c202b2": "## MT-YOLOv6: A YOLO-Inspired Object Detection Model Released\n[MT-YOLOv6](https://github.com/meituan/YOLOv6) (or just [YOLOv6](https://github.com/meituan/YOLOv6/blob/main/docs/About_naming_yolov6.md) for short) is a newly released model which was inspired by Ultralytics' YOLO models and is developed by researchers at [Meituan](https://tech.meituan.com/). All the code needed to get this model running is available in the GitHub repository, including pre-trained weights for nano, tiny, and small model sizes (4.3M, 15M, and 17.2M parameters respectively) - larger model sizes are coming soon.  \n\n## How does MT-YOLOv6 compare to YOLOv5?  \n\nAccording to benchmarking performed by the team, YOLOv6 outperforms YOLOv5 and other YOLO models in both accuracy and speed on the [COCO dataset](https://cocodataset.org/). The following image (taken from the GitHub) shows:  \n\n## Find out more  \n\n[Read their detailed report by clicking here.](https://tech.meituan.com/2022/06/23/yolov6-a-fast-and-accurate-target-detection-framework-is-opening-source.html) - You may need to translate.",
    "a1d33b87-3a8a-48cc-8950-e763f3ddc1b6": "\"  \n\n# How To Use Autocast in PyTorch  \n\nDescription: In this article, we learn how to implement Tensor Autocasting in a short tutorial, complete with code and interactive visualizations, so you can try it yourself.  \n\nBody:  \n\nIn this article, we'll look at how you can use the [torch.cuda.amp.autocast()](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast) in PyTorch to implement automatic Tensor Casting for writing compute-efficient training loops.  \n\nUnlike Tensorflow, [PyTorch](http://wandb.ai/fully-connected/blog/pytorch) provides an easy interface to easily use compute efficient methods, which we can easily add into the training loop with just a couple of lines of code.  \n\n>  \n\n### Table of Contents  \n\n## Show Me the Code  \n\nMost PyTorch training loops are of the following form:  \n\n## Summary  \n\n## Recommended Reading  \n\n\"",
    "b9ff4802-ea02-4ac2-91cc-3d93b3737cb1": "\"  \n\n# Multi-Task Learning as a Bargaining Game  \n\nDescription: In this article, we explore gradient combination in multi-task learning (MTL) as a cooperative bargaining game, and discuss Nash MTL \u2014 a novel approach \u2014 in detail.  \n\nBody:  \n\nMulti-task Learning, or MTL, is a paradigm in which a joint model is trained to simultaneously make predictions for several tasks. Joint training reduces computation costs and improves data efficiency. However, since the gradients of these different tasks may conflict, training a joint model for MTL often yields lower performance than its corresponding single-task counterparts.  \n\n# Relevant Background On MTL  \n\nLet us set up and revisit some concepts which would act as the background for our understanding of the concepts of Nash-MTL as proposed by the authors.  \n\n## Pareto Optimality  \n\nOptimization for Multi-task Learning is a specific case of Multi-objective Optimization, which can be formally defined as follows:  \n\n## Nash Bargaining Solution  \n\n# Practical Speedup",
    "9592eb3c-9284-49b7-a953-b20002c70d5f": "\"  \n\n# Fun With Neural Cellular Automata  \n\nDescription: In this article, we take a look at how to make pretty pictures using differentiable self-organizing systems, using Weights & Biases to keep track of our results.  \n\nBody:  \n\nBelow are two examples of neural cellular automata. Click and drag on each to 'erase' part of the image, and watch as the patterns try to re-form. Each cell (pixel) sees only its immediate neighbors, yet together they can organize to produce these dynamic patterns and textures.  \n\nCome along for the ride as we discuss what neural cellular automata are, how to train them, and some creative ways in which we can put these to use. Here's what we'll cover:  \n\n## Table of Contents  \n\n# Background to Neural Cellular Automata  \n\n# Training NCAs  \n\n# Making Fractals  \n\n# Adding Video  \n\n# Learn More",
    "b4d9e615-215f-49d1-a88e-c1fdbba78ede": "  \n\n# Creating a Semantic Search Engine for My Photos  \n\nDescription: In this article, we explore the results of using a CLIP model to find photos in a personal image library using open-text search queries and image similarity.  \n\nBody:  \n\nI like photography, and I try to be organized with my photo collection.  \n\nI save files to folders arranged by date, and I keep a few collections in both Lightroom and Capture One. Nevertheless, I usually have a hard time trying to locate photos I know exist in my library \u2013 I don't know where!  \n\nWhat if I could search using a free-form text query without going through the effort to caption all my photos beforehand? I've had this in the back of my mind for some time, and last Saturday, it was finally the time when I attempted an initial proof-of-concept prototype.  \n\n# Approach  \n\n# Results for free-form text queries  \n\n# Visual Search  \n\n# Visual and Text search, Vector Arithmetic  \n\n# Final thoughts",
    "894814df-7cc4-473b-afaf-db6d853bbe55": "## OncoHost Raises $35 Million To Utilize ML In Cancer Treatment\n[OncoHost](https://oncohost.com/) is a company developing technology called PROphet that is able to quickly and accurately determine the best treatment for patients, personalized to their situation. The technology is powered by machine learning and is able to predict the most effective treatment for it's patients by leveraging it's model trained on existing data and outcomes. Through a process of blood sampling and analysis to determine a patient profile, then running the profile through the machine learning model, a prediction for the optimal treatment is produced.  \n\n## Who's funding OncoHost and where's the money going?  \n\nOncoHost raised $35 million thanks to a Series C funding round led by ALIVE Isreal HealthTech Fund with additional participation from Leumi Partners, Menora Mivtachim, and OurCrowd.  \n\nThe money will be used to continue funding and expanding the ongoing trials using PROphet, as well as facilitating the launch of their products in the US.  \n\n## Find out more",
    "7f02ff54-4bb1-470c-9567-40e2c9178b83": "## Timestamps\n1:04 [Quantum computing and ML applications](https://wandb.ai/wandb_fc/gradient-dissent-drafts/reports/Johannes-Otterbach--VmlldzoxOTMzODg5#quantum-computing-and-ml-applications)  \n\n9:21 [Merantix, Ventures, and ML consulting](https://wandb.ai/wandb_fc/gradient-dissent-drafts/reports/Johannes-Otterbach--VmlldzoxOTMzODg5#merantix,-ventures,-and-ml-consulting)  \n\n19:09 [Building a cloud-agnostic tech stack](https://wandb.ai/wandb_fc/gradient-dissent-drafts/reports/Johannes-Otterbach--VmlldzoxOTMzODg5#building-a-cloud-agnostic-tech-stack)  \n\n24:40 [The open source tooling ecosystem ](https://wandb.ai/wandb_fc/gradient-dissent-drafts/reports/Johannes-Otterbach--VmlldzoxOTMzODg5#the-open-source-tooling-ecosystem-)  \n\n30:28 [Handing off models to customers](https://wandb.ai/wandb_fc/gradient-dissent-drafts/reports/Johannes-Otterbach--VmlldzoxOTMzODg5#handing-off-models-to-customers)",
    "402d3d15-e5cb-4a94-aaa7-ffee12575ac6": "## Unlearn.AI Raises $50 Million To Use AI To Accelerate Clinical Trials\n### Who's funding Unlearn.AI?\nUnlearn.AI has raised a total of $50 million in a Series B funding effort headed by Insight Partners with additional participation from Radical Ventures as well several already existing investors into the company.  \n\nDylan Morris, Managing Director at Insight Partners, has also joined the Board of Directors at Unlearn through this investment process.  \n\nUnlearn.AI has earned a total of nearly $70 million through investment rounds to date.  \n\nThe raised money is to be used in hiring efforts to double their employee count and broaden the scope of their Digital Twin project to cover more diseases.",
    "9525d3de-89a6-4218-9f4b-537b6f81af6a": "## Train, Optimize, Analyze, Visualize and Deploy Models for Automatic Speech Recognition with NVIDIA's NeMo\n### Introduction: What is Automated Speech Recognition (ASR)?\nThus, we can see the appeal of end-to-end ASR architectures: discriminative models that simply take an audio input and give a textual output, and in which all components of the architecture are trained together towards the same goal. The model's encoder would be akin to an acoustic model for extracting speech features, which can then be directly piped to a decoder which outputs text. If desired, we could integrate a language model that would improve our predictions, as well.  \n\nAnd this way, the entire end-to-end ASR model can be trained at once--a much easier pipeline to handle!  \n\nFor our task today, we'll be using Nvidia's NeMo toolkit to train an end-to-end ASR architecture and use Weights and Biases for logging performance metrics.  \n\nLet's get started!",
    "f188f3cc-6fb2-4a78-b9d5-a8dbfa818464": "## Timestamps\n15:53 Democratizing scientific computing  \n\n20:59 How Jensen stays current with technology trends  \n\n25:10 The global chip shortage  \n\n27:00 Leadership lessons that Jensen has learned  \n\n32:32 Keeping a steady vision for NVIDIA  \n\n35:48 Omniverse and the next era of AI  \n\n42:00 ML topics that Jensen's excited about  \n\n45:05 Why MLOps is vital  \n\n48:38 Outro",
    "d74362a4-9f92-4de4-a4cc-e10e8b935858": "## WebFormer: The Next Logical Step In Structure Information Extraction\n### How Does WebFormer Work?\nIf you're not interested in that you can simply jump to [the results](https://wandb.ai/onlineinference/paper-reading/reports/WebFormer-The-Next-Logical-Step-In-Structure-Information-Extraction--VmlldzoxNTM3MDQ1#the-results).  \n\nFor the rest?  \n\n### General Outline Of How WebFormer Works  \n\nAt a high level, rather than just using one type of token, WebFormer uses three. This allows the model to use the encoded field, the HTML, and the content as opposed to just the content.  \n\nThe authors of the paper summarize the main contributions as follows:  \n\n### The WebFormer Input Layer  \n\nRather than just encoding the text sequence, this model also encodes the HTML layout.  \n\nThe full model requires three token types:  \n\n### The WebFormer Encoder  \n\nThe WebFormer encoder is \"a stack of \ud835\udc3f identical contextual layers, which efficiently connects the field, HTML and text tokens with rich attention patterns followed by a feed-forward network.\"  \n\n### The Attention Layers  \n\n### Field Token Attention",
    "54efbfd8-879b-4c29-b086-41da6e025159": "## Syncing OpenAI API Fine-Tunes with Weights & Biases\nOn the right, you can see all of the training metrics pulled automatically into Weights & Biases for each fine-tune. This means that no matter how many fine-tunes you do, you can always easily compare which of those perform the best.  \n\nHere's what the training charts look like:  \n\nAdditionally, I've shown above how I've pinned some of the hyperparameters to be displayed in the project dashboard to give us more understanding behind the naming of the fine-tunes.  \n\nWe can also find all of the hyperparameters and metadata of the model and the fine-tune inside the run page itself:  \n\nThis is really important for future reproducibility: we can see which models work the best and what hyperparameters they use to do so, as well as what data they were trained on.  \n\nNot bad for one line of code, huh?  \n\n## Last Real or Fake Synopses Guess:  \n\n### Episode title: \"The Husbands Of River Song\"  \n\n### <--- Open the toggle to see the answer",
    "9fc7a831-6947-442a-93f7-3d5e15c239bb": "## Transcript\n### Why data pipelines are underrated\nFolks have figured out, wow, these problems are hard and they're hairy and they're nasty. I would encourage folks when they're looking at data pipeline to survey the space. There's new offerings every day out there.  \n\nI think you'd be doing yourself a disservice if you didn't at least evaluate them before you embark on your data pipeline journey. I don't know that there's a solution out there that's going to do everything that you want, but there could be something that gets you pretty far, and then you can add your own plugins or something to that effect.  \n\nBut I would say if you're sort of like clean slate, how do I get started? Well, you should definitely look at some of the software packages that are out there, because this is not really a problem that you necessarily want to solve, unless you absolutely have to.  \n\nLukas:  \n\n> Well, as someone that makes one of these software packages, I strongly agree with that.  \n\nChris:  \n\n> Yeah. I can imagine.",
    "b7c88f70-90e0-4c4b-b106-b8ef4949c2a6": "## Transcript\n### Outro\nLukas:  \n\n> Well, congratulations on making such an amazing product that helps farmers and helps the world. It's great to talk to you about it.  \n\nChris:  \n\n> Yeah. Thanks a lot. We definitely are excited by this product.  \n\nWhat I'm personally really excited about is, as we scale this thing out to tens of machines and hundreds of machines and thousands of machines, those savings are going to go up proportionally. I've been talking to John Deere about this. I want to get a \"Gallons of herbicide saved\" on the John Deere website. And it's going to just like keep increasing, going like to this really big number. It's going to be like the national debt, but it'll be like a good number.  \n\nLukas:  \n\n> Awesome. Well, let me know when that happens. I'll take a look and feel a little bit of pressure too.  \n\nChris:  \n\n> Yeah. I'll give you the link.  \n\nLukas:  \n\n> Awesome. Excellent. Thanks for your time.  \n\nChris:  \n\n> Okay. Thanks, Lukas. Take care.  \n\nLukas:",
    "b3009cc2-5456-41b7-935a-4cf204ec5abf": "  \n\n# Let's Talk ConvMixer Architecture. What it is? Why you should use it? And how you can use it?  \n\nDescription: In this blog you will learn what the novel ConvMixer model is and how to implement it in TensorFlow (with a little vision transformers for good measure).  \n\nBody:  \n\n# Introduction  \n\nFor the longest time, Convolution based models (CNN's) have dominated the Computer Vision landscape. In classification, segmentation, object detection and other sub-domains models like VGG-net, Resnet, Unet's, YOLO and similar model architectures have the dominion over basically anything else. Even in Kaggle competitions, all you would see is some variation or ensemble of these models.  \n\nBut things have changed recently with the introduction of Vision Transformers (ViTs).  \n\n# What are Vision Transformers?  \n\n# ConvMixer  \n\n# Conclusion  \n\n# Main Points  \n\n# References  \n\n[1] [https://keras.io/examples/vision/convmixer/](https://keras.io/examples/vision/convmixer/)",
    "ec7e787f-2f06-4e83-9200-9fdaf8a7104c": "## Transcript\n### How the ML space has changed\nAfter that, Shawn and I, we pulled an all-nighter just cranking out the interface that they wanted and got it to them within a couple days. I remember thinking, \"How precious is this relationship with OpenAI, this institution that I really, really admire?\"  \n\nAlso, that same feeling that Shawn described of some users saying, \"Hey, I have this problem,\" and we had the power to go back and actually fix that problem for them.  \n\nLukas:  \n\n> Do you remember the afternoon when they turned on Weights & Biases?  \n\nShawn:  \n\n> That was another all-nighter.  \n\nChris:  \n\n> Yeah, it turns out there was some performance problems with my Python backend if I'm recalling correctly.  \n\nShawn:  \n\n> Well, there were a couple things. We did not anticipate OpenAI's scale, because we're doing the thing that you do as a startup, which is you make an MVP. It doesn't really need to scale. But it turns out our very first customer was one of the largest-scale customers we could have.",
    "094f2161-38ec-4280-b244-9ba5cd4582d2": "## How Weights and Biases Can Help with Audits & Regulatory Guidelines\n### Before W&B integration\nBefore integrating with Weights & Biases, things were really pretty messy. A large excel sheet had to be maintained for logging in every model change. This excel sheet looked something like:  \n\nAnd since the company had three different models (one for X-rays, another for CT-scans and final one for MRIs) that solved different problems for more than 50 different clients, you can imagine how messy and massive these excel sheets became!  \n\nPast that, [Australian medical regulatory guidelines ](https://www.tga.gov.au/regulation-software-based-medical-devices)are pretty strict. That meant the medical company I worked for was part of an audit almost every year. This meant that every process was cross-verified. This included data collection, data cleaning, model training, model evaluation, data leakage etc.  \n\nOne question I remember being asked by the auditors that I didn't have an answer for at the time was:",
    "51c3f20c-1cf5-474d-b6c1-9c4452a2e105": "  \n\n# Image-to-Image Translation Using CycleGAN and Pix2Pix  \n\nDescription: A practical introduction to image-to-image translation, complete with code and examples.  \n\nBody:  \n\n# Image-to-Image Translation  \n\nImage-to-image translation is a vision task where the goal is to learn the mapping between an input image and an output image. It covers specific tasks like enhancement and style transfer, among others.  \n\nIn this report, we'll look at two methods used for image-to-image translation tasks, CycleGAN, and Pix2Pix.  \n\n# Official project repository - [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)  \n\n# Follow along using Colabs!  \n\nLet's start with CycleGAN and move on the Pix2Pix.  \n\n# CycleGAN  \n\n## Architecture Overview  \n\nFigure is from [CycleGAN](https://arxiv.org/pdf/1703.10593.pdf) [Zhu*, Park*, et al., ICCV 2017] [](https://arxiv.org/pdf/1703.10593.pdf)  \n\n## Results  \n\n### Inference  \n\n### Training  \n\nDownload the dataset of your choice.  \n\n# Pix2Pix",
    "8563a28e-42cc-46db-ba1b-dacebb0ace5d": "  \n\n# Pivotal Tuning for Latent-Based Editing of Real Images  \n\nDescription: An interesting method has come up that edits facial attributes, without allowing the loss of tattoos and pimples. This report will be a journey through this paper.  \n\nBody:  \n\n# Contents  \n\n*\n*\n*\n*\n*  \n\n# 1. Introduction  \n\nWelcome to my report on \"Pivotal Tuning for Latent-based editing of Real Images (PTI),\" which is part one of my series on StyleGAN Inversion! Below, you'll find a quick intro, a description of the methods, and a lot of tangible, interactive examples of the technique. Without further ado, let's get going!  \n\n[The paper](https://arxiv.org/abs/2106.05744) we'll be talking about came out on arxiv in June this year and it's generated a lot of interest in certain Twitter circles. For example, here's is one by popular AI practitioner [Ahsen Khaliq](https://www.linkedin.com/in/ahsenkhaliq/).  \n\n# 2. The PTI Method  \n\n# 3. Experiments  \n\n# 4. Additional Observations  \n\n# 5. Conclusion",
    "763efd14-f264-416c-a87e-49fdd29aa458": "## Watch on YouTube\n### Transcript\n#### How language models can be harmful\nSentiment analysis is the task of taking some natural language text, and her example is English, and using it to calculate or predict the sentiment. Is this a text expressing positive feeling towards something, negative feeling towards something, or not expressing feelings?  \n\nThe particular data set she was working with, I think, was Yelp restaurant reviews. So there, it's \"Take the text, predict the stars.\"  \n\nLukas:  \n\n> Yeah, I've used that data set. Yeah, for sure.  \n\nEmily:  \n\n> And then as an external component, she's using word embeddings, which are representations of words into a vector space based on what other words they could occur with. So, some of the training data is in-domain, the Yelp reviews, but then there's this component that's trained on general web garbage.  \n\nWhat she found using the sort of generic word embeddings was that the system systematically un-predicted the star ratings for Mexican restaurants.",
    "1085cbf6-3a9c-4802-9be4-92f0694072fb": "## Watch on YouTube\n### Transcript\n#### The important difference between linguistic form and meaning\nOf course, when I was a kid, I learned about the Turing test, which seems like a pretty good test of understanding on its face. I think the test is like, if you have a conversation with something and you can't tell if it's an automated system or a human, then we can say that it has intelligence, tand it sort of seems to me like these language models are on the verge of passing the Turing test.  \n\nWhat would it take for you to feel like some automated technique actually has understanding of what it's consuming?  \n\nEmily:  \n\n> Yeah. So, I think the first thing I want to say about the Turing test is the reason it doesn't work...and I hate to disagree with a giant like Turing, because Turing's work was really important and foundational\u2014  \n\nLukas:  \n\n> But it was 100 years ago, it's possible to miss something.  \n\nEmily:  \n\n> 70?  \n\nLukas:  \n\n> 70, fair, fair. All right, 80? 70? Okay, 70. Sorry.  \n\nEmily:",
    "8e372e48-6c39-41e3-959a-9dc6078e78c6": "## Show Notes\n### Transcript\n#### Cloudera's success, in retrospect\n\nLukas:  \n\n> I think one thing that was interesting at the time \u2014 it seems so wrong in retrospect that it's hard to believe people thought this \u2014 but I remember actually talking to Matt Cohler about Cloudera and he was thinking, \"How many companies would really use this? Maybe it's tens or maybe a hundred,\" or something like that at the time.  \n\nI think even you expressed a little bit of doubt to me when you were starting. Did you feel worried about the market size or how did you think about that? Were you just sure that it would work or was that ...  \n\nJeff:  \n\n> Nah. For me, it was about manifesting a product vision, not about building a huge company.  \n\nLukas:  \n\n> Interesting.  \n\nJeff:  \n\n> I didn't expect it to get as big as it did, or people to care as much as they did.",
    "1b7be40b-5478-40e0-b816-3011774a3ba4": "## Show Notes\n### Jeff's transition into biomedicine\n#### Why Jeff created Related Sciences\n\nLukas:  \n\n> What types of things are you working on now in your lab\u2074?  \n\nJeff:  \n\n> Well, nothing actually. I'm on leave from my lab so-  \n\nLukas:  \n\n> Well, what are you working on? What are you up to?  \n\nJeff:  \n\n> Yeah. I went on leave from my lab in January of 2020 because I started a biotech venture creation firm with two of my friends,    and   , in mid 2019\u2075.  \n\nOne of the things that I did with my lab...so I started my lab up in New York City and it was purely computational. But one thing that you learn quickly if you're running an academic lab is that it's difficult to collaborate in academia, and it's a lot easier if you own vertical research ideas rather than being a person who brings a skill into a horizontal research network. Those are just a lot harder to build, those horizontal research groups, and they're often built through pedigree like, \"Oh, I did my PhD with this professor and so I'm going to work with you.\"",
    "b8e7d896-9d67-4ad3-851a-301db714514c": "## Show Notes\n### Transcript\n#### The unexpected challenges of prototype to production\nIt seems, conceptually, really simple but when you actually get down into it, you're like, \"Wow, we've been at this for two months and we're still not quite there yet. What's happening?\" And that's sort of been our experience, I think.  \n\nLukas:  \n\n> Interesting. And I guess at the time, there was probably a lot less stuff to help you.  \n\nSpence:  \n\n> Yes, there was no Kubernetes, there was none of that type of infrastructure.  \n\nLukas:  \n\n> Awesome. Well, thanks so much. This was really fun. And thanks for sharing so much about how your company operates.  \n\nSpence:  \n\n> Yeah, it's always good to chat with you.  \n\nLukas:  \n\n> If you're enjoying Gradient Dissent, I'd really love for you to check out Fully Connected, which is an inclusive machine learning community that we're building to let everyone know about all the stuff going on in ML and all the new research coming out.",
    "25c2e9c1-5731-45ec-a724-a3acda545b91": "## Rachael's Bio\n### Transcript\n> But I guess downstream then the error is higher?  \n\n> Definitely, the regional and racial differences are due to things that you could fix with machine learning, whereas I think the other differences are not due to that.  \n\n> How were you able to pull that apart?  \n\n> I believe I had fairly balanced classes specifically on the modeling side I used mixed-effects models so you can control for some features as well, identifying the effects of others.  \n\n> What do you think is an underrated aspect of machine learning that you think people should pay more attention to?  \n\n> Data visualization. I've seen a lot of really excellent machine learning engineers who have a hard time communicating their results and models because their charts are just unreadable.  \n\n> Does anything come to mind where you saw a really good visualization and something you want to call out as an excellent example?",
    "ac15a315-c2ac-4227-aac6-751421c4ab2d": "## Show Notes\n### Topics Covered\n\n0:00 Introduction  \n\n0:52 Dad things  \n\n2:40 The story of Fast.ai  \n\n4:57 How the courses have evolved over time  \n\n9:24 Jeremy\u2019s top-down approach to teaching  \n\n13:02 From Fast.ai the course to Fast.ai the library  \n\n15:08 Designing V2 of the library from the ground up  \n\n21:44 The ingenious type dispatch system that powers Fast.ai  \n\n25:52 Were you able to realize the vision behind v2 of the library  \n\n28:05 Is it important to you that Fast.ai is used by everyone in the world, beyond the context of learning  \n\n29:37 Real-world applications of Fast.ai, including animal husbandry  \n\n35:08 Staying ahead of the new developments in the field  \n\n38:50 A bias towards learning by doing  \n\n40:02 What\u2019s next for Fast.ai  \n\n40.35 Python is not the future of Machine Learning  \n\n43:58 One underrated aspect of machine learning  \n\n45:25 Biggest challenge of machine learning in the real world",
    "bd1ea50e-98b3-4c9f-b9ef-6fea922b4acc": "## Show Notes\n### Transcript\nPeter:  \n\n> Well, it depends on what you're debugging though because you can make it compact. Then, when you debug it, it's like, \"Are you debugging an actual bug in the runtime of NumPy itself?\" Are you debugging a performance mismatch with your expectation relative to how the data structure is laid out in memory? Are you debugging a impedance mismatch between your understanding of what NumPy is going to do in each of these steps versus what it's...  \n\nThere's a lot of things to debug, so to speak, but that's one of the downsides of making really tight NumPy snippets because I did some of that back in the day. I was like, \"Oh, this is so great.\" Then, something blows up, but it's like, \"Oh, crap.\"  \n\nLukas:  \n\n> But wait. I'm like taking off in all these tangents. I'm actually really fascinated.  \n\nPeter:  \n\n> This is a conversation.  \n\nLukas:  \n\n> Totally. You were saying, so you're comparing to K which actually Jeremy Howard did talk about and really, really praised.  \n\nPeter:  \n\n> Great.  \n\nLukas:",
    "8085ed94-9748-4005-9e8b-8521b34871dd": "  \n\n# Funnel Activation for Visual Recognition (CVPR 2020)  \n\nDescription: Reproducibility Challenge 2020 report for the CVPR 2020 paper titled 'Funnel Activation for Visual Recognition' by Ningning Ma, Xiangyu Zhang, and Jian Sun.  \n\nBody:  \n\n[Paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560341.pdf) | [GitHub Repository](http://github.com/megvii-model/FunnelAct)  \n\n# Summary  \n\n## Scope of Reproducibility  \n\nExpanding the family of rectified activations, the proposed function (Funnel Activation) demonstrates a novel way of constructing activations by preserving higher activated features by involving a bilevel input to a simple max function which improves expressivity at a small computational overhead cost.  \n\n## Methodology  \n\n## Results  \n\n## What was Easy  \n\n## What was Difficult  \n\n## Communication with Original Authors  \n\n# Introduction  \n\n# Scope of Reproducibility  \n\nIn our attempt of reproducing the results presented in the paper, we focus on the following target questions:",
    "3cf32d32-04ca-463c-a477-0d6cd9ba3c91": "## Appendix\n\n## A\tHyper-parameters  \n\n## B\tAttention visualization  \n\nWe present more examples of visualizing attention in various models.  \n\n### B.1\tLearned embedding with content, 9 heads  \n\n### B.2\tLearned embedding without content, 9 heads  \n\n### B.3\tHierarchical learned embedding with content, 9 heads  \n\n### B.4\tLearned embedding with content, 16 heads  \n\n### B.5\tLearned embedding without content, 16 heads  \n\n### B.6\tHierarchical learned embedding with content, 16 heads  \n\n### B.7\tHierarchical SAN pairwise  \n\n### B.8\tHierarchical SAN patchwise  \n\n### B.9\tVision transformer (ViT)  \n\n### B.10 Hierarchical Vision transformer (ViT)  \n\n## C\tWandB Training Logs  \n\nWe provide training logs for all our experiments here for the reader's reference.  \n\n'",
    "84158462-5ed1-4430-9b21-7b8be43747c3": "## Overview of the Proposed Method\nLet's first level-set on the notations that we'll be using and get a little clarity on the goal of this research. From the paper:  \n\n> Let    be an image of a person, referred to as the source image. Let    be a talking-head video, called the driving video, where   \u2019s are the individual frames, and    is the total number of frames.  \n\n> Our goal is to generate an output video   , where the identity in   \u2019s is inherited from    and the motions are derived from   \u2019s.  \n\nDepending on the  (i.e. the image of the person), the goal can be either one of the two broader deep learning tasks:  \n\n* If the person in the source image () is the same as in the driving video (), then it's a video reconstruction task. The generated output video () still takes the identity information from  and motion information from .\n* If the person in  is not the same as in , then it's a motion transfer task.",
    "a7d5f453-b1b5-4187-b10a-e1ad7e18d3cd": "## The Objective\nThe method proposed by Piotr Bojanowski et. al. in this paper is a direct extension of the Skip-Gram model. Before we dive into the subword space, let us revise a little on Skip-Grams.  \n\n> Given a word vocabulary of size   , where a word is identified by its index    the goal is to learn a vectorial representation for each word   . Given a large training corpus represented as a sequence of words    the objective of the skip-gram model is to maximize the following log-likelihood:  \n\n$$\nJ(\\theta)=\\sum ^{T}_{t=1}\\sum _{c\\in C_{t}}\\log p( w_{c} |w_{t})\n$$  \n\nWhere the context  is the set of indices of words surrounding the target word .  \n\nThe question arises about the parameterization of the log-likelihood function, to be specific \"What do we tweak to maximize the log-likelihood?\". The answer to this lies in the probability function.  \n\n$$\np( w_{c} |w_{t}) =\\frac{\\exp( s( w_{t} ,w_{c}))}{\\sum ^{W}_{j=1}\\exp( s( w_{t} ,j))}\n$$",
    "802b3371-fe3f-4f71-8528-d97798f8c306": "  \n\n# Reformer: The Efficient Transformer  \n\nDescription: Our fast.ai community submission to the Reproducibility challenge 2020: \"Reformer: The Efficient Transformer\" by Nikita Kitaev, \u0141ukasz Kaiser and Anselm Levskaya, accepted to ICLR 2020.  \n\nBody:  \n\nThis report was originally published on OpenReview as part of the Papers With Code 2020 Reproducibility Challenge, you can find that paper [here](https://openreview.net/forum?id=3s8Y7dHYkN-) and our docs with all code to reproduce our results [here](http://arampacha.github.io/reformer_fastai/)  \n\n## Reproducibility Summary  \n\n## Scope of Reproducibility  \n\nThe scope of this work is to verify the claims of memory efficiency and speed on longer sequences of the Reformer. We replicated only the NLP experiments due to limited computational resources.  \n\n## Methodology  \n\n## Results  \n\n## What was easy  \n\n## What was difficult  \n\n## Communication with original authors  \n\n# Report  \n\n# 1.Introduction  \n\n# 2. Scope of Reproducibility  \n\n# 3. Methodology",
    "51729964-9eff-4499-937b-f78fd3457411": "## Further Scope\n\nWhile I processed the data set I also created a [kernel dataset](https://www.kaggle.com/aritrag/license) that is way easier to use. Beginners in computer vision do not need to think about the input pipeline so much, they can dive straight into the algorithms that they want to use and come up with something of their own.\nI had some great prospects with the dataset:  \n\n* Think about a better objective function for the problem statement.\n* Use a better architecture for the feature extractor.\n* Use something other than the linear regressor for the bounding boxes.\n* Using an Optical Character Recognition model on top of the license plate.\n* The whole model can be ported to a mobile device and used by the traffic police.",
    "c9fba3e5-12b7-4979-905d-5f3428b8bb3c": "\"  \n\n# Part 2: Deep Representations, a Way Towards Neural Style Transfer  \n\nDescription: This article is the second part of two, and it provides a top-down approach to conceiving neural style transfer using Weights & Biases.  \n\nBody:  \n\nThis is the second part of the two-part series on Neural Style Transfer. If you have arrived here from the first part, you already know how content representations are learned, how to visualize the deep embeddings of a [convolutional neural network](https://wandb.ai/site/tutorial/convolutional-neural-networks), and are also familiar with amalgamation.  \n\nIf you are unfamiliar with these concepts, [check out part one of the series](https://wandb.ai/authors/nerual_style_transfer/reports/Part-1-Deep-representations-a-way-to-conceive-Neural-Style-Transfer--VmlldzoyMjQzNDY).  \n\n[Checkout the code on GitHub](https://github.com/ariG23498/NeuralStyleTransfer)  \n\n## Table of Contents  \n\n# Style Representation  \n\n# Neural Style Transfer  \n\n# The Game of Losses  \n\n# Unnormalized VGG16",
    "f8ee73ec-11d0-492a-ab59-981a81aae589": "  \n\n# LSTM RNN in Keras: Examples of One-to-Many, Many-to-One & Many-to-Many  \n\nDescription: In this report, I explain long short-term memory (LSTM) recurrent neural networks (RNN) and how to build them with Keras. Covering One-to-Many, Many-to-One & Many-to-Many.  \n\nBody:  \n\nThere are principally the four modes to run a recurrent neural network (RNN).  \n\nOne-to-One is straight-forward enough, but let's look at the others:  \n\n*\n*\n*  \n\n## Let us see how a minimalistic code snippet for each of them looks in Keras  \n\nLSTMs can be used for a multitude of deep learning tasks using different modes. We will go through each of these modes along with its use case and code snippet in Keras.  \n\n## [Try the Experiments in Google Colab ](https://colab.research.google.com/gist/ayulockin/c53e9b3c1e804a05c05360807d88220a/how-to-use-lstm-for-one-to-many-many-to-one-and-many-to-many-sequences.ipynb)  \n\n## What Are One-to-Many Sequence Problems?  \n\n## What Are Many-to-One Sequence Problems?  \n\n## Try Weights & Biases",
    "980a4dd9-ee7e-43e0-8556-de5204cb915c": "## Dissecting the Weight Space of a (Deep) Ensemble\n### Disagreement Between Predictions (Different Inits)\n#### Observations\n* The predictions for the same model with different initializations trained on the same dataset with same hyperparameters disagree. \ud83d\ude32\n* Obviously, there is a subset of examples that the model trained with different trajectories will agree upon.\n* There must be a subset of intrinsically hard examples that the model trained with different trajectories will misclassify similarly. We shall investigate in the next section.  \n\n[Reproduce analysis \u2192](https://github.com/ayulockin/LossLandscape)",
    "f5e9c07d-80d7-4243-bc38-8dd52820547f": "## Putting the Pieces Together\nInside , we find the inner product of  with every other vector in the batch but with some restrictions. These restrictions are applied with the help of some terms. Let us look at them:  \n\n* The term  restricts finding out the inner product of any vector with the same vector. i.e., it never lets the loss function calculate the dot product between  and  because it will not take us anywhere.\n* The term  ensures that  and  are the vectors belonging to the same class. These are present in the numerator of the log term.\n* The term   ensures that and  are different vectors. i.e.,  does not belong to the class of  and . The vector  is present at the denominator of the loss term.  \n\nNow coming to the log term of , the numerator and denominator have  term. This exponential term ensures that the log argument goes no higher than 1.",
    "2f21e0ed-5949-4386-8fc8-2cf81c2d1b4e": "  \n\n# Drought Watch Benchmark Progress  \n\nDescription: This article walks through the process of developing the baseline and exploring submissions to the Drought Watch benchmark  \n\nBody:  \n\nDrought Watch is a community benchmark for machine learning models that detect drought from satellites. With better models, index insurance companies can monitor drought conditions\u2014and send resources to families in the area\u2014more effectively. The goal is to learn from ~100K expert labels of forage quality in Northern Kenya (concretely, how many cows from 0 to 3+ can the given location feed?) to more accurately predict drought from unlabeled satellite images.  \n\nYou can read more about the dataset and methods [in this paper](https://arxiv.org/abs/2004.04081). Since this is an open collaborative benchmark, we encourage you to share and discuss your code, training workflows, analysis, and questions\u2014together we can build a better model faster.  \n\n# Community Submissions So Far  \n\n## Community Improved Baseline By 2%",
    "9f7011a9-9268-4f13-9ac0-292fe6c53bee": "## Regression\n\n## Outlier Candidates Plot  \n\nMeasures a datapoint's influence on regression model via cook's distance. Instances with heavily skewed influences could potentially be outliers. Useful for outlier detection.  \n\nExample  \n\nwandb.sklearn.plot_outlier_candidates(model, X, y)  \n\n* model (regressor): Takes in a fitted classifier.\n* X (arr): Training set features.\n* y (arr): Training set labels.  \n\n## Residuals Plot  \n\nMeasures and plots the predicted target values (y-axis) vs the difference between actual and predicted target values (x-axis), as well as the distribution of the residual error.  \n\nGenerally, the residuals of a well-fit model should be randomly distributed because good models will account for most phenomena in a data set, except for random error.  \n\nHere we can see most of the error made by our model is between +/-5, and is evenly distributed for both training and test datasets.  \n\nExample  \n\nwandb.sklearn.plot_residuals(model, X, y)",
    "a6dca679-5f30-4ccb-ac84-209b54be3a60": "  \n\n# Meaning and Noise in Hyperparameter Search with Weights & Biases  \n\nDescription: How do we distinguish signal from pareidolia (imaginary patterns)? This article is showcases what is possible with W&B and aims to inspire further exploration.  \n\nBody:  \n\n# TL;DR: To understand the significance of your results, quantify the random variance first  \n\n### Motivation  \n\nWhen running a hyperparameter search on a model, I often wonder if the changes I see in my key metrics, e.g. validation accuracy, are significant. I might plot the accuracy over epochs for 10 different values of, say, weight decay and see that the final accuracy only varies by about 0.5%.  \n\nIs that a meaningful correlation or not? Should I explore a bigger range of values or abandon tuning this hyperparameter? Are there some other settings masking any possible impact of weight decay?  \n\n### The Signal vs. Noise Scenario  \n\n# Model & Methods  \n\n### Model: Bidirectional RNN on MNIST in Pytorch  \n\n### Experiment setup  \n\n# Findings so far",
    "f06b84a5-715c-4af4-90bc-2f55251e2590": "## Batch size matters more than GPU count\n### Use smaller batches when acc matters\n#### Notes/next steps\n* batch size 32 is smaller but performs worse than 64: perhaps sub-batches of only 8 items are inefficient when split across 4 GPUs.\n* test effects of 1) more data, 2) bigger model (simply larger layers/deeper net, optionally Inception-ResNet V2/resnet)\n* sudden jump in training loss for batch size 256 and 64, around 125 minutes in  \u2014 a side effect of how run averaging works? need to run more trials to average over shifting training dynamics or different clusters?\n* this is hitting CPU limits",
    "6ef52fc5-c05b-4560-85a2-e3f454d431d3": "## Your Mission\n1. Write a function that calculates maintenance and repair costs when you use any supplied decision function for determining when to do machine maintenance\n2. Create 3 decision functions and compare their costs using the function you just wrote in step 1\n3. Write and share a Weights & Biases report so your colleagues can see the implications of using your preferred decision function. In your report, describe (in just a few sentences):\n4. How you might deal with potential covariate shift if training data was collected in winter when air temperatures are lower, so you expect future air temperatures to be warmer\n5. How you might test your decision function in reality to make sure it works before applying it at scale",
    "16694565-8035-41a9-8132-6f99f03be553": "## Autoencoder Networks for MNIST\n### Training\n\nTo run training, execute the cell below.\nYou can configure the network and training procedure\nby changing the values of the `config` dictionary.  \n\nUse the value of `erase` to switch tasks:\nwhen `erase` is `True`,\na random portion of the input (but not the output!)\nis erased before being fed to the network,\nwhich makes the task a form of\nimage in-painting.\nWhen it is `False`,\nthe input is unaltered,\nand the task is a vanilla reconstruction task.  \n\nIn between training runs,\nespecially runs that crashed,\nyou may wish to restart the notebook\nand re-run the preceding cells\nto get rid of accumulated state\n(`Runtime > Restart runtime`).",
    "0b4bfa56-9bf5-4248-8a29-fca218b529d3": "## Surprises and Machine Learning Loss Functions\n### Machine Learning Loss Functions\nThese concepts come together in the definition of a typical loss function in machine learning.  \n\nAbove we noted that ML models often output\nprobability distributions.\nIn machine learning, we align our model\nwith our goals by defining a loss function\nthat can compare the model's actual outputs\nwith the desired outputs.  \n\nFor probability distributions,\nthe usual choice is the cross entropy\n(though KL divergences and entropies also come up in other ML loss functions!).  \n\nThe desired output is the distribution $p$,\nand we compute the cross entropy of $p$\non the model's output $q$ --\n`crossentropy(p, q)`.  \n\nThere's one catch -- in many implementations\n(e.g. in PyTorch and\nKeras),\nthe model outputs are not a pmf directly.\nInstead, they must be passed through the `softmax` function first.\nBefore being passed through that function,\nthe values are known as\n*logits*.",
    "83e5bbfa-e908-4230-9b64-1067217b77e5": "### Continual-Train\n|  | feature | importance |\n| --- | --- | --- |\n| 6 | cmc\\_0\\_0\\_0\\_2\\_interpolated | 22.321365 |\n| 89 | gfs\\_temperature\\_sea\\_interpolated | 15.095626 |\n| 99 | wrf\\_t2 | 11.442965 |\n| 109 | wrf\\_t2\\_interpolated | 11.176995 |\n| 8 | cmc\\_0\\_0\\_0\\_2 | 8.895680 |\n| 100 | wrf\\_t2\\_next | 8.157100 |\n| 87 | gfs\\_temperature\\_sea | 5.164831 |\n| 7 | cmc\\_0\\_0\\_0\\_2\\_next | 3.692906 |\n| 90 | gfs\\_temperature\\_sea\\_next | 1.909396 |\n| 1 | sun\\_elevation | 1.583235 |\n| 0 | topography\\_bathymetry | 1.381163 |\n| 4 | cmc\\_0\\_0\\_0\\_1000 | 1.201706 |\n| 2 | climate\\_temperature | 1.098449 |\n| 20 | cmc\\_0\\_1\\_0\\_0 | 0.530070 |\n| 12 | cmc\\_0\\_0\\_0\\_925 | 0.386553 |\n| 61 | gfs\\_pressure | 0.338134 |\n| 82 | gfs\\_temperature\\_85000 | 0.313681 |\n| 102 | wrf\\_rh2 | 0.297663 |\n| 84 | gfs\\_temperature\\_92500 | 0.291478 |\n| 75 | gfs\\_temperature\\_55000 | 0.271491 |\n| 10 | cmc\\_0\\_0\\_0\\_700 | 0.213619 |\n| 63 | gfs\\_soil\\_temperature | 0.196249 |",
    "43fe8216-ffa4-4f20-87ea-9021149755ec": "| 2 | 2018-09-01 00:00:00 | 35.533333 | 35.766667 | 25.0 | 0.0 | mild temperate | 2.0 | -35.902113 | 26.162143 | 751.406267 | ... | 0.0000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.182770 | 22.032770 | 0.0 | 0.0 | 2018\\_35 |\n| 3 | 2018-09-01 00:00:00 | 47.432201 | 0.727606 | 15.0 | 0.0 | mild temperate | 103.0 | -35.725373 | 16.347143 | 756.800746 | ... | 0.0000 | 0.0 | 0.000000 | 0.0 | 0.0 | 0.000000 | 8.749994 | -1.0 | 0.0 | 2018\\_35 |\n| 4 | 2018-09-01 00:00:00 | 43.427101 | -3.820010 | 18.0 | 0.0 | mild temperate | 5.0 | -39.615037 | 18.630714 | 758.808740 | ... | 0.0000 | 0.0 | 0.000000 | 0.0 | 0.0 | -0.799988 | 11.650018 | 0.0 | 0.0 | 2018\\_35 |\n| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n| 5684 | 2018-09-02 23:56:00 | 27.692600 | -97.291100 | 31.0 | 10.0 | dry | 6.0 | 22.706908 | 30.584286 | 760.531371 | ... | 0.0000 | 0.0 | 0.000000 | 0.0 | 0.0 | 1.221222 | 23.749994 | 0.0 | 0.0 | 2018\\_35 |",
    "da73a0d5-3a48-4beb-80b3-72192437dd8b": "## Teams\n### Team Roles and Permissions\n#### Team Settings\n\nTeam settings allow you to manage the settings for your team and its members. With these privileges, you can effectively oversee and organize your team within Weights & Biases.  \n\n| Permissions | View-Only | Team Member | Team Admin |\n| --- | --- | --- | --- |\n| Add team members |  |  | X |\n| Remove team members |  |  | X |\n| Manage team settings |  |  | X |",
    "a9c595b6-b759-4e6e-9e5a-ef0e3b127cda": "## Resources for Educators, Teaching Assistants, and Students\n\nBelow are some resources that you can make use of as a student, student instructor (TA, GSI), or educator.  \n\nWe've included introductory content to help get you and your students started using *Weights & Biases* to enable collaborative, repeatable machine and deep learning in your classroom, research lab, or student-run organization.  \n\n## Teaching with Weights & Biases  \n\nThe resources below are targeted at educators or instructors who are teaching machine learning, deep learning or reinforcement learning courses who want to make use of Weights & Biases in the classroom. We've curated a mix of videos, slide decks, and free resources for educators to help you and your students build reproducible, collaborative models with ease:  \n\n## Using Weights & Biases with Your Favorite Library  \n\n## How to Cite Us  \n\n## Contact us",
    "b4f4b88f-b706-4c7b-8484-ea2571bb1386": "## Enterprise Model Management Course\n### Evaluation\n#### Create Automation\nBefore creating the automation, try launching your job manually from the Jobs panel in your project. Once this is working, it should be easy to trigger this job with an automation.  \n\n1. Go to Model Registry and view your registered model\n2. Click to add New Automation\n3. Select Event Type (An alias is added to a model version), choose Alias regex (e.g. \"candidate\") and Action Type (Jobs)\n4. Select the Job you created. You can now overwrite some of the run configs. Try for example replacing the \"alias eval\" value with a dynamic variable \"$(alias)\" so that it refers to the alias being updated that triggers the automation.\n5. Select destination project and pick the Launch queue you created\n6. Choose a name for your automation and save it - you can now add it by adding the chosen alias (e.g. \"candidate\") to a model version in model registry",
    "3f0a4901-2f1a-4e39-9793-7ae38530685a": "## Perform the train-test split on the raw dataset\n### Step 3: Log the train-test split we'll be using\nYou'll see square nodes, representing runs,\nand circular nodes, representing generated artifacts.\nArrows connect runs to the artifacts they generated\nand artifacts to the runs that use them.  \n\nIn the screenshot below,\nsee if you can find the runs used to upload and split the dataset\nand the dataset artifacts that those runs generated.",
    "961e4453-e162-4efb-a7ee-fd940522fd61": "## Running a Hyperparameter Sweep \ud83e\uddf9\n\nW&B sweeps allow you to optimize your model hyperparameters with minimal effort. In general, the workflow of sweeps is:\n1. Construct a dictionary or YAML file that defines the hyperparameter space\n2. Call `wandb.sweep(<sweep-dict>)` from the python library or `wandb sweep <yaml-file>` from the command line to initialize the sweep in W&B\n3. Run `wandb.agent(<sweep-id>)` (python lib) or `wandb agent <sweep-id>` (cli) to start a sweep agent to continuously:\n- pull hyperparameter combinations from W&B\n- run training with the given hyperparameters\n- log training metrics back to W&B",
    "7149e693-5a9e-459e-9a16-5b2d07b8c087": "## \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f W&B + \ud83e\uddea Scikit-learn\n\nUse Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.  \n\n## What this notebook covers:  \n\n* Easy integration of Weights and Biases with Scikit.\n* W&B Scikit plots for model interpretation and diagnostics for regression, classification, and clustering.  \n\n**Note**: Sections starting with *Step* are all you need to integrate W&B to existing code.  \n\n## The interactive W&B Dashboard will look like this:",
    "c05a6b88-ade1-4ae1-bc73-c77cecd8298c": "## partitioned-table\n\n## Chainable Ops  \n\n### `asset-file`  \n\nReturns the *file* of the asset  \n\n| Argument |  |\n| --- | --- |\n| `asset` | The asset |  \n\n#### Return Value  \n\nThe *file* of the asset  \n\n### `partitionedtable-file`  \n\nReturns the *file* of a *partitioned-table*  \n\n| Argument |  |\n| --- | --- |\n| `partitionedTable` | The *partitioned-table* |  \n\n#### Return Value  \n\n*file* of the *partitioned-table*  \n\n### `partitionedtable-rows`  \n\nReturns the rows of a *partitioned-table*  \n\n| Argument |  |\n| --- | --- |\n| `partitionedTable` | The *partitioned-table* to get rows from |  \n\n#### Return Value  \n\nRows of the *partitioned-table*  \n\n## List Ops  \n\n### `asset-file`  \n\nReturns the *file* of the asset  \n\n| Argument |  |\n| --- | --- |\n| `asset` | The asset |  \n\n#### Return Value  \n\nThe *file* of the asset",
    "b6ba3c1e-ddc0-41d0-8b79-c9e4e2260276": "## number\n### Chainable Ops\n#### `number-lessEqual`\n\nCheck if a number is less than or equal to another  \n\n| Argument |  |\n| --- | --- |\n| `lhs` | number to compare |\n| `rhs` | number to compare to |  \n\n#### Return Value  \n\nWhether the first number is less than or equal to the second",
    "4ff3061a-ddef-485e-a43d-aac2aaa9f0e1": "## Run\n### Methods\n#### `log`\n| Arguments |  |\n| --- | --- |\n| `data` | (dict, optional) A dict of serializable python objects i.e `str`, `ints`, `floats`, `Tensors`, `dicts`, or any of the `wandb.data_types`. |\n| `commit` | (boolean, optional) Save the metrics dict to the wandb server and increment the step. If false `wandb.log` just updates the current metrics dict with the data argument and metrics won't be saved until `wandb.log` is called with `commit=True`. |\n| `step` | (integer, optional) The global step in processing. This persists any non-committed earlier steps but defaults to not committing the specified step. |\n| `sync` | (boolean, True) This argument is deprecated and currently doesn't change the behaviour of `wandb.log`. |  \n\n#### Examples:  \n\nFor more and more detailed examples, see\nour guides to logging.",
    "df50412d-5b7e-4a29-98e8-adc37238a605": "## WandbTracer\n### Methods\n#### `init_run`\n##### Parameters:\n\n* **`run_args`**: (dict, optional) Arguments to pass to `wandb.init()`. If not provided, `wandb.init()` will be\ncalled with no arguments. Please refer to the `wandb.init` for more details.  \n\nWe only want to start a new run if the run args differ. This will reduce\nthe number of W&B runs created, which is more ideal in a notebook\nsetting. Note: it is uncommon to call this method directly. Instead, you\nshould use the `WandbTracer.init()` method. This method is exposed if you\nwant to manually initialize the tracer and add it to the list of handlers.",
    "4d5a67d4-268f-4319-b8c1-a7c503467bab": "| `validation_steps` | (int) if `validation_data` is a generator, how many steps to run the generator for the full validation set. |\n| `labels` | (list) If you are visualizing your data with wandb this list of labels will convert numeric output to understandable string if you are building a multiclass classifier. If you are making a binary classifier you can pass in a list of two labels [\"label for false\", \"label for true\"]. If `validate_data` and generator are both false, this won't do anything. |\n| `predictions` | (int) the number of predictions to make for visualization each epoch, max is 100. |\n| `input_type` | (string) type of the model input to help visualization. can be one of: (`image`, `images`, `segmentation_mask`, `auto`). |\n| `output_type` | (string) type of the model output to help visualization. can be one of: (`image`, `images`, `segmentation_mask`, `label`). |\n| `log_evaluation` | (boolean) if True, save a Table containing validation data and the model's predictions at each epoch. See `validation_indexes`, `validation_row_processor`, and `output_row_processor` for additional details. |\n| `class_colors` | ([float, float, float]) if the input or output is a segmentation mask, an array containing an rgb tuple (range 0-1) for each class. |\n| `log_batch_frequency` | (integer) if None, callback will log every epoch. If set to integer, callback will log training metrics every `log_batch_frequency` batches. |\n| `log_best_prefix` | (string) if None, no extra summary metrics will be saved. If set to a string, the monitored metric and epoch will be prepended with this value and stored as summary metrics. |",
    "7954422d-3953-4fb3-9765-701aec56c707": "## Command Line Interface\n| **Command** | **Description** |\n| --- | --- |\n| agent | Run the W&B agent |\n| artifact | Commands for interacting with artifacts |\n| controller | Run the W&B local sweep controller |\n| disabled | Disable W&B. |\n| docker | Run your code in a docker container. |\n| docker-run | Wrap `docker run` and adds WANDB\\_API\\_KEY and WANDB\\_DOCKER... |\n| enabled | Enable W&B. |\n| import | Commands for importing data from other systems |\n| init | Configure a directory with Weights & Biases |\n| job | Commands for managing and viewing W&B jobs |\n| launch | Launch or queue a W&B Job. |\n| launch-agent | Run a W&B launch agent. |\n| launch-sweep | Run a W&B launch sweep (Experimental). |\n| login | Login to Weights & Biases |\n| offline | Disable W&B sync |\n| online | Enable W&B sync |\n| pull | Pull files from Weights & Biases |\n| restore | Restore code, config and docker state for a run |\n| scheduler | Run a W&B launch sweep scheduler (Experimental) |\n| server | Commands for operating a local W&B server |\n| status | Show configuration settings |\n| sweep | Create a sweep |\n| sync | Upload an offline training directory to W&B |\n| verify | Verify your local instance |",
    "8b8ace64-e346-4b74-af85-212bb8e6bc79": "## Production monitoring\n\n:::info\nProduction monitoring is currently in preview and under active development.\n:::  \n\nProduction monitoring for AI means real-time observability and analytics for any models served from your application. For models deployed to production, monitoring tools and dashboards help track key performance metrics like query rates and latency and enable interactive analytics around model prediction quality and trends, patterns of errors or edge cases, data drift, etc.  \n\n## How do I use W&B to monitor models?  \n\nW&B offers a data management service to compliment the open source Weave project. You can stream live data (and/or save batch tables) in any schema that makes sense for your use case and workflow\u2014like a no-setup database, without the SQL. This approach is effective for tracking and visualizing live production queries, model predictions, dynamic evaluation metrics, user feedback, and more.  \n\nGet started in two steps:  \n\n## Weave StreamTable API  \n\n## Weave Monitor Decorator",
    "6fab5684-679d-4e4c-baa9-1016ff28e256": "## Import & Export Data\n### Export Data\nUse the Public API to export or update data that you have saved to W&B. Before using this API, you'll want to log data from your script \u2014 check the Quickstart for more details.  \n\n**Use Cases for the Public API**  \n\n* **Export Data**: Pull down a dataframe for custom analysis in a Jupyter Notebook. Once you have explored the data, you can sync your findings by creating a new analysis run and logging results, for example: `wandb.init(job_type=\"analysis\")`\n* **Update Existing Runs**: You can update the data logged in association with a W&B run. For example, you might want to update the config of a set of runs to include additional information, like the architecture or a hyperparameter that wasn't originally logged.  \n\nSee the Generated Reference Docs for details on available functions.  \n\n### Authentication  \n\nAuthenticate your machine with your API key in one of two ways:  \n\n1. Run `wandb login` on the command line and paste in your API key.\n2. Set the `WANDB_API_KEY` environment variable to your API key.",
    "ca4d12c5-e7a3-461a-9eb8-d62483e8bc03": "## Configure Experiments\n\nConfigure a Machine Learning Experiment  \n\n**Try in a Colab Notebook here**  \n\nUse the `wandb.config` object to save your training configuration such as:\n- hyperparameters\n- input settings such as the dataset name or model type\n- any other independent variables for your experiments.  \n\nThe `wandb.config` attribute makes it easy to analyze your experiments and reproduce your work in the future. You can group by configuration values in the W&B App, compare the settings of different W&B Runs and view how different training configurations affect the output. A Run's `config` attribute is a dictionary-like object, and it can be built from lots of dictionary-like objects.  \n\n:::info\nDependent variables (like loss and accuracy) or output metrics should be saved with `wandb.log`instead.\n:::  \n\n## Set up an experiment configuration  \n\n## `absl.FLAGS`  \n\nYou can also pass in `absl` flags.  \n\n## File-Based Configs  \n\n## TensorFlow v1 Flags",
    "9be52b6b-beb0-4aa0-9c02-acbf72e86abf": "## Teams\n### Team Roles and Permissions\n:::note\nW&B recommends to have more than one admin in a team. It is a best practice to ensure that admin operations can continue when the primary admin is not available.\n:::  \n\n:::note\nIf you're on W&B Server (Dedicated Cloud or Self-managed deployment), you will need a updated enterprise license to use the **Custom Roles** feature.\n:::  \n\n### Team Settings  \n\nTeam settings allow you to manage the settings for your team and its members. With these privileges, you can effectively oversee and organize your team within W&B.  \n\n| Permissions | View-Only | Team Member | Team Admin |\n| --- | --- | --- | --- |\n| Add team members |  |  | X |\n| Remove team members |  |  | X |\n| Manage team settings |  |  | X |  \n\n### Model Registry  \n\nThe proceeding table lists permissions that apply to all projects across a given team.  \n\n### Reports  \n\nReport permissions grant access to create, view, and edit reports. The proceeding table lists permissions that apply to all reports across a given team.  \n\n### Experiments",
    "7eef2a90-c634-4d96-93c1-adc5f1b1f50d": "## Simple Transformers\n### Structure\nThe library is designed to have a separate class for every NLP task. The classes that provide similar functionality are grouped together.  \n\n* `simpletransformers.classification` - Includes all Classification models.\n* `ClassificationModel`\n* `MultiLabelClassificationModel`\n* `simpletransformers.ner` - Includes all Named Entity Recognition models.\n* `NERModel`\n* `simpletransformers.question_answering` - Includes all Question Answering models.\n* `QuestionAnsweringModel`  \n\nHere are some minimal examples",
    "60398977-4b59-46e8-96f5-19c48e1763ff": "## Manage users\n### Manage a team\n#### Create a team\n1. Navigate to the W&B Organization dashboard.\n2. Select the **Create new team** button on the left navigation panel.\n3. A modal will appear. Prove a name for your team in the **Team name** field.\n4. Select a storage type.\n5. Click on the **Create team** button.  \n\nThis will redirect you to a newly created Team home page.",
    "e2eb9749-e1c7-4fb1-beaa-608cd2acb9c2": "## General\n#### How is W&B different from TensorBoard?\n1. **Reproduce models**: W&B is good for experimentation, exploration, and reproducing models later. We capture not just the metrics, but also the hyperparameters and version of the code, and we can save your model checkpoints for you so your project is reproducible.\n2. **Automatic organization**: If you hand off a project to a collaborator or take a vacation, W&B makes it easy to see all the models you've tried so you're not wasting hours re-running old experiments.\n3. **Fast, flexible integration**: Add W&B to your project in 5 minutes. Install our free open-source Python package and add a couple of lines to your code, and every time you run your model you'll have nice logged metrics and records.\n4. **Persistent, centralized dashboard**: Anywhere you train your models, whether on your local machine, your lab cluster, or spot instances in the cloud, we give you the same centralized dashboard. You don't need to spend your time copying and organizing TensorBoard files from different machines.\n5. **Powerful table**: Search, filter, sort, and group results from different models. It's easy to look over thousands of model versions and find the best-performing models for different tasks. TensorBoard isn't built to work well on large projects.\n6. **Tools for collaboration**: Use W&B to organize complex machine learning projects. It's easy to share a link to W&B, and you can use private teams to have everyone send results to a shared project. We also support collaboration via reports\u2014 add interactive visualizations and describe your work in markdown. This is a great way to keep a work log, share findings with your supervisor, or present findings to your lab.",
    "c3926ce9-5288-49a1-9397-99f8a22087f0": "## Reports FAQ\n#### WYSIWYG FAQ\nAbsolutely! Type \"/mark\" anywhere in the document and hit enter to insert a Markdown block. You can edit these blocks with Markdown the way you used to.  \n\n**My report is running slowly now.**  \n\nSorry! We're constantly working on performance improvements, but WYSIWYG reports may run slowly on older hardware or exceptionally large reports. You can assuage the problem for now by collapsing sections of the report that you're not currently working on, like so:  \n\n**How do I delete a panel grid?**  \n\nSelect the panel grid, and hit delete/backspace. The easiest way to select a panel grid is by clicking its drag handle on the top-right, like so:  \n\n**How do I insert a table?**  \n\nTables are the only feature from Markdown that we haven't added a WYSIWYG equivalent for yet. But because we still support Markdown, you can add a table by adding a Markdown block and creating a table inside of it.  \n\n**I converted my report to WYSIWYG but I'd like to revert back to Markdown.**",
    "cb7e80c1-d22a-4fa0-80d4-71b92bac1c73": "## 1\ufe0f\u20e3 Log a Dataset\n#### \ud83d\ude80 `wandb.init`\nDepending on your workflow,\na project might be as big as `car-that-drives-itself`\nor as small as `iterative-architecture-experiment-117`.  \n\n>\n> **Rule of \ud83d\udc4d**: if you can, keep all of the `Run`s that share `Artifact`s\n> inside a single project. This keeps things simple,\n> but don't worry -- `Artifact`s are portable across projects!\n>\n>\n>  \n\nTo help keep track of all the different kinds of jobs you might run,\nit's useful to provide a `job_type` when making `Runs`.\nThis keeps the graph of your Artifacts nice and tidy.  \n\n>\n> **Rule of \ud83d\udc4d**: the `job_type` should be descriptive and correspond to a single step of your pipeline. Here, we separate out `load`ing data from `preprocess`ing data.\n>\n>\n>"
}