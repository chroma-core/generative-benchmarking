{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Custom Benchmark\n",
    "\n",
    "This notebook walks through how to generate a custom benchmark based on your data.\n",
    "\n",
    "We will be using Anthropic's claude-3-5-sonnet for generating queries and OpenAI's text-embedding-3-large for embedding, but these models can easily be switched out:\n",
    "- Various embedding functions are provided in `embedding_functions.py`\n",
    "- LLM prompts are provided in `llm_functions.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install & Import\n",
    "\n",
    "Install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import json\n",
    "import datetime\n",
    "from openai import OpenAI as OpenAIClient\n",
    "from anthropic import Anthropic as AnthropicClient\n",
    "from functions.llm import *\n",
    "from functions.embed import *\n",
    "from functions.chroma import *\n",
    "from functions.evaluate import *\n",
    "from functions.visualize import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load API Keys\n",
    "\n",
    "To use Chroma Cloud, you can sign up for a Chroma Cloud account [here](https://www.trychroma.com/) and create a new database. If you want to use local Chroma, skip this step and simply input `COLLECTION_NAME`, `OPENAI_API_KEY`, and `CLAUDE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma Cloud\n",
    "CHROMA_TENANT = \"YOUR CHROMA TENANT ID\"\n",
    "X_CHROMA_TOKEN = \"YOUR CHROMA API KEY\"\n",
    "DATABASE_NAME = \"YOUR CHROMA DATABASE NAME\"\n",
    "\n",
    "# Chroma Collection\n",
    "COLLECTION_NAME = \"YOUR COLLECTION NAME\"\n",
    "\n",
    "# Embedding Model\n",
    "OPENAI_API_KEY = \"YOUR OPENAI API KEY\"\n",
    "\n",
    "# LLM\n",
    "ANTHROPIC_API_KEY = \"YOUR ANTHROPIC API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Set Clients\n",
    "\n",
    "Initialize the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.HttpClient(\n",
    "  ssl=True,\n",
    "  host='api.trychroma.com',\n",
    "  tenant=CHROMA_TENANT,\n",
    "  database=DATABASE_NAME,\n",
    "  headers={\n",
    "    'x-chroma-token': X_CHROMA_TOKEN\n",
    "  }\n",
    ")\n",
    "\n",
    "# If you want to use the local Chroma instead, uncomment the following line:\n",
    "# chroma_client = chromadb.Client()\n",
    "\n",
    "openai_client = OpenAIClient(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = AnthropicClient(api_key=ANTHROPIC_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Chroma Collection\n",
    "\n",
    "If you already have a Chroma Collection for your data, skip to **3. Filter Documents for Quality**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load in Your Data\n",
    "\n",
    "We use pre-chunked [Chroma Docs](https://docs.trychroma.com/docs/overview/introduction) as an example, but replace this with your own data.\n",
    "\n",
    "NOTE: should we add a chunking function for people to process their data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/chroma_docs.json', 'r') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ids = list(corpus.keys())\n",
    "corpus_documents = [corpus[key] for key in corpus_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Embed Data & Add to Chroma Collection\n",
    "\n",
    "Embed your documents using an embedding model of your choice. We use Openai's text-embedding-3-large here, but have other functions available in `embed.py`. You may also define your own embedding function.\n",
    "\n",
    "We use batching and multi-threading for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = openai_embed_in_batches(\n",
    "    openai_client=openai_client,\n",
    "    texts=corpus_documents,\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "\n",
    "corpus_collection = chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "collection_add_in_batches(\n",
    "    collection=corpus_collection,\n",
    "    ids=corpus_ids,\n",
    "    texts=corpus_documents,\n",
    "    embeddings=corpus_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Documents for Quality\n",
    "\n",
    "We begin by filtering our documents prior to query generation, this step ensures that we avoid generating queries from irrelevant or incomplete documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set Criteria\n",
    "\n",
    "We use the following criteria:\n",
    "- `relevance` checks whether the document is relevant to the specified context\n",
    "- `completeness` checks for overall quality of the document\n",
    "\n",
    "You can modify the criteria as you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in `context` according to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"FILL IN WITH CONTEXT RELEVANT TO YOUR USE CASE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = f\"The document is relevant to the following context: {context}\"\n",
    "completeness = \"The document is complete, meaning that it contains useful information to answer queries and does not only serve as an introduction to the main content that users may be looking for.\"\n",
    "\n",
    "criteria = [relevance, completeness]\n",
    "criteria_labels = [\"relevance\", \"completeness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get Documents\n",
    "\n",
    "Get your Chroma collection and filter documents according to criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_collection = chroma_client.get_collection(\n",
    "    name=COLLECTION_NAME\n",
    ")\n",
    "\n",
    "corpus = get_collection_items(\n",
    "    collection=corpus_collection\n",
    ")\n",
    "\n",
    "corpus_ids = [key for key in corpus.keys()]\n",
    "corpus_documents = [corpus[key]['document'] for key in corpus_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Filter Documents\n",
    "\n",
    "We create a batch request for our LLM calls (this is cheaper and typically faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_documents_batch_id = create_document_filter_batch(\n",
    "    client=anthropic_client,\n",
    "    documents=corpus_documents,\n",
    "    ids=corpus_ids,\n",
    "    criteria=criteria,\n",
    "    criteria_labels=criteria_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the status of your batch through the [Anthropic Console](https://console.anthropic.com/workspaces/default/batches).\n",
    "\n",
    "Retrieve the batch once it is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_documents_batch = retrieve_document_filter_batch_df(\n",
    "    client=anthropic_client,\n",
    "    batch_id=filtered_documents_batch_id\n",
    ")\n",
    "\n",
    "passed_document_ids = get_filtered_ids(\n",
    "    filtered_documents_batch_df=filtered_documents_batch\n",
    ")\n",
    "\n",
    "passed_documents = [corpus[id]['document'] for id in passed_document_ids]\n",
    "\n",
    "failed_document_ids = [id for id in corpus_ids if id not in passed_document_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents passed: {len(passed_document_ids)}\")\n",
    "print(f\"Number of documents failed: {len(failed_document_ids)}\")\n",
    "print(\"-\"*80)\n",
    "print(\"Example of passed document:\")\n",
    "print(corpus[passed_document_ids[0]]['document'])\n",
    "print(\"-\"*80)\n",
    "print(\"Example of failed document:\")\n",
    "print(corpus[failed_document_ids[0]]['document'])\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Golden Dataset\n",
    "\n",
    "Using our filtered documents, we can genereate a golden dataset of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Custom Prompt\n",
    "\n",
    "We will use the `context` (from the prior section) and `example_queries` for query generation.\n",
    "\n",
    "Fill in `example_queries` with examples of what users may ask. These examples help indicate what kind of topics users typically focus on, as well as the style of query that should be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_queries = \"FILL IN WITH EXAMPLE QUERIES\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate Queries\n",
    "\n",
    "Send a batch request for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset_batch_id = create_golden_dataset_batch(\n",
    "    client=anthropic_client,\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    documents=passed_documents,\n",
    "    ids=passed_document_ids,\n",
    "    context=context,\n",
    "    example_queries=example_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset = retrieve_batch(\n",
    "    client=anthropic_client,\n",
    "    batch_id=golden_dataset_batch_id\n",
    ")\n",
    "\n",
    "golden_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate\n",
    "\n",
    "Now that we have our golden dataset, we will can run our evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = golden_dataset['query'].tolist()\n",
    "ids = golden_dataset['id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed generated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = openai_embed_in_batches(\n",
    "    openai_client=openai_client,\n",
    "    texts=golden_dataset[\"query\"],\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "query_embeddings_lookup = {\n",
    "    id: {\n",
    "        \"text\": query,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "    for id, query, embedding in zip(golden_dataset[\"id\"], golden_dataset[\"query\"], query_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our qrels (query relevance labels) dataframe. In this case, each query and its corresponding document share the same id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = pd.DataFrame(\n",
    "    {\n",
    "        \"query-id\": ids,\n",
    "        \"corpus-id\": ids,\n",
    "        \"score\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_benchmark(\n",
    "    query_embeddings_lookup=query_embeddings_lookup,\n",
    "    collection=corpus_collection,\n",
    "    qrels=qrels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results.\n",
    "\n",
    "This is helpful for comparison (e.g. comparing different embedding models or chunking strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_save = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model\": \"text-embedding-3-large\",\n",
    "    \"results\": results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/results_v1.json', 'w') as f:\n",
    "    json.dump(results_to_save, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
