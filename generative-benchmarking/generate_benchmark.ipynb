{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Custom Benchmark\n",
    "\n",
    "This notebook walks through how to generate a custom benchmark based on your data.\n",
    "\n",
    "We will be using OpenAI for our embedding model and LLM, but this can easily be switched out:\n",
    "- Various embedding functions are provided in `embedding_functions.py`\n",
    "- LLM prompts are provided in `llm_functions.py`\n",
    "\n",
    "NOTE: When switching out embedding models, you will need to make a new collection for your new embeddings. Then, embed the same documents and queries with the embedding model of your choice. \n",
    "\n",
    "Use the same golden dataset of queries when comparing embedding models on the same data.\n",
    "\n",
    "Cells that should be modified when switching out embedding models are labeled as **[Modify]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install & Import\n",
    "\n",
    "Install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI as OpenAIClient\n",
    "from anthropic import Anthropic as AnthropicClient\n",
    "from functions.llm import *\n",
    "from functions.embed import *\n",
    "from functions.chroma import *\n",
    "from functions.evaluate import *\n",
    "from functions.visualize import *\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pre-chunked [Chroma Docs](https://docs.trychroma.com/docs/overview/introduction) as an example. To run this notebook with your own data, uncomment the commented out lines and fill in.\n",
    "\n",
    "**[Modify]** `COLLECTION_NAME` when you change your embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/chroma_docs.json', 'r') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "context = \"This is a technical support bot for Chroma, a vector database company often used by developers for building AI applications.\"\n",
    "example_queries = \"\"\"\n",
    "    how to add to a collection\n",
    "    filter by metadata\n",
    "    retrieve embeddings when querying\n",
    "    how to use openai embedding function when adding to collection\n",
    "    \"\"\"\n",
    "\n",
    "COLLECTION_NAME = \"chroma-docs-openai-large\" # change this collection name whenever you switch embedding models\n",
    "\n",
    "# Generate a Benchmark with your own data:\n",
    "\n",
    "# with open('filepath/to/your/data.json', 'r') as f:\n",
    "#     corpus = json.load(f)\n",
    "\n",
    "# context = \"FILL IN WITH CONTEXT RELEVANT TO YOUR USE CASE\"\n",
    "# example_queries = \"FILL IN WITH EXAMPLE QUERIES\"\n",
    "\n",
    "# COLLECTION_NAME = \"YOUR COLLECTION NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load API Keys\n",
    "\n",
    "To use Chroma Cloud, you can sign up for a Chroma Cloud account [here](https://www.trychroma.com/) and create a new database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model & LLM\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# If you want to use Chroma Cloud, uncomment and fill in the following:\n",
    "# CHROMA_TENANT = \"YOUR CHROMA TENANT ID\"\n",
    "# X_CHROMA_TOKEN = \"YOUR CHROMA API KEY\"\n",
    "# DATABASE_NAME = \"YOUR CHROMA DATABASE NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Set Clients\n",
    "\n",
    "Initialize the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# If you want to use Chroma Cloud, uncomment the following line:\n",
    "# chroma_client = chromadb.HttpClient(\n",
    "#   ssl=True,\n",
    "#   host='api.trychroma.com',\n",
    "#   tenant=CHROMA_TENANT,\n",
    "#   database=DATABASE_NAME,\n",
    "#   headers={\n",
    "#     'x-chroma-token': X_CHROMA_TOKEN\n",
    "#   }\n",
    "# )\n",
    "\n",
    "openai_client = OpenAIClient(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Chroma Collection\n",
    "\n",
    "If you already have a Chroma Collection for your data, skip to **2.3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load in Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ids = list(corpus.keys())\n",
    "corpus_documents = [corpus[key] for key in corpus_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Embed Data & Add to Chroma Collection\n",
    "\n",
    "Embed your documents using an embedding model of your choice. We use Openai's text-embedding-3-large here, but have other functions available in `embed.py`. You may also define your own embedding function.\n",
    "\n",
    "We use batching and multi-threading for efficiency.\n",
    "\n",
    "**[Modify]** embedding function (`openai_embed_in_batches`) to the embedding model you wish to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OpenAI batches: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 0 to 100Adding 100 to 200\n",
      "\n",
      "Adding 200 to 216\n"
     ]
    }
   ],
   "source": [
    "corpus_embeddings = openai_embed_in_batches(\n",
    "    openai_client=openai_client,\n",
    "    texts=corpus_documents,\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "\n",
    "corpus_collection = chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "collection_add_in_batches(\n",
    "    collection=corpus_collection,\n",
    "    ids=corpus_ids,\n",
    "    texts=corpus_documents,\n",
    "    embeddings=corpus_embeddings,\n",
    ")\n",
    "\n",
    "corpus = {\n",
    "    id: {\n",
    "        'document': document,\n",
    "        'embedding': embedding\n",
    "    }\n",
    "    for id, document, embedding in zip(corpus_ids, corpus_documents, corpus_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 3/3 [00:00<00:00, 108.07it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_collection = chroma_client.get_collection(\n",
    "    name=COLLECTION_NAME\n",
    ")\n",
    "\n",
    "corpus = get_collection_items(\n",
    "    collection=corpus_collection\n",
    ")\n",
    "\n",
    "corpus_ids = [key for key in corpus.keys()]\n",
    "corpus_documents = [corpus[key]['document'] for key in corpus_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Documents for Quality\n",
    "\n",
    "We begin by filtering our documents prior to query generation, this step ensures that we avoid generating queries from irrelevant or incomplete documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set Criteria\n",
    "\n",
    "We use the following criteria:\n",
    "- `relevance` checks whether the document is relevant to the specified context\n",
    "- `completeness` checks for overall quality of the document\n",
    "\n",
    "You can modify the criteria as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = f\"The document is relevant to the following context: {context}\"\n",
    "completeness = \"The document is complete, meaning that it contains useful information to answer queries and does not only serve as an introduction to the main content that users may be looking for.\"\n",
    "\n",
    "criteria = [relevance, completeness]\n",
    "criteria_labels = [\"relevance\", \"completeness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filter Documents\n",
    "\n",
    "We filter our documents using gpt-4o-mini. Batching functions are also available in `llm.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_document_ids = filter_documents(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    documents=corpus_documents,\n",
    "    ids=corpus_ids,\n",
    "    criteria=criteria,\n",
    "    criteria_labels=criteria_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed_documents = [corpus[id]['document'] for id in filtered_document_ids]\n",
    "\n",
    "failed_document_ids = [id for id in corpus_ids if id not in filtered_document_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents passed: 44\n",
      "Number of documents failed: 172\n",
      "--------------------------------------------------------------------------------\n",
      "Example of passed document:\n",
      " Chroma Collections\n",
      "\n",
      "Chroma collections can be queried in a variety of ways, using the `.query` method.\n",
      "\n",
      "You can query by a set of `query embeddings`.\n",
      "\n",
      "{% TabbedCodeBlock %}\n",
      "\n",
      "{% Tab label=\"python\" %}\n",
      "```python\n",
      "collection.query(\n",
      "    query_embeddings=[[11.1, 12.1, 13.1],[1.1, 2.3, 3.2], ...],\n",
      "    n_results=10,\n",
      "    where={\"metadata_field\": \"is_equal_to_this\"},\n",
      "    where_document={\"$contains\":\"search_string\"}\n",
      ")\n",
      "```\n",
      "{% /Tab %}\n",
      "\n",
      "{% Tab label=\"typescript\" %}\n",
      "```typescript\n",
      "const result = await collection.query({\n",
      "    queryEmbeddings: [[11.1, 12.1, 13.1],[1.1, 2.3, 3.2], ...],\n",
      "    nResults: 10,\n",
      "    where: {\"metadata_field\": \"is_equal_to_this\"},\n",
      "})\n",
      "```\n",
      "{% /Tab %}\n",
      "\n",
      "{% /TabbedCodeBlock %}\n",
      "\n",
      "The query will return the `n results` closest matches to each `query embedding`, in order.\n",
      "An optional `where` filter dictionary can be supplied to filter by the `metadata`\n",
      "--------------------------------------------------------------------------------\n",
      "Example of failed document:\n",
      "\n",
      "    # typescript\n",
      "    import { OpenAIEmbeddingFunction } from 'chromadb';\n",
      "\n",
      "    const embeddingFunction = new OpenAIEmbeddingFunction({\n",
      "        openai_api_key: \"apiKey\",\n",
      "        openai_model: \"text-embedding-3-small\"\n",
      "    })\n",
      "\n",
      "    // use directly\n",
      "    const embeddings = embeddingFunction.generate([\"document1\",\"document2\"])\n",
      "\n",
      "    // pass documents to query for .add and .query\n",
      "    let collection = await client.createCollection({\n",
      "        name: \"name\",\n",
      "        embeddingFunction: embeddingFunction\n",
      "    })\n",
      "    collection = await client.getCollection({\n",
      "        name: \"name\",\n",
      "        embeddingFunction: embeddingFunction\n",
      "    })\n",
      "    \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents passed: {len(filtered_document_ids)}\")\n",
    "print(f\"Number of documents failed: {len(failed_document_ids)}\")\n",
    "print(\"-\"*80)\n",
    "print(\"Example of passed document:\")\n",
    "print(corpus[filtered_document_ids[0]]['document'])\n",
    "print(\"-\"*80)\n",
    "print(\"Example of failed document:\")\n",
    "print(corpus[failed_document_ids[0]]['document'])\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Golden Dataset\n",
    "\n",
    "Using our filtered documents, we can genereate a golden dataset of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Custom Prompt\n",
    "\n",
    "We will use `context` and `example_queries` for query generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate Queries\n",
    "\n",
    "Generate queries with gpt-4o. Batching functions are available in `llm.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating queries: 100%|██████████| 44/44 [00:24<00:00,  1.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51dedd38-9496-4e7a-8666-b5acf2bd29bb</td>\n",
       "      <td>query by embedding with typescript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9c820c40-7492-481c-9aba-07b08b0c860d</td>\n",
       "      <td>use of include parameter for customizing retur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e00d6a02-5557-44f0-b349-dc2a646f71cc</td>\n",
       "      <td>setup Chroma backend with Docker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e71ec041-f435-4434-a2e7-59c1d93ce9c0</td>\n",
       "      <td>connect client to server using HttpClient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e827e31d-eb51-4877-ad1e-aeb667766a31</td>\n",
       "      <td>chroma utils vacuum usage and steps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  51dedd38-9496-4e7a-8666-b5acf2bd29bb   \n",
       "1  9c820c40-7492-481c-9aba-07b08b0c860d   \n",
       "2  e00d6a02-5557-44f0-b349-dc2a646f71cc   \n",
       "3  e71ec041-f435-4434-a2e7-59c1d93ce9c0   \n",
       "4  e827e31d-eb51-4877-ad1e-aeb667766a31   \n",
       "\n",
       "                                               query  \n",
       "0                 query by embedding with typescript  \n",
       "1  use of include parameter for customizing retur...  \n",
       "2                   setup Chroma backend with Docker  \n",
       "3          connect client to server using HttpClient  \n",
       "4                chroma utils vacuum usage and steps  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_dataset = create_golden_dataset(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o\",\n",
    "    documents=passed_documents,\n",
    "    ids=filtered_document_ids,\n",
    "    context=context,\n",
    "    example_queries=example_queries\n",
    ")\n",
    "\n",
    "golden_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate\n",
    "\n",
    "Now that we have our golden dataset, we will can run our evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = golden_dataset['query'].tolist()\n",
    "ids = golden_dataset['id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed generated queries.\n",
    "\n",
    "**[Modify]** embedding function (`openai_embed_in_batches`) to the embedding model you wish to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OpenAI batches: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = openai_embed_in_batches(\n",
    "    openai_client=openai_client,\n",
    "    texts=queries,\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "query_embeddings_lookup = {\n",
    "    id: {\n",
    "        \"text\": query,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "    for id, query, embedding in zip(ids, queries, query_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our qrels (query relevance labels) dataframe. In this case, each query and its corresponding document share the same id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = pd.DataFrame(\n",
    "    {\n",
    "        \"query-id\": ids,\n",
    "        \"corpus-id\": ids,\n",
    "        \"score\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:00<00:00, 14.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@1: 0.63636\n",
      "NDCG@3: 0.67641\n",
      "NDCG@5: 0.69499\n",
      "NDCG@10: 0.72507\n",
      "MAP@1: 0.63636\n",
      "MAP@3: 0.66667\n",
      "MAP@5: 0.67689\n",
      "MAP@10: 0.6897\n",
      "Recall@1: 0.63636\n",
      "Recall@3: 0.70455\n",
      "Recall@5: 0.75\n",
      "Recall@10: 0.84091\n",
      "P@1: 0.63636\n",
      "P@3: 0.23485\n",
      "P@5: 0.15\n",
      "P@10: 0.08409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_benchmark(\n",
    "    query_embeddings_lookup=query_embeddings_lookup,\n",
    "    collection=corpus_collection,\n",
    "    qrels=qrels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results.\n",
    "\n",
    "This is helpful for comparison (e.g. comparing different embedding models).\n",
    "\n",
    "**[Modify]** \"model\" to the model you are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "results_to_save = {\n",
    "    \"model\": \"text-embedding-3-large\",\n",
    "    \"results\": results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/{timestamp}.json', 'w') as f:\n",
    "    json.dump(results_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
